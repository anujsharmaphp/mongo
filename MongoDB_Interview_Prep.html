<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MongoDB Interview Prep</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code:wght@400;500&display=swap');
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Inter', sans-serif; background: #f8f9fa; color: #1e293b; line-height: 1.6; }

        .layout { display: flex; min-height: 100vh; }

        /* Sidebar */
        .sidebar {
            width: 260px;
            min-width: 260px;
            background: #fff;
            border-right: 1px solid #e2e8f0;
            padding: 24px 0;
            position: fixed;
            top: 0;
            left: 0;
            height: 100vh;
            overflow-y: auto;
        }
        .sidebar h2 { font-size: 15px; color: #0f766e; padding: 0 20px; margin-bottom: 4px; }
        .sidebar .sub { font-size: 11px; color: #94a3b8; padding: 0 20px; margin-bottom: 20px; }
        .sidebar nav a {
            display: block;
            padding: 9px 20px;
            font-size: 13px;
            color: #475569;
            text-decoration: none;
            border-left: 3px solid transparent;
            transition: all 0.2s;
        }
        .sidebar nav a:hover { background: #f0fdf4; color: #0f766e; border-left-color: #0f766e; }
        .sidebar nav a.active { background: #f0fdf4; color: #0f766e; border-left-color: #0f766e; font-weight: 600; }
        .sidebar nav a.pending { color: #cbd5e1; }

        /* Main content */
        .main { margin-left: 260px; padding: 30px 40px; max-width: 820px; }

        /* Concept block */
        .concept { background: #fff; border-radius: 10px; padding: 28px; margin-bottom: 24px; border: 1px solid #e2e8f0; }
        .concept h2 { font-size: 20px; color: #0f766e; margin-bottom: 4px; }
        .concept .tag { display: inline-block; background: #f0fdf4; color: #0f766e; font-size: 11px; padding: 2px 10px; border-radius: 20px; margin-bottom: 14px; font-weight: 500; }

        /* Simple points */
        .point { margin-bottom: 14px; }
        .point strong { color: #0f766e; font-size: 13px; }
        .point p { font-size: 13px; color: #475569; margin-top: 2px; }
        .point ul { padding-left: 18px; margin-top: 4px; }
        .point li { font-size: 12.5px; color: #475569; margin-bottom: 3px; }
        .point li strong { color: #1e293b; }

        /* Code */
        pre { background: #f1f5f9; border-radius: 8px; padding: 14px; overflow-x: auto; margin: 10px 0; border: 1px solid #e2e8f0; }
        code { font-family: 'Fira Code', monospace; font-size: 12px; color: #0f766e; }

        /* Q&A */
        .qa h3 { font-size: 13px; color: #b45309; margin: 18px 0 10px; text-transform: uppercase; letter-spacing: 1px; border-top: 1px solid #e2e8f0; padding-top: 16px; }
        .q { background: #fffbeb; border-radius: 8px; padding: 12px 14px; margin-bottom: 8px; border: 1px solid #fde68a; }
        .q .question { font-size: 13px; font-weight: 600; color: #1e293b; margin-bottom: 6px; }
        .q .answer { font-size: 12.5px; color: #57534e; }
        .q .answer strong { color: #1e293b; }

        /* Responsive */
        @media (max-width: 768px) {
            .sidebar { display: none; }
            .main { margin-left: 0; padding: 20px; }
        }
    </style>
</head>
<body>
<div class="layout">

    <!-- Sidebar -->
    <aside class="sidebar">
        <h2>MongoDB Prep</h2>
        <p class="sub">Interview Guide</p>
        <nav>
            <a href="#concept-1" class="active">1. Fundamentals</a>
            <a href="#concept-2">2. Schema Design</a>
            <a href="#concept-3">3. Indexing</a>
            <a href="#concept-4">4. Aggregation</a>
            <a href="#concept-5">5. Sharding</a>
            <a href="#concept-6">6. Replication</a>
            <a href="#concept-7">7. Atlas & Cloud</a>
            <a href="#concept-8" class="pending">8. Performance Tuning</a>
            <a href="#concept-9" class="pending">9. Transactions</a>
            <a href="#concept-10" class="pending">10. Security</a>
            <a href="#concept-11" class="pending">11. Kafka + MongoDB</a>
            <a href="#concept-12" class="pending">12. Mongo vs Elastic</a>
            <a href="#concept-13" class="pending">13. Project Scenarios</a>
        </nav>
    </aside>

    <!-- Main Content -->
    <main class="main">

        <!-- CONCEPT 1 -->
        <div class="concept" id="concept-1">
            <h2>1. Fundamentals & Architecture</h2>
            <span class="tag">Core Foundation</span>

            <div class="point">
                <strong>What is MongoDB?</strong>
                <p>NoSQL document database. Stores data as BSON (Binary JSON) documents. No fixed schema needed.</p>
            </div>

            <div class="point">
                <strong>Structure (vs MySQL)</strong>
                <ul>
                    <li><strong>Database</strong> = Database</li>
                    <li><strong>Collection</strong> = Table</li>
                    <li><strong>Document</strong> = Row (but flexible, no fixed columns)</li>
                    <li><strong>Field</strong> = Column</li>
                    <li><strong>_id</strong> = Primary Key (auto-generated 12-byte ObjectId)</li>
                </ul>
            </div>

            <div class="point">
                <strong>ObjectId (12 bytes)</strong>
                <p>4 bytes timestamp + 5 bytes random + 3 bytes counter. Sortable by creation time.</p>
            </div>

            <div class="point">
                <strong>WiredTiger Engine</strong>
                <ul>
                    <li><strong>Document-level locking</strong> &mdash; multiple writes to different docs at same time</li>
                    <li><strong>Compression</strong> &mdash; Snappy (default), zstd (better ratio)</li>
                    <li><strong>Journal</strong> &mdash; WAL synced every 50ms for crash recovery</li>
                    <li><strong>Checkpoint</strong> &mdash; flushes to disk every 60 seconds</li>
                    <li><strong>Cache</strong> &mdash; uses 50% of (RAM - 1GB)</li>
                </ul>
            </div>

            <div class="point">
                <strong>BSON vs JSON</strong>
                <p>BSON adds types: Date, ObjectId, Decimal128, Int64, BinData. Faster to parse. Max doc = <strong>16MB</strong>.</p>
            </div>

            <div class="point">
                <strong>CRUD Examples</strong>
<pre><code>// INSERT
db.patients.insertOne({ name: "Rajesh", age: 45, codes: ["E11", "I10"] })
db.patients.insertMany([{ name: "A" }, { name: "B" }])

// FIND
db.patients.find({ age: { $gte: 30 } })       // age >= 30
db.patients.find({ "records.bp": "140/90" })   // nested
db.patients.find({ codes: "E11" })             // array contains

// UPDATE
db.patients.updateOne(
  { name: "Rajesh" },
  { $set: { age: 46 }, $push: { codes: "M19" } }
)

// DELETE
db.patients.deleteMany({ age: { $lt: 18 } })</code></pre>
            </div>

            <div class="point">
                <strong>Key Operators (remember these)</strong>
                <ul>
                    <li><strong>Compare:</strong> $eq, $ne, $gt, $gte, $lt, $lte, $in, $nin</li>
                    <li><strong>Logic:</strong> $and, $or, $not, $nor</li>
                    <li><strong>Array:</strong> $all, $elemMatch, $size, $push, $pull, $addToSet</li>
                    <li><strong>Update:</strong> $set, $unset, $inc, $rename, $pop</li>
                    <li><strong>Element:</strong> $exists, $type</li>
                </ul>
            </div>

            <div class="point">
                <strong>Write Concern (durability level)</strong>
                <ul>
                    <li><code>w: 1</code> &mdash; acknowledged by primary only</li>
                    <li><code>w: "majority"</code> &mdash; acknowledged by majority of replica set</li>
                    <li><code>j: true</code> &mdash; wait until written to journal</li>
                </ul>
            </div>

            <div class="point">
                <strong>Read Preference</strong>
                <ul>
                    <li><strong>primary</strong> &mdash; always read from primary (default, strongest consistency)</li>
                    <li><strong>secondary</strong> &mdash; read from secondaries (for analytics, may be stale)</li>
                    <li><strong>nearest</strong> &mdash; lowest latency node</li>
                </ul>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: Why MongoDB over MySQL for medical data?</div>
                    <div class="answer">Medical records are semi-structured &mdash; each encounter has different fields, nested labs, NLP-extracted codes. MongoDB's flexible schema avoids ALTER TABLE. One patient doc embeds vitals + diagnoses + AI-codes as nested objects, <strong>no JOINs needed</strong> at read time.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How does WiredTiger handle concurrency?</div>
                    <div class="answer"><strong>Document-level concurrency</strong> with MVCC (Multi-Version Concurrency Control). Readers see a consistent snapshot without blocking writers. Different from MySQL's row-level locking &mdash; MongoDB is optimistic, no lock waits on different docs.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Max document size? What if data is bigger?</div>
                    <div class="answer"><strong>16MB</strong>. For larger files use <strong>GridFS</strong> (splits into 255KB chunks across fs.files + fs.chunks). Or store files in S3 and keep reference URL in MongoDB.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Explain ObjectId structure.</div>
                    <div class="answer">12 bytes: <strong>4</strong> (unix timestamp) + <strong>5</strong> (random/machine) + <strong>3</strong> (incrementing counter). Unique across distributed systems. Roughly sortable by time.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Difference between insertMany and bulkWrite?</div>
                    <div class="answer"><code>insertMany</code> = batch inserts only. <code>bulkWrite</code> = mixed operations (insert + update + delete) in one request. Use bulkWrite for Kafka event processing &mdash; batch hundreds of price/inventory updates per second.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How does MongoDB ensure durability?</div>
                    <div class="answer">3 layers: (1) <strong>Journal/WAL</strong> synced every 50ms, (2) <strong>Checkpoint</strong> every 60s, (3) <strong>Write Concern</strong> &mdash; <code>w:"majority", j:true</code> for max safety. For healthcare, always use majority + journal.</div>
                </div>

                <div class="q">
                    <div class="question">Q: When to use which Read Preference?</div>
                    <div class="answer"><strong>primary</strong> for real-time medical coding (consistency matters). <strong>secondaryPreferred</strong> for analytics dashboards (slight staleness OK). <strong>nearest</strong> for geo-distributed reads (lowest latency).</div>
                </div>

                <div class="q">
                    <div class="question">Q: BSON vs JSON &mdash; why not just store JSON?</div>
                    <div class="answer">BSON adds types (Date, Decimal128, ObjectId) that JSON lacks. Has length prefixes for O(1) field skipping. Faster encode/decode. Slightly larger on wire but optimized for traversal.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 2 -->
        <div class="concept" id="concept-2">
            <h2>2. Document Modeling & Schema Design</h2>
            <span class="tag">Most Asked Topic</span>

            <div class="point">
                <strong>Golden Rule</strong>
                <p>Design schema based on <strong>how your app reads data</strong>, not how data is related. Opposite of MySQL normalization.</p>
            </div>

            <div class="point">
                <strong>Embedding vs Referencing</strong>
                <ul>
                    <li><strong>Embed</strong> = put related data inside the same document (denormalize)</li>
                    <li><strong>Reference</strong> = store _id link to another collection (like foreign key)</li>
                </ul>
<pre><code>// EMBEDDED - patient with vitals inside
{
  _id: ObjectId("..."),
  name: "Rajesh",
  vitals: [
    { date: "2024-01-10", bp: "140/90", sugar: 280 },
    { date: "2024-01-11", bp: "130/85", sugar: 220 }
  ]
}

// REFERENCED - vitals in separate collection
// patients collection
{ _id: ObjectId("p1"), name: "Rajesh" }

// vitals collection
{ _id: ObjectId("v1"), patientId: ObjectId("p1"), bp: "140/90" }
{ _id: ObjectId("v2"), patientId: ObjectId("p1"), bp: "130/85" }</code></pre>
            </div>

            <div class="point">
                <strong>When to Embed (use most of the time)</strong>
                <ul>
                    <li>Data is read together &mdash; "give me patient + vitals in one query"</li>
                    <li>1:1 relationship (user &rarr; profile)</li>
                    <li>1:Few relationship (order &rarr; 5-10 items)</li>
                    <li>Data doesn't grow unbounded</li>
                    <li>Atomic updates needed on parent + child</li>
                </ul>
            </div>

            <div class="point">
                <strong>When to Reference</strong>
                <ul>
                    <li>1:Many with unbounded growth (user &rarr; millions of logs)</li>
                    <li>Many:Many relationship (students &harr; courses)</li>
                    <li>Data accessed independently ("show me just the order, not user")</li>
                    <li>Document would exceed 16MB</li>
                    <li>Data shared across many parents</li>
                </ul>
            </div>

            <div class="point">
                <strong>Schema Design Patterns (important!)</strong>
                <ul>
                    <li><strong>Bucket Pattern</strong> &mdash; group time-series data into buckets (e.g., 1 doc = 1 hour of sensor readings)</li>
                    <li><strong>Outlier Pattern</strong> &mdash; handle documents where arrays grow beyond normal (flag + overflow collection)</li>
                    <li><strong>Computed Pattern</strong> &mdash; pre-calculate values (total, average) instead of computing on every read</li>
                    <li><strong>Subset Pattern</strong> &mdash; embed only recent/frequent data, keep rest in separate collection</li>
                    <li><strong>Extended Reference</strong> &mdash; copy frequently needed fields from referenced doc to avoid $lookup</li>
                    <li><strong>Polymorphic Pattern</strong> &mdash; different doc shapes in same collection (sports cards, game cards, all in "products")</li>
                </ul>
            </div>

            <div class="point">
                <strong>Bucket Pattern Example (Healthcare)</strong>
<pre><code>// Instead of 1 doc per vital reading (millions of docs)
// Group into hourly buckets
{
  patientId: ObjectId("p1"),
  date: ISODate("2024-01-10"),
  hour: 14,
  readings: [
    { time: "14:00", bp: "140/90", temp: 98.6 },
    { time: "14:15", bp: "138/88", temp: 98.4 },
    { time: "14:30", bp: "135/85", temp: 98.5 }
  ],
  count: 3,
  avgBp: "137/87"  // computed pattern combined
}</code></pre>
            </div>

            <div class="point">
                <strong>Polymorphic Pattern Example (eCommerce)</strong>
<pre><code>// Same "products" collection, different shapes
// Sports card
{
  type: "sports_card",
  name: "Mike Trout RC",
  sport: "baseball",
  year: 2011,
  graded: true,
  grade: "PSA 10"
}
// Game card
{
  type: "game_card",
  name: "Black Lotus",
  game: "MTG",
  set: "Alpha",
  rarity: "rare",
  foil: false
}</code></pre>
            </div>

            <div class="point">
                <strong>Anti-Patterns (avoid these)</strong>
                <ul>
                    <li><strong>Massive arrays</strong> &mdash; unbounded arrays slow down reads and hit 16MB limit</li>
                    <li><strong>Unnecessary normalization</strong> &mdash; don't split data into 10 collections like MySQL</li>
                    <li><strong>No indexes on referenced fields</strong> &mdash; $lookup without index = full collection scan</li>
                    <li><strong>Storing blobs in document</strong> &mdash; use GridFS or S3 for files</li>
                </ul>
            </div>

            <div class="point">
                <strong>$lookup (JOIN equivalent)</strong>
<pre><code>// Get patient with their lab reports from separate collection
db.patients.aggregate([
  { $match: { name: "Rajesh" } },
  { $lookup: {
      from: "lab_reports",
      localField: "_id",
      foreignField: "patientId",
      as: "labs"
  }}
])</code></pre>
                <p>$lookup is expensive. If you need it frequently, consider embedding or extended reference pattern instead.</p>
            </div>

            <div class="point">
                <strong>Schema Validation</strong>
<pre><code>db.createCollection("patients", {
  validator: {
    $jsonSchema: {
      bsonType: "object",
      required: ["name", "age"],
      properties: {
        name: { bsonType: "string", description: "required" },
        age:  { bsonType: "int", minimum: 0, maximum: 150 }
      }
    }
  },
  validationLevel: "strict",    // or "moderate"
  validationAction: "error"     // or "warn"
})</code></pre>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How did you design the schema for medical records?</div>
                    <div class="answer">Used <strong>embedding</strong> for patient + encounter data (vitals, ICD codes, NLP entities) since they're always read together. Used <strong>bucket pattern</strong> for time-series vitals to reduce doc count. Used <strong>referencing</strong> for lab reports since they can be accessed independently and grow unbounded.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Embed or Reference? How do you decide?</div>
                    <div class="answer">Ask 3 questions: (1) Is data read together? &rarr; embed. (2) Does it grow unbounded? &rarr; reference. (3) Is it shared by multiple parents? &rarr; reference. Default to embedding unless there's a reason not to. MongoDB is optimized for single-doc reads.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What schema pattern did you use for the eCommerce collectibles platform?</div>
                    <div class="answer"><strong>Polymorphic pattern</strong> &mdash; sports cards, game cards, and collectibles all in one "products" collection with different field shapes. This enabled a single search index across all product types. Also used <strong>extended reference</strong> &mdash; copied seller name + rating into product doc to avoid $lookup on every product listing.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is the Bucket Pattern and when to use it?</div>
                    <div class="answer">Groups time-series data into buckets (e.g., 1 doc = 1 hour). Instead of millions of individual readings, you get thousands of bucket docs. Reduces index size, improves write performance. Used for patient vitals monitoring &mdash; each bucket holds readings for one hour with pre-computed averages.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How do you handle Many-to-Many in MongoDB?</div>
                    <div class="answer">Two approaches: (1) <strong>Array of references</strong> in both sides &mdash; student has courseIds[], course has studentIds[]. (2) <strong>Junction collection</strong> for unbounded or with extra attributes &mdash; enrollment { studentId, courseId, grade, date }. Choose based on array growth.</div>
                </div>

                <div class="q">
                    <div class="question">Q: $lookup performance &mdash; is it like a SQL JOIN?</div>
                    <div class="answer">Similar but <strong>much slower</strong>. $lookup does a full collection scan on "from" collection unless the foreignField is indexed. It doesn't use hash joins or merge joins. Best practice: (1) always index foreignField, (2) if $lookup is frequent, consider denormalizing with embedding or extended reference pattern.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is Schema Validation in MongoDB?</div>
                    <div class="answer">MongoDB supports <strong>$jsonSchema</strong> validation on collections. Define required fields, types, min/max, enums. Set <code>validationLevel: "strict"</code> to enforce on all writes, or <code>"moderate"</code> to only validate new inserts. Set <code>validationAction: "error"</code> to reject or <code>"warn"</code> to just log.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What are common schema design anti-patterns?</div>
                    <div class="answer">(1) <strong>Unbounded arrays</strong> &mdash; arrays that grow forever slow reads, hit 16MB. (2) <strong>Over-normalization</strong> &mdash; splitting into too many collections like SQL, causing excessive $lookups. (3) <strong>Bloated documents</strong> &mdash; embedding data that's rarely accessed. (4) <strong>No validation</strong> &mdash; flexible schema doesn't mean no rules, use $jsonSchema.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 3 -->
        <div class="concept" id="concept-3">
            <h2>3. Indexing Deep Dive</h2>
            <span class="tag">Performance Critical</span>

            <div class="point">
                <strong>Why Indexes?</strong>
                <p>Without index = <strong>collection scan</strong> (reads every document). With index = B-Tree lookup, jumps directly to matching docs. Like a book's table of contents.</p>
            </div>

            <div class="point">
                <strong>Default Index</strong>
                <p>Every collection has <code>_id</code> index automatically. Cannot be dropped.</p>
            </div>

            <div class="point">
                <strong>Index Types</strong>
                <ul>
                    <li><strong>Single Field</strong> &mdash; index on one field</li>
                    <li><strong>Compound</strong> &mdash; index on multiple fields (order matters!)</li>
                    <li><strong>Multikey</strong> &mdash; auto-created when indexing array fields</li>
                    <li><strong>Text</strong> &mdash; full-text search on string fields</li>
                    <li><strong>Hashed</strong> &mdash; for hash-based sharding</li>
                    <li><strong>Geospatial</strong> &mdash; 2d / 2dsphere for location queries</li>
                    <li><strong>Wildcard</strong> &mdash; index on dynamic/unknown field names</li>
                    <li><strong>TTL</strong> &mdash; auto-delete docs after a time period</li>
                </ul>
            </div>

            <div class="point">
                <strong>Creating Indexes</strong>
<pre><code>// Single field
db.patients.createIndex({ name: 1 })          // 1 = ascending
db.patients.createIndex({ age: -1 })          // -1 = descending

// Compound index (order matters!)
db.patients.createIndex({ status: 1, age: -1 })

// Unique index
db.patients.createIndex({ email: 1 }, { unique: true })

// TTL index - auto-delete after 30 days
db.sessions.createIndex({ createdAt: 1 }, { expireAfterSeconds: 2592000 })

// Text index
db.products.createIndex({ name: "text", description: "text" })

// Partial index - only index active patients
db.patients.createIndex(
  { name: 1 },
  { partialFilterExpression: { status: "active" } }
)

// Wildcard index - for dynamic fields
db.products.createIndex({ "attributes.$**": 1 })</code></pre>
            </div>

            <div class="point">
                <strong>Compound Index - ESR Rule (Equality, Sort, Range)</strong>
                <p>Order fields in compound index as: <strong>Equality</strong> first, then <strong>Sort</strong>, then <strong>Range</strong>.</p>
<pre><code>// Query: find active patients aged > 30, sorted by name
db.patients.find({ status: "active", age: { $gt: 30 } }).sort({ name: 1 })

// Best index (ESR rule):
db.patients.createIndex({ status: 1, name: 1, age: 1 })
//                        Equality     Sort     Range</code></pre>
            </div>

            <div class="point">
                <strong>Covered Query (fastest possible)</strong>
                <p>Query answered entirely from index, no need to fetch document. All queried + returned fields must be in the index.</p>
<pre><code>// Index
db.patients.createIndex({ status: 1, name: 1 })

// Covered query - returns only indexed fields, _id excluded
db.patients.find(
  { status: "active" },
  { name: 1, _id: 0 }
)
// explain() will show: totalDocsExamined: 0</code></pre>
            </div>

            <div class="point">
                <strong>explain() - How to Check Index Usage</strong>
<pre><code>db.patients.find({ name: "Rajesh" }).explain("executionStats")

// Key things to check:
// winningPlan.stage: "IXSCAN" = using index, "COLLSCAN" = bad!
// totalKeysExamined: how many index entries scanned
// totalDocsExamined: how many docs fetched
// executionTimeMillis: query time

// Goal: keysExamined ≈ docsExamined ≈ nReturned</code></pre>
            </div>

            <div class="point">
                <strong>Index Properties</strong>
                <ul>
                    <li><strong>Unique</strong> &mdash; no duplicate values allowed</li>
                    <li><strong>Sparse</strong> &mdash; only index docs that have the field (skip nulls)</li>
                    <li><strong>Partial</strong> &mdash; index only docs matching a filter (more flexible than sparse)</li>
                    <li><strong>TTL</strong> &mdash; auto-expire documents after X seconds</li>
                    <li><strong>Hidden</strong> &mdash; exists but not used by planner (for testing before dropping)</li>
                </ul>
            </div>

            <div class="point">
                <strong>Multikey Index (arrays)</strong>
<pre><code>// Document
{ name: "Rajesh", codes: ["E11", "I10", "M19"] }

// Index on array field - MongoDB creates index entry for EACH element
db.patients.createIndex({ codes: 1 })

// Now this is fast:
db.patients.find({ codes: "E11" })

// Limitation: compound index can have only ONE array field</code></pre>
            </div>

            <div class="point">
                <strong>Index Costs (trade-offs)</strong>
                <ul>
                    <li>Each index <strong>slows down writes</strong> &mdash; every insert/update must update all indexes</li>
                    <li>Indexes use <strong>RAM</strong> &mdash; working set should fit in memory</li>
                    <li>Too many indexes = slow writes + wasted memory</li>
                    <li>Rule: max <strong>5-10 indexes</strong> per collection for write-heavy workloads</li>
                </ul>
            </div>

            <div class="point">
                <strong>Manage Indexes</strong>
<pre><code>db.patients.getIndexes()                    // list all
db.patients.dropIndex("name_1")             // drop by name
db.patients.dropIndexes()                   // drop all except _id
db.patients.hideIndex("name_1")             // hide (test before drop)
db.patients.unhideIndex("name_1")           // unhide</code></pre>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How did you optimize query performance for healthcare data?</div>
                    <div class="answer">Used <strong>compound indexes</strong> following ESR rule. For the most common query (find patients by status + date range sorted by name), created index <code>{ status: 1, name: 1, admitDate: 1 }</code>. Used <strong>partial indexes</strong> to only index active records, saving 60% index memory. Used <strong>explain()</strong> to verify IXSCAN and minimize docsExamined.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is the ESR rule?</div>
                    <div class="answer">Order compound index fields as: <strong>Equality</strong> (exact match fields first), <strong>Sort</strong> (sort fields next), <strong>Range</strong> ($gt, $lt, $in last). This lets MongoDB traverse the index optimally &mdash; narrow by equality, walk in sort order, then filter range.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is a Covered Query?</div>
                    <div class="answer">A query answered <strong>entirely from the index</strong> without touching documents. All query conditions AND projected fields must be in the index. Fastest possible query. In explain(), <code>totalDocsExamined: 0</code>. Must exclude _id with <code>{ _id: 0 }</code> unless _id is part of the index.</div>
                </div>

                <div class="q">
                    <div class="question">Q: TTL Index - how did you use it?</div>
                    <div class="answer">Used TTL index on session and temporary token collections to <strong>auto-delete expired docs</strong>. <code>createIndex({ createdAt: 1 }, { expireAfterSeconds: 86400 })</code> deletes docs 24 hours after creation. MongoDB background thread checks every 60 seconds. No cron job needed.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Multikey index limitation?</div>
                    <div class="answer">MongoDB creates one index entry <strong>per array element</strong>. Limitation: a compound index can have <strong>at most one array field</strong>. If both fields are arrays, MongoDB rejects the index (Cartesian product too large). For the medical codes array at CorroHealth, we index <code>{ codes: 1 }</code> to quickly find patients by ICD code.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How many indexes is too many?</div>
                    <div class="answer">Each index slows writes and uses RAM. Rule of thumb: <strong>5-10 indexes</strong> for write-heavy, up to 15-20 for read-heavy. Monitor with <code>db.collection.stats()</code> &mdash; check <code>totalIndexSize</code> vs available RAM. If working set doesn't fit in memory, performance degrades sharply.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Partial vs Sparse index?</div>
                    <div class="answer"><strong>Sparse</strong> only indexes docs where the field exists (skips null/missing). <strong>Partial</strong> is more flexible &mdash; index docs matching any filter expression like <code>{ status: "active" }</code>. Partial is preferred. Example: index only active patients saves 70% space if 70% of records are archived.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to safely remove an unused index in production?</div>
                    <div class="answer">Step 1: <code>hideIndex()</code> &mdash; makes planner ignore it without dropping. Step 2: Monitor for a week &mdash; if no performance regression, it's safe. Step 3: <code>dropIndex()</code>. Never drop directly in production &mdash; hiding is reversible, dropping is not.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 4 -->
        <div class="concept" id="concept-4">
            <h2>4. Aggregation Framework</h2>
            <span class="tag">Heavy Interview Topic</span>

            <div class="point">
                <strong>What is it?</strong>
                <p>Pipeline of stages that process documents. Each stage transforms data and passes result to next stage. Like Unix pipes: <code>data | $match | $group | $sort | result</code></p>
            </div>

            <div class="point">
                <strong>Core Stages (must know)</strong>
                <ul>
                    <li><strong>$match</strong> &mdash; filter docs (like WHERE). Put early to reduce data.</li>
                    <li><strong>$group</strong> &mdash; group by field + apply accumulators (like GROUP BY)</li>
                    <li><strong>$project</strong> &mdash; reshape docs, include/exclude/compute fields (like SELECT)</li>
                    <li><strong>$sort</strong> &mdash; order results</li>
                    <li><strong>$limit / $skip</strong> &mdash; pagination</li>
                    <li><strong>$unwind</strong> &mdash; flatten an array (1 doc per array element)</li>
                    <li><strong>$lookup</strong> &mdash; join with another collection</li>
                    <li><strong>$addFields</strong> &mdash; add new computed fields (keeps existing)</li>
                    <li><strong>$count</strong> &mdash; count total docs</li>
                    <li><strong>$facet</strong> &mdash; run multiple pipelines in parallel on same data</li>
                </ul>
            </div>

            <div class="point">
                <strong>$group Accumulators</strong>
                <ul>
                    <li><strong>$sum</strong>, <strong>$avg</strong>, <strong>$min</strong>, <strong>$max</strong> &mdash; math operations</li>
                    <li><strong>$first</strong>, <strong>$last</strong> &mdash; first/last value in group</li>
                    <li><strong>$push</strong> &mdash; collect values into array</li>
                    <li><strong>$addToSet</strong> &mdash; collect unique values into array</li>
                    <li><strong>$count</strong> &mdash; count docs in group</li>
                </ul>
            </div>

            <div class="point">
                <strong>Example 1: Patient stats by diagnosis</strong>
<pre><code>db.patients.aggregate([
  { $match: { status: "active" } },
  { $unwind: "$codes" },
  { $group: {
      _id: "$codes",
      count: { $sum: 1 },
      avgAge: { $avg: "$age" },
      patients: { $push: "$name" }
  }},
  { $sort: { count: -1 } },
  { $limit: 10 }
])
// Output: top 10 diagnosis codes with patient count and avg age</code></pre>
            </div>

            <div class="point">
                <strong>Example 2: Monthly revenue (eCommerce)</strong>
<pre><code>db.orders.aggregate([
  { $match: { status: "completed", year: 2024 } },
  { $group: {
      _id: { month: "$month", category: "$category" },
      revenue: { $sum: "$total" },
      orders: { $sum: 1 },
      avgOrder: { $avg: "$total" }
  }},
  { $sort: { "_id.month": 1 } },
  { $project: {
      month: "$_id.month",
      category: "$_id.category",
      revenue: 1,
      orders: 1,
      avgOrder: { $round: ["$avgOrder", 2] },
      _id: 0
  }}
])</code></pre>
            </div>

            <div class="point">
                <strong>$unwind (flatten arrays)</strong>
<pre><code>// Before unwind
{ name: "Rajesh", codes: ["E11", "I10", "M19"] }

// After { $unwind: "$codes" }
{ name: "Rajesh", codes: "E11" }
{ name: "Rajesh", codes: "I10" }
{ name: "Rajesh", codes: "M19" }

// Preserve empty arrays (don't lose docs without the field)
{ $unwind: { path: "$codes", preserveNullAndEmptyArrays: true } }</code></pre>
            </div>

            <div class="point">
                <strong>$lookup (JOIN)</strong>
<pre><code>// Simple lookup
{ $lookup: {
    from: "doctors",
    localField: "doctorId",
    foreignField: "_id",
    as: "doctor"
}}

// Pipeline lookup (more powerful - add conditions)
{ $lookup: {
    from: "lab_reports",
    let: { pid: "$_id" },
    pipeline: [
      { $match: { $expr: { $eq: ["$patientId", "$$pid"] } } },
      { $match: { type: "blood_test" } },
      { $sort: { date: -1 } },
      { $limit: 5 }
    ],
    as: "recentLabs"
}}</code></pre>
            </div>

            <div class="point">
                <strong>$facet (multiple pipelines at once)</strong>
<pre><code>// Get total count + paginated results + category breakdown in ONE query
db.products.aggregate([
  { $match: { status: "active" } },
  { $facet: {
      totalCount: [{ $count: "count" }],
      results: [{ $skip: 20 }, { $limit: 10 }],
      byCategory: [
        { $group: { _id: "$category", count: { $sum: 1 } } }
      ]
  }}
])</code></pre>
            </div>

            <div class="point">
                <strong>$bucket / $bucketAuto (histogram)</strong>
<pre><code>// Group patients by age ranges
db.patients.aggregate([
  { $bucket: {
      groupBy: "$age",
      boundaries: [0, 18, 30, 50, 70, 120],
      default: "unknown",
      output: {
        count: { $sum: 1 },
        names: { $push: "$name" }
      }
  }}
])
// Output: { _id: 0, count: 5 }, { _id: 18, count: 12 }, ...</code></pre>
            </div>

            <div class="point">
                <strong>Performance Tips</strong>
                <ul>
                    <li><strong>$match early</strong> &mdash; filter first, reduces data for later stages</li>
                    <li><strong>$match + $sort at start</strong> can use indexes</li>
                    <li>After $group/$project, indexes can't be used</li>
                    <li><strong>allowDiskUse: true</strong> for large datasets (default 100MB RAM limit per stage)</li>
                    <li>Use <strong>$project</strong> early to drop unnecessary fields</li>
                </ul>
<pre><code>// Allow disk use for large aggregations
db.patients.aggregate([...], { allowDiskUse: true })</code></pre>
            </div>

            <div class="point">
                <strong>$out / $merge (save results)</strong>
<pre><code>// $out - replace entire collection with results
{ $out: "monthly_reports" }

// $merge - upsert results into existing collection (better)
{ $merge: {
    into: "monthly_reports",
    on: "_id",
    whenMatched: "replace",
    whenNotMatched: "insert"
}}</code></pre>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How did you use aggregation for clickstream intelligence at Beckett?</div>
                    <div class="answer">Built pipelines to analyze user browsing patterns: <code>$match</code> by date range, <code>$group</code> by productId to count views/clicks, <code>$sort</code> by engagement score, <code>$lookup</code> to join product details. Used <code>$facet</code> to get trending products + category breakdown + hourly patterns in a single query. Results saved with <code>$merge</code> into a precomputed analytics collection.</div>
                </div>

                <div class="q">
                    <div class="question">Q: $match at beginning vs end - does it matter?</div>
                    <div class="answer">Yes, hugely. <strong>$match at the start</strong> can use indexes and reduces docs flowing through the pipeline. $match after $group or $project <strong>cannot use indexes</strong> and processes all docs. Always filter as early as possible. Same for $sort &mdash; at the start it uses indexes, later it sorts in memory.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is $facet and when to use it?</div>
                    <div class="answer">Runs <strong>multiple sub-pipelines in parallel</strong> on the same input data. Perfect for search result pages &mdash; get paginated results + total count + category filters in ONE database call instead of three. Each sub-pipeline gets the full input and produces independent output.</div>
                </div>

                <div class="q">
                    <div class="question">Q: $unwind with preserveNullAndEmptyArrays?</div>
                    <div class="answer">By default, $unwind <strong>drops documents</strong> where the array is empty or missing. <code>preserveNullAndEmptyArrays: true</code> keeps those docs with the field set to null. Important when you don't want to lose data &mdash; e.g., patients without diagnosis codes should still appear in reports.</div>
                </div>

                <div class="q">
                    <div class="question">Q: $out vs $merge - which to use?</div>
                    <div class="answer"><code>$out</code> replaces the entire target collection (destructive). <code>$merge</code> does upsert &mdash; update existing docs, insert new ones. <strong>Always prefer $merge</strong> in production. $merge also works with sharded collections and allows configuring whenMatched/whenNotMatched behavior.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Pipeline vs simple lookup - what's the difference?</div>
                    <div class="answer">Simple lookup matches localField to foreignField (basic equality join). <strong>Pipeline lookup</strong> lets you add $match, $sort, $limit inside the join &mdash; e.g., "get only last 5 blood tests for this patient." Pipeline lookup is more flexible but slightly slower. Use it when you need filtered/sorted joins.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What happens when aggregation exceeds memory?</div>
                    <div class="answer">Each stage has <strong>100MB RAM limit</strong> by default. Exceeded = error. Fix: (1) <code>allowDiskUse: true</code> to spill to disk. (2) $match early to reduce data. (3) $project to drop fields. Note: allowDiskUse is slower since it uses disk I/O. For production analytics, consider precomputing with $merge.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Write an aggregation for "top 5 doctors by patient count this month"</div>
                    <div class="answer"><pre><code>db.patients.aggregate([
  { $match: {
      admitDate: {
        $gte: ISODate("2024-12-01"),
        $lt: ISODate("2025-01-01")
      }
  }},
  { $group: { _id: "$doctorId", count: { $sum: 1 } } },
  { $sort: { count: -1 } },
  { $limit: 5 },
  { $lookup: {
      from: "doctors",
      localField: "_id",
      foreignField: "_id",
      as: "doctor"
  }},
  { $unwind: "$doctor" },
  { $project: {
      name: "$doctor.name",
      patientCount: "$count", _id: 0
  }}
])</code></pre></div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 5 -->
        <div class="concept" id="concept-5">
            <h2>5. Sharding & Horizontal Scaling</h2>
            <span class="tag">Architecture Level</span>

            <div class="point">
                <strong>What is Sharding?</strong>
                <p>Splitting data across multiple servers (shards). Each shard holds a subset of data. Enables <strong>horizontal scaling</strong> &mdash; add more machines instead of bigger machine.</p>
            </div>

            <div class="point">
                <strong>Sharded Cluster Components</strong>
                <ul>
                    <li><strong>Shard</strong> &mdash; each shard is a replica set holding a portion of data</li>
                    <li><strong>mongos</strong> &mdash; query router. App connects to mongos, not directly to shards</li>
                    <li><strong>Config Servers</strong> &mdash; store metadata (which data is on which shard). Also a replica set</li>
                </ul>
<pre><code>// Architecture:
App &rarr; mongos (router) &rarr; Shard 1 (replica set)
                         &rarr; Shard 2 (replica set)
                         &rarr; Shard 3 (replica set)
         Config Servers (replica set) - stores chunk mappings</code></pre>
            </div>

            <div class="point">
                <strong>Shard Key (most important decision)</strong>
                <p>The field used to distribute data across shards. <strong>Cannot be changed after sharding</strong> (until MongoDB 5.0 resharding). Choose carefully!</p>
<pre><code>// Shard a collection
sh.enableSharding("healthcare_db")
sh.shardCollection("healthcare_db.patients", { region: 1 })
//                                             ^ shard key</code></pre>
            </div>

            <div class="point">
                <strong>Sharding Strategies</strong>
                <ul>
                    <li><strong>Ranged Sharding</strong> &mdash; consecutive values go to same shard. Good for range queries. Risk: hotspot if writes are sequential (e.g., timestamps)</li>
                    <li><strong>Hashed Sharding</strong> &mdash; hash of shard key distributes evenly. Good write distribution. Bad for range queries (scatter-gather)</li>
                    <li><strong>Zone Sharding</strong> &mdash; assign data ranges to specific shards. Good for geo/compliance (e.g., India data stays on India shard)</li>
                </ul>
<pre><code>// Hashed sharding
sh.shardCollection("db.orders", { orderId: "hashed" })

// Zone sharding (geo-based)
sh.addShardTag("shard1", "INDIA")
sh.addShardTag("shard2", "US")
sh.addTagRange("db.patients", { region: "IN" }, { region: "IN\uffff" }, "INDIA")
sh.addTagRange("db.patients", { region: "US" }, { region: "US\uffff" }, "US")</code></pre>
            </div>

            <div class="point">
                <strong>Good Shard Key Properties</strong>
                <ul>
                    <li><strong>High cardinality</strong> &mdash; many unique values (NOT status: "active/inactive")</li>
                    <li><strong>Even distribution</strong> &mdash; data spread equally across shards</li>
                    <li><strong>Query isolation</strong> &mdash; most queries include shard key (targeted queries)</li>
                    <li><strong>Not monotonically increasing</strong> &mdash; avoid _id, timestamps (causes hotspot on last shard)</li>
                </ul>
            </div>

            <div class="point">
                <strong>Targeted vs Scatter-Gather Queries</strong>
                <ul>
                    <li><strong>Targeted</strong> &mdash; query includes shard key, mongos routes to ONE shard. Fast.</li>
                    <li><strong>Scatter-Gather</strong> &mdash; query without shard key, mongos asks ALL shards, merges results. Slow.</li>
                </ul>
<pre><code>// Shard key: { region: 1 }

// TARGETED - includes shard key
db.patients.find({ region: "IN", name: "Rajesh" })  // goes to 1 shard

// SCATTER-GATHER - no shard key
db.patients.find({ name: "Rajesh" })  // asks ALL shards</code></pre>
            </div>

            <div class="point">
                <strong>Chunks & Balancer</strong>
                <ul>
                    <li><strong>Chunk</strong> &mdash; contiguous range of shard key values. Default max size: 128MB</li>
                    <li><strong>Splitting</strong> &mdash; when chunk exceeds max size, MongoDB splits it into two</li>
                    <li><strong>Balancer</strong> &mdash; background process that moves chunks between shards to keep data even</li>
                    <li>Balancer runs on config server, can be scheduled to off-peak hours</li>
                </ul>
            </div>

            <div class="point">
                <strong>When to Shard</strong>
                <ul>
                    <li>Single server can't handle data volume (disk full)</li>
                    <li>Single server can't handle throughput (CPU/RAM maxed)</li>
                    <li>Working set doesn't fit in RAM</li>
                    <li>Typically at <strong>1TB+</strong> data or <strong>50K+ ops/sec</strong></li>
                </ul>
                <p>Don't shard too early &mdash; adds complexity. Start with replica set, shard when needed.</p>
            </div>

            <div class="point">
                <strong>Sharding Limitations</strong>
                <ul>
                    <li>Unique indexes must include the shard key</li>
                    <li>$lookup only works on unsharded or same-shard collections (before 5.1)</li>
                    <li>Transactions across shards are slower than single-shard</li>
                    <li>Resharding (changing shard key) possible since 5.0 but expensive</li>
                </ul>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How would you shard the healthcare patient collection?</div>
                    <div class="answer">Use <strong>compound shard key</strong>: <code>{ region: 1, patientId: "hashed" }</code>. Region provides query isolation (most queries filter by hospital/region) and enables zone sharding for data compliance. Hashed patientId ensures even distribution within each region. Avoids hotspot since patientId is random.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Why not use _id as shard key?</div>
                    <div class="answer">ObjectId is <strong>monotonically increasing</strong> (starts with timestamp). All new writes go to the last shard &mdash; creating a <strong>hotspot</strong>. One shard gets all the writes while others sit idle. Use hashed _id if you must: <code>{ _id: "hashed" }</code>, but then range queries on _id become scatter-gather.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Targeted vs Scatter-Gather query?</div>
                    <div class="answer"><strong>Targeted</strong>: query includes shard key, goes to one shard. O(1) routing. <strong>Scatter-Gather</strong>: no shard key in query, mongos asks all shards, merges. O(n) where n = number of shards. Always design queries to include shard key. If 80% of queries use region, shard by region.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Ranged vs Hashed sharding &mdash; when to use each?</div>
                    <div class="answer"><strong>Ranged</strong>: when you need range queries on shard key (date ranges, alphabetical). Risk: uneven distribution. <strong>Hashed</strong>: when you need even write distribution. Risk: range queries become scatter-gather. For eCommerce orders, used hashed on orderId for even writes. For analytics by date, used ranged on date.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is the Balancer and can it cause issues?</div>
                    <div class="answer">Background process that <strong>migrates chunks</strong> between shards for even distribution. Can cause performance impact during migration (network + disk I/O). Best practice: schedule balancer window during off-peak hours using <code>sh.setBalancerState()</code> and balancer window configuration. Monitor with <code>sh.status()</code>.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is Zone Sharding and when did you use it?</div>
                    <div class="answer">Assigns data ranges to specific shards based on tags. Used for <strong>data residency compliance</strong> &mdash; India patient data must stay on India servers. Tag shard1 as "INDIA", define zone range for region "IN". Also useful for tiered storage &mdash; hot data on SSD shards, cold data on HDD shards.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Can you change the shard key after sharding?</div>
                    <div class="answer">Before 5.0: <strong>No</strong>. Had to create new collection, reshard, migrate. Since MongoDB 5.0: <strong>reshardCollection</strong> command allows changing shard key online. It creates a temp collection, copies data, does a fast cutover. Still expensive on large collections &mdash; plan during maintenance window.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How did you design sharding for the eCommerce collectibles platform?</div>
                    <div class="answer">Products collection: <code>{ category: 1, _id: "hashed" }</code> &mdash; category gives targeted queries (users browse by category), hashed _id distributes evenly within category. Orders collection: <code>{ userId: "hashed" }</code> &mdash; even distribution, user's orders go to same shard (targeted lookup). Clickstream: <code>{ date: 1 }</code> ranged &mdash; analytics queries always filter by date range.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 6 -->
        <div class="concept" id="concept-6">
            <h2>6. Replication & High Availability</h2>
            <span class="tag">SRE / Reliability</span>

            <div class="point">
                <strong>What is a Replica Set?</strong>
                <p>Group of mongod instances with the same data. One <strong>Primary</strong> (reads+writes) and multiple <strong>Secondaries</strong> (replicate from primary). Automatic failover.</p>
            </div>

            <div class="point">
                <strong>Replica Set Members</strong>
                <ul>
                    <li><strong>Primary</strong> &mdash; receives all writes. Only one at a time.</li>
                    <li><strong>Secondary</strong> &mdash; replicates from primary's oplog. Can serve reads if configured.</li>
                    <li><strong>Arbiter</strong> &mdash; votes in elections but holds NO data. Use to break tie (avoid if possible).</li>
                    <li><strong>Hidden</strong> &mdash; invisible to app, won't become primary. Used for backups/analytics.</li>
                    <li><strong>Delayed</strong> &mdash; replicates with a time delay (e.g., 1 hour behind). Protection against accidental deletes.</li>
                </ul>
<pre><code>// Typical setup: 3 members (1 primary + 2 secondaries)
// Minimum for automatic failover: 3 voting members

rs.initiate({
  _id: "healthcareRS",
  members: [
    { _id: 0, host: "mongo1:27017" },
    { _id: 1, host: "mongo2:27017" },
    { _id: 2, host: "mongo3:27017" }
  ]
})</code></pre>
            </div>

            <div class="point">
                <strong>Oplog (Operations Log)</strong>
                <ul>
                    <li>Capped collection on primary: <code>local.oplog.rs</code></li>
                    <li>Records every write operation</li>
                    <li>Secondaries tail the oplog and replay operations</li>
                    <li>Idempotent &mdash; safe to replay multiple times</li>
                    <li>Size matters &mdash; if secondary falls behind more than oplog window, needs full resync</li>
                </ul>
            </div>

            <div class="point">
                <strong>Election & Failover</strong>
                <ul>
                    <li>If primary goes down, secondaries hold an <strong>election</strong></li>
                    <li>Needs <strong>majority vote</strong> (2 out of 3, 3 out of 5)</li>
                    <li>Election takes <strong>~10-12 seconds</strong> (configurable with electionTimeoutMillis)</li>
                    <li>During election, <strong>no writes</strong> are accepted</li>
                    <li>Priority setting controls which member is preferred as primary</li>
                </ul>
<pre><code>// Set priority (higher = preferred primary)
cfg = rs.conf()
cfg.members[0].priority = 10  // strongly prefer this node
cfg.members[1].priority = 5
cfg.members[2].priority = 1
rs.reconfig(cfg)

// Check replica set status
rs.status()    // shows who is primary, replication lag
rs.printReplicationInfo()  // oplog size and window</code></pre>
            </div>

            <div class="point">
                <strong>Write Concern (how many nodes confirm write)</strong>
                <ul>
                    <li><code>w: 1</code> &mdash; primary only (fast, risk of data loss on failover)</li>
                    <li><code>w: "majority"</code> &mdash; majority of replica set (safe, slightly slower)</li>
                    <li><code>w: 3</code> &mdash; all 3 members confirm</li>
                    <li><code>j: true</code> &mdash; wait for journal write (most durable)</li>
                </ul>
<pre><code>// Healthcare: maximum durability
db.patients.insertOne(
  { name: "Rajesh", codes: ["E11"] },
  { writeConcern: { w: "majority", j: true, wtimeout: 5000 } }
)</code></pre>
            </div>

            <div class="point">
                <strong>Read Concern (consistency level for reads)</strong>
                <ul>
                    <li><strong>"local"</strong> &mdash; returns most recent data on node (may be rolled back)</li>
                    <li><strong>"available"</strong> &mdash; like local, for sharded collections</li>
                    <li><strong>"majority"</strong> &mdash; returns data acknowledged by majority (won't be rolled back)</li>
                    <li><strong>"linearizable"</strong> &mdash; strongest. Waits for all prior majority writes. Slow.</li>
                    <li><strong>"snapshot"</strong> &mdash; for multi-document transactions</li>
                </ul>
            </div>

            <div class="point">
                <strong>Replication Lag</strong>
                <p>Time difference between primary's latest write and secondary's latest applied op. Monitor with:</p>
<pre><code>rs.printSecondaryReplicationInfo()
// Output shows lag in seconds for each secondary

// Or check in Atlas: Replication Lag chart
// Alert threshold: typically > 10 seconds = warning</code></pre>
            </div>

            <div class="point">
                <strong>Rollback</strong>
                <ul>
                    <li>Happens when old primary had writes not replicated to majority before failover</li>
                    <li>When old primary rejoins, it rolls back those unreplicated writes</li>
                    <li>Rolled-back data saved to <code>rollback/</code> directory</li>
                    <li><strong>Prevention:</strong> use <code>w: "majority"</code> for all critical writes</li>
                </ul>
            </div>

            <div class="point">
                <strong>Best Practices</strong>
                <ul>
                    <li>Always odd number of voting members (3, 5, 7 &mdash; max 7 voters)</li>
                    <li>Max 50 members total in a replica set</li>
                    <li>Spread members across data centers / AZs</li>
                    <li>Use <code>w: "majority"</code> for important data</li>
                    <li>Monitor replication lag, set alerts</li>
                    <li>Use hidden member for backups, delayed member for disaster recovery</li>
                </ul>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How did you ensure high availability for healthcare systems?</div>
                    <div class="answer">Used <strong>3-member replica set</strong> across AWS availability zones. Write concern <code>w: "majority", j: true</code> for all patient data. Hidden member for nightly backups. Delayed member (1 hour) for accidental delete recovery. Monitoring replication lag with alerts at >5 seconds. Achieved <strong>99.99% uptime</strong>.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What happens when the primary goes down?</div>
                    <div class="answer">Remaining secondaries hold an <strong>election</strong> (needs majority vote). New primary elected in ~10-12 seconds. During election, writes are rejected (app retries). Reads can continue from secondaries if readPreference is secondaryPreferred. Once new primary is elected, app reconnects automatically via connection string.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is the oplog and why does its size matter?</div>
                    <div class="answer">Oplog is a <strong>capped collection</strong> recording all writes. Secondaries tail it to stay in sync. If a secondary is offline longer than the <strong>oplog window</strong> (time it takes for oplog to fill), it can't catch up and needs full resync (hours/days for large datasets). Size oplog to cover at least 24-48 hours of writes.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Write Concern "majority" vs w:1 &mdash; trade-off?</div>
                    <div class="answer"><code>w:1</code>: fast (acknowledged by primary only) but risk of <strong>data loss</strong> if primary crashes before replicating. <code>w:"majority"</code>: slightly slower (~1-5ms more) but data <strong>survives any single node failure</strong>. For healthcare/financial data, always use majority. For logs/analytics, w:1 is acceptable.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is a rollback and how to prevent it?</div>
                    <div class="answer">Rollback occurs when old primary had writes <strong>not replicated to majority</strong> before failover. When it rejoins as secondary, those writes are rolled back. <strong>Prevention:</strong> use <code>w: "majority"</code> &mdash; guarantees data is on majority before acknowledging. Rolled-back data is saved to files for manual recovery.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Why use a Hidden or Delayed member?</div>
                    <div class="answer"><strong>Hidden:</strong> never becomes primary, invisible to app. Perfect for running backups, analytics queries, or reporting without impacting production. <strong>Delayed:</strong> replicates X hours behind. If someone drops a collection, you have a window to recover from the delayed member before the drop propagates.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Arbiter vs adding a third data-bearing member?</div>
                    <div class="answer">Arbiter only votes, holds no data. Cheaper but <strong>risky</strong> &mdash; if one data member fails, you have only 1 copy of data. Always prefer 3 data-bearing members. Arbiter is a last resort for cost-sensitive environments. MongoDB docs recommend against arbiters for production.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to handle replica set across regions/AZs?</div>
                    <div class="answer">Spread members across AZs: primary + 1 secondary in primary region, 1 secondary in DR region. Set <strong>priority</strong> so primary stays in main region. If main region goes down, DR secondary gets elected. Use <code>readPreference: "nearest"</code> for geo-distributed reads. For healthcare, this ensures data survives entire AZ outage.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 7 -->
        <div class="concept" id="concept-7">
            <h2>7. MongoDB Atlas & Cloud</h2>
            <span class="tag">Cloud / Managed Service</span>

            <div class="point">
                <strong>What is Atlas?</strong>
                <p>MongoDB's fully managed cloud database service. Handles provisioning, patching, backups, scaling &mdash; you focus on code. Available on AWS, Azure, GCP.</p>
            </div>

            <div class="point">
                <strong>Cluster Tiers</strong>
                <p><strong>M0/M2/M5:</strong> Free/Shared tiers for dev/POC. <strong>M10+:</strong> Dedicated clusters with full features. <strong>M30+:</strong> For production. <strong>Serverless:</strong> Pay-per-operation, auto-scales to zero.</p>
                <pre>// Connection string pattern
mongodb+srv://user:pass@cluster0.abc123.mongodb.net/myDB?retryWrites=true&w=majority</pre>
            </div>

            <div class="point">
                <strong>Atlas Search (Lucene-based)</strong>
                <p>Full-text search engine built into Atlas. No separate Elasticsearch needed. Create search indexes, query with <code>$search</code> aggregation stage.</p>
                <pre>// Create search index (Atlas UI or API)
{
  "mappings": {
    "dynamic": true,
    "fields": {
      "diagnosis": { "type": "string", "analyzer": "lucene.standard" }
    }
  }
}

// Query with $search
db.patients.aggregate([
  { $search: {
      index: "default",
      text: { query: "diabetes type 2", path: "diagnosis" }
  }},
  { $limit: 10 },
  { $project: { diagnosis: 1, score: { $meta: "searchScore" } } }
])</pre>
            </div>

            <div class="point">
                <strong>Change Streams</strong>
                <p>Real-time event-driven pipeline. Watch insert/update/delete on collection, DB, or entire deployment. Built on oplog. Perfect for syncing caches, triggering notifications.</p>
                <pre>// Watch a collection for changes
const pipeline = [{ $match: { operationType: "insert" } }];
const changeStream = db.collection("orders").watch(pipeline);

changeStream.on("change", (event) => {
  console.log("New order:", event.fullDocument);
  // Trigger notification, update cache, sync to Kafka
});</pre>
            </div>

            <div class="point">
                <strong>Atlas Triggers</strong>
                <p>Serverless functions that fire on DB events (insert/update/delete), scheduled (cron), or authentication events. Written in JS, run on Atlas App Services.</p>
                <pre>// Database Trigger example - runs on every new patient insert
exports = function(changeEvent) {
  const patient = changeEvent.fullDocument;
  if (patient.riskScore > 80) {
    // Send alert to care team via external API
    context.http.post({ url: "https://alerts.api/notify", body: patient });
  }
};</pre>
            </div>

            <div class="point">
                <strong>Online Archive</strong>
                <p>Automatically tier cold data from hot cluster to cheaper S3-backed storage. Query both with single connection string. Saves cost on large historical datasets.</p>
                <pre>// Archive rule: move records older than 365 days
{
  "dbName": "hospital",
  "collName": "visits",
  "partitionFields": [
    { "fieldName": "visitDate", "order": 0 }
  ],
  "criteria": {
    "type": "DATE",
    "dateField": "visitDate",
    "dateFormat": "ISODATE",
    "expireAfterDays": 365
  }
}</pre>
            </div>

            <div class="point">
                <strong>Backup & Restore</strong>
                <p><strong>Cloud Backup:</strong> Continuous backup with point-in-time restore (M10+). Snapshots every 6 hours (configurable). Restore to any second within retention window. <strong>M0-M5:</strong> Daily snapshots only.</p>
            </div>

            <div class="point">
                <strong>Performance Advisor & Monitoring</strong>
                <p><strong>Performance Advisor:</strong> Suggests missing indexes based on slow queries. <strong>Real-Time Performance Panel:</strong> Shows ops/sec, connections, network. <strong>Alerts:</strong> Configure thresholds for CPU, disk, oplog window, replication lag.</p>
            </div>

            <!-- Interview Q&A -->
            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: Why choose Atlas over self-managed MongoDB?</div>
                    <div class="answer">Atlas handles infra: automated backups, patching, scaling, monitoring, security (encryption at rest + in transit by default). For healthcare project, Atlas ensured <strong>HIPAA compliance</strong> with encryption, VPC peering, and audit logs out of the box. Team focused on features instead of ops. Self-managed makes sense only when you need full control or have strict on-prem requirements.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Atlas Search vs Elasticsearch &mdash; when to pick which?</div>
                    <div class="answer">Atlas Search: <strong>simpler architecture</strong> (no separate cluster), Lucene-powered, good for 80% of search use cases. Use when search is secondary feature. Elasticsearch: better for <strong>heavy analytics, log aggregation, complex scoring</strong>. In our project, we used Atlas Search for patient diagnosis lookup &mdash; avoided running separate ES cluster, reduced cost and complexity.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How do Change Streams work internally?</div>
                    <div class="answer">Built on top of the <strong>oplog</strong> (replication log). Atlas tails the oplog and pushes matching events to your cursor. Requires replica set or sharded cluster. Supports <strong>resume tokens</strong> &mdash; if your app crashes, reconnect with last token and pick up where you left off. No events lost.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Explain Atlas connection string &mdash; what does +srv do?</div>
                    <div class="answer"><code>mongodb+srv://</code> uses <strong>DNS SRV records</strong> to discover all replica set members automatically. Client doesn't need to list every host. If Atlas adds/removes nodes, DNS updates and client discovers new topology. The <code>retryWrites=true</code> ensures transient failures auto-retry. <code>w=majority</code> ensures writes acknowledged by majority.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How would you handle cost optimization in Atlas?</div>
                    <div class="answer"><strong>1)</strong> Use Online Archive to move cold data to cheap storage. <strong>2)</strong> Right-size cluster tier based on actual workload. <strong>3)</strong> Use auto-scaling (M10+) to scale up during peak, down at off-peak. <strong>4)</strong> Use serverless instances for sporadic workloads. <strong>5)</strong> Optimize indexes &mdash; fewer indexes = less storage and faster writes. <strong>6)</strong> Compress data with zstd (WiredTiger default).</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is Atlas Data Federation?</div>
                    <div class="answer">Query data across Atlas clusters, S3 buckets, and Atlas Online Archive using <strong>single MQL endpoint</strong>. No ETL needed. Use for cross-source analytics &mdash; e.g., join live patient data with archived historical records in one aggregation pipeline.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to ensure high availability in Atlas across regions?</div>
                    <div class="answer">Atlas supports <strong>multi-region clusters</strong>. Configure preferred region for primary, add electable nodes in other regions. If primary region fails, automatic failover to another region. Use <code>readPreference: "nearest"</code> for low-latency reads from closest node. For healthcare, this ensures zero downtime even during regional cloud outages.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Describe Atlas Triggers vs Change Streams &mdash; difference?</div>
                    <div class="answer"><strong>Change Streams:</strong> Client-side cursor that your app code listens to. You manage the listener process. <strong>Atlas Triggers:</strong> Serverless &mdash; Atlas runs your function automatically, no infrastructure to manage. Use Triggers for simple reactions (send email, call API). Use Change Streams when you need complex processing in your own app/service (e.g., sync to Kafka).</div>
                </div>
            </div>
        </div>

    </main>
</div>

<script>
    // Highlight active sidebar link on scroll
    const links = document.querySelectorAll('.sidebar nav a');
    const sections = document.querySelectorAll('.concept');
    window.addEventListener('scroll', () => {
        let current = '';
        sections.forEach(s => {
            if (window.scrollY >= s.offsetTop - 100) current = s.id;
        });
        links.forEach(a => {
            a.classList.remove('active');
            if (a.getAttribute('href') === '#' + current) a.classList.add('active');
        });
    });
</script>
</body>
</html>