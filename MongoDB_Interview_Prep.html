<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MongoDB Interview Prep</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code:wght@400;500&display=swap');
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Inter', sans-serif; background: #f8f9fa; color: #1e293b; line-height: 1.6; }

        .layout { display: flex; min-height: 100vh; }

        /* Sidebar */
        .sidebar {
            width: 260px;
            min-width: 260px;
            background: #fff;
            border-right: 1px solid #e2e8f0;
            padding: 24px 0;
            position: fixed;
            top: 0;
            left: 0;
            height: 100vh;
            overflow-y: auto;
        }
        .sidebar h2 { font-size: 15px; color: #0f766e; padding: 0 20px; margin-bottom: 4px; }
        .sidebar .sub { font-size: 11px; color: #94a3b8; padding: 0 20px; margin-bottom: 20px; }
        .sidebar nav a {
            display: block;
            padding: 9px 20px;
            font-size: 13px;
            color: #475569;
            text-decoration: none;
            border-left: 3px solid transparent;
            transition: all 0.2s;
        }
        .sidebar nav a:hover { background: #f0fdf4; color: #0f766e; border-left-color: #0f766e; }
        .sidebar nav a.active { background: #f0fdf4; color: #0f766e; border-left-color: #0f766e; font-weight: 600; }
        .sidebar nav a.pending { color: #cbd5e1; }

        /* Main content */
        .main { margin-left: 260px; padding: 30px 40px; max-width: 820px; }

        /* Concept block */
        .concept { background: #fff; border-radius: 10px; padding: 28px; margin-bottom: 24px; border: 1px solid #e2e8f0; }
        .concept h2 { font-size: 20px; color: #0f766e; margin-bottom: 4px; }
        .concept .tag { display: inline-block; background: #f0fdf4; color: #0f766e; font-size: 11px; padding: 2px 10px; border-radius: 20px; margin-bottom: 14px; font-weight: 500; }

        /* Simple points */
        .point { margin-bottom: 14px; }
        .point strong { color: #0f766e; font-size: 13px; }
        .point p { font-size: 13px; color: #475569; margin-top: 2px; }
        .point ul { padding-left: 18px; margin-top: 4px; }
        .point li { font-size: 12.5px; color: #475569; margin-bottom: 3px; }
        .point li strong { color: #1e293b; }

        /* Code */
        pre { background: #f1f5f9; border-radius: 8px; padding: 14px; overflow-x: auto; margin: 10px 0; border: 1px solid #e2e8f0; }
        code { font-family: 'Fira Code', monospace; font-size: 12px; color: #0f766e; }

        /* Q&A */
        .qa h3 { font-size: 13px; color: #b45309; margin: 18px 0 10px; text-transform: uppercase; letter-spacing: 1px; border-top: 1px solid #e2e8f0; padding-top: 16px; }
        .q { background: #fffbeb; border-radius: 8px; padding: 12px 14px; margin-bottom: 8px; border: 1px solid #fde68a; }
        .q .question { font-size: 13px; font-weight: 600; color: #1e293b; margin-bottom: 6px; }
        .q .answer { font-size: 12.5px; color: #57534e; }
        .q .answer strong { color: #1e293b; }

        /* Mobile menu button */
        .menu-btn {
            display: none;
            position: fixed;
            top: 12px;
            left: 12px;
            z-index: 1001;
            background: #0f766e;
            color: #fff;
            border: none;
            border-radius: 8px;
            width: 40px;
            height: 40px;
            cursor: pointer;
            font-size: 20px;
            align-items: center;
            justify-content: center;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }
        .menu-btn span { line-height: 1; }

        /* Overlay */
        .sidebar-overlay {
            display: none;
            position: fixed;
            top: 0; left: 0; right: 0; bottom: 0;
            background: rgba(0,0,0,0.4);
            z-index: 999;
        }
        .sidebar-overlay.show { display: block; }

        /* Responsive */
        @media (max-width: 768px) {
            .menu-btn { display: flex; }
            .sidebar {
                display: block;
                transform: translateX(-100%);
                transition: transform 0.3s ease;
                z-index: 1000;
            }
            .sidebar.open { transform: translateX(0); }
            .main { margin-left: 0; padding: 60px 16px 20px; }
            .concept { padding: 18px; }
            pre { padding: 10px; font-size: 11px; }
        }
    </style>
</head>
<body>
<button class="menu-btn" onclick="toggleMenu()" aria-label="Toggle menu"><span>&#9776;</span></button>
<div class="sidebar-overlay" onclick="toggleMenu()"></div>

<div class="layout">

    <!-- Sidebar -->
    <aside class="sidebar">
        <h2>MongoDB Prep</h2>
        <p class="sub">Interview Guide</p>
        <nav>
            <a href="#concept-1" class="active">1. Fundamentals</a>
            <a href="#concept-2">2. Schema Design</a>
            <a href="#concept-3">3. Indexing</a>
            <a href="#concept-4">4. Aggregation</a>
            <a href="#concept-5">5. Sharding</a>
            <a href="#concept-6">6. Replication</a>
            <a href="#concept-7">7. Atlas & Cloud</a>
            <a href="#concept-8">8. Performance Tuning</a>
            <a href="#concept-9">9. Transactions</a>
            <a href="#concept-10">10. Security</a>
            <a href="#concept-11">11. Kafka + MongoDB</a>
            <a href="#concept-12">12. Mongo vs Elastic</a>
            <a href="#concept-13">13. Project Scenarios</a>
        </nav>
    </aside>

    <!-- Main Content -->
    <main class="main">

        <!-- CONCEPT 1 -->
        <div class="concept" id="concept-1">
            <h2>1. Fundamentals & Architecture</h2>
            <span class="tag">Core Foundation</span>

            <div class="point">
                <strong>What is MongoDB?</strong>
                <p>NoSQL document database. Stores data as BSON (Binary JSON) documents. No fixed schema needed.</p>
            </div>

            <div class="point">
                <strong>Structure (vs MySQL)</strong>
                <ul>
                    <li><strong>Database</strong> = Database</li>
                    <li><strong>Collection</strong> = Table</li>
                    <li><strong>Document</strong> = Row (but flexible, no fixed columns)</li>
                    <li><strong>Field</strong> = Column</li>
                    <li><strong>_id</strong> = Primary Key (auto-generated 12-byte ObjectId)</li>
                </ul>
            </div>

            <div class="point">
                <strong>ObjectId (12 bytes)</strong>
                <p>4 bytes timestamp + 5 bytes random + 3 bytes counter. Sortable by creation time.</p>
            </div>

            <div class="point">
                <strong>WiredTiger Engine</strong>
                <ul>
                    <li><strong>Document-level locking</strong> &mdash; multiple writes to different docs at same time</li>
                    <li><strong>Compression</strong> &mdash; Snappy (default), zstd (better ratio)</li>
                    <li><strong>Journal</strong> &mdash; WAL synced every 50ms for crash recovery</li>
                    <li><strong>Checkpoint</strong> &mdash; flushes to disk every 60 seconds</li>
                    <li><strong>Cache</strong> &mdash; uses 50% of (RAM - 1GB)</li>
                </ul>
            </div>

            <div class="point">
                <strong>BSON vs JSON</strong>
                <p>BSON adds types: Date, ObjectId, Decimal128, Int64, BinData. Faster to parse. Max doc = <strong>16MB</strong>.</p>
            </div>

            <div class="point">
                <strong>CRUD Examples</strong>
<pre><code>// INSERT
db.patients.insertOne({ name: "Rajesh", age: 45, codes: ["E11", "I10"] })
db.patients.insertMany([{ name: "A" }, { name: "B" }])

// FIND
db.patients.find({ age: { $gte: 30 } })       // age >= 30
db.patients.find({ "records.bp": "140/90" })   // nested
db.patients.find({ codes: "E11" })             // array contains

// UPDATE
db.patients.updateOne(
  { name: "Rajesh" },
  { $set: { age: 46 }, $push: { codes: "M19" } }
)

// DELETE
db.patients.deleteMany({ age: { $lt: 18 } })</code></pre>
            </div>

            <div class="point">
                <strong>Key Operators (remember these)</strong>
                <ul>
                    <li><strong>Compare:</strong> $eq, $ne, $gt, $gte, $lt, $lte, $in, $nin</li>
                    <li><strong>Logic:</strong> $and, $or, $not, $nor</li>
                    <li><strong>Array:</strong> $all, $elemMatch, $size, $push, $pull, $addToSet</li>
                    <li><strong>Update:</strong> $set, $unset, $inc, $rename, $pop</li>
                    <li><strong>Element:</strong> $exists, $type</li>
                </ul>
            </div>

            <div class="point">
                <strong>Write Concern (durability level)</strong>
                <ul>
                    <li><code>w: 1</code> &mdash; acknowledged by primary only</li>
                    <li><code>w: "majority"</code> &mdash; acknowledged by majority of replica set</li>
                    <li><code>j: true</code> &mdash; wait until written to journal</li>
                </ul>
            </div>

            <div class="point">
                <strong>Read Preference</strong>
                <ul>
                    <li><strong>primary</strong> &mdash; always read from primary (default, strongest consistency)</li>
                    <li><strong>secondary</strong> &mdash; read from secondaries (for analytics, may be stale)</li>
                    <li><strong>nearest</strong> &mdash; lowest latency node</li>
                </ul>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: Why MongoDB over MySQL for medical data?</div>
                    <div class="answer">Medical records are semi-structured &mdash; each encounter has different fields, nested labs, NLP-extracted codes. MongoDB's flexible schema avoids ALTER TABLE. One patient doc embeds vitals + diagnoses + AI-codes as nested objects, <strong>no JOINs needed</strong> at read time.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How does WiredTiger handle concurrency?</div>
                    <div class="answer"><strong>Document-level concurrency</strong> with MVCC (Multi-Version Concurrency Control). Readers see a consistent snapshot without blocking writers. Different from MySQL's row-level locking &mdash; MongoDB is optimistic, no lock waits on different docs.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Max document size? What if data is bigger?</div>
                    <div class="answer"><strong>16MB</strong>. For larger files use <strong>GridFS</strong> (splits into 255KB chunks across fs.files + fs.chunks). Or store files in S3 and keep reference URL in MongoDB.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Explain ObjectId structure.</div>
                    <div class="answer">12 bytes: <strong>4</strong> (unix timestamp) + <strong>5</strong> (random/machine) + <strong>3</strong> (incrementing counter). Unique across distributed systems. Roughly sortable by time.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Difference between insertMany and bulkWrite?</div>
                    <div class="answer"><code>insertMany</code> = batch inserts only. <code>bulkWrite</code> = mixed operations (insert + update + delete) in one request. Use bulkWrite for Kafka event processing &mdash; batch hundreds of price/inventory updates per second.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How does MongoDB ensure durability?</div>
                    <div class="answer">3 layers: (1) <strong>Journal/WAL</strong> synced every 50ms, (2) <strong>Checkpoint</strong> every 60s, (3) <strong>Write Concern</strong> &mdash; <code>w:"majority", j:true</code> for max safety. For healthcare, always use majority + journal.</div>
                </div>

                <div class="q">
                    <div class="question">Q: When to use which Read Preference?</div>
                    <div class="answer"><strong>primary</strong> for real-time medical coding (consistency matters). <strong>secondaryPreferred</strong> for analytics dashboards (slight staleness OK). <strong>nearest</strong> for geo-distributed reads (lowest latency).</div>
                </div>

                <div class="q">
                    <div class="question">Q: BSON vs JSON &mdash; why not just store JSON?</div>
                    <div class="answer">BSON adds types (Date, Decimal128, ObjectId) that JSON lacks. Has length prefixes for O(1) field skipping. Faster encode/decode. Slightly larger on wire but optimized for traversal.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 2 -->
        <div class="concept" id="concept-2">
            <h2>2. Document Modeling & Schema Design</h2>
            <span class="tag">Most Asked Topic</span>

            <div class="point">
                <strong>Golden Rule</strong>
                <p>Design schema based on <strong>how your app reads data</strong>, not how data is related. Opposite of MySQL normalization.</p>
            </div>

            <div class="point">
                <strong>Embedding vs Referencing</strong>
                <ul>
                    <li><strong>Embed</strong> = put related data inside the same document (denormalize)</li>
                    <li><strong>Reference</strong> = store _id link to another collection (like foreign key)</li>
                </ul>
<pre><code>// EMBEDDED - patient with vitals inside
{
  _id: ObjectId("..."),
  name: "Rajesh",
  vitals: [
    { date: "2024-01-10", bp: "140/90", sugar: 280 },
    { date: "2024-01-11", bp: "130/85", sugar: 220 }
  ]
}

// REFERENCED - vitals in separate collection
// patients collection
{ _id: ObjectId("p1"), name: "Rajesh" }

// vitals collection
{ _id: ObjectId("v1"), patientId: ObjectId("p1"), bp: "140/90" }
{ _id: ObjectId("v2"), patientId: ObjectId("p1"), bp: "130/85" }</code></pre>
            </div>

            <div class="point">
                <strong>When to Embed (use most of the time)</strong>
                <ul>
                    <li>Data is read together &mdash; "give me patient + vitals in one query"</li>
                    <li>1:1 relationship (user &rarr; profile)</li>
                    <li>1:Few relationship (order &rarr; 5-10 items)</li>
                    <li>Data doesn't grow unbounded</li>
                    <li>Atomic updates needed on parent + child</li>
                </ul>
            </div>

            <div class="point">
                <strong>When to Reference</strong>
                <ul>
                    <li>1:Many with unbounded growth (user &rarr; millions of logs)</li>
                    <li>Many:Many relationship (students &harr; courses)</li>
                    <li>Data accessed independently ("show me just the order, not user")</li>
                    <li>Document would exceed 16MB</li>
                    <li>Data shared across many parents</li>
                </ul>
            </div>

            <div class="point">
                <strong>Schema Design Patterns (important!)</strong>
                <ul>
                    <li><strong>Bucket Pattern</strong> &mdash; group time-series data into buckets (e.g., 1 doc = 1 hour of sensor readings)</li>
                    <li><strong>Outlier Pattern</strong> &mdash; handle documents where arrays grow beyond normal (flag + overflow collection)</li>
                    <li><strong>Computed Pattern</strong> &mdash; pre-calculate values (total, average) instead of computing on every read</li>
                    <li><strong>Subset Pattern</strong> &mdash; embed only recent/frequent data, keep rest in separate collection</li>
                    <li><strong>Extended Reference</strong> &mdash; copy frequently needed fields from referenced doc to avoid $lookup</li>
                    <li><strong>Polymorphic Pattern</strong> &mdash; different doc shapes in same collection (sports cards, game cards, all in "products")</li>
                </ul>
            </div>

            <div class="point">
                <strong>Bucket Pattern Example (Healthcare)</strong>
<pre><code>// Instead of 1 doc per vital reading (millions of docs)
// Group into hourly buckets
{
  patientId: ObjectId("p1"),
  date: ISODate("2024-01-10"),
  hour: 14,
  readings: [
    { time: "14:00", bp: "140/90", temp: 98.6 },
    { time: "14:15", bp: "138/88", temp: 98.4 },
    { time: "14:30", bp: "135/85", temp: 98.5 }
  ],
  count: 3,
  avgBp: "137/87"  // computed pattern combined
}</code></pre>
            </div>

            <div class="point">
                <strong>Polymorphic Pattern Example (eCommerce)</strong>
<pre><code>// Same "products" collection, different shapes
// Sports card
{
  type: "sports_card",
  name: "Mike Trout RC",
  sport: "baseball",
  year: 2011,
  graded: true,
  grade: "PSA 10"
}
// Game card
{
  type: "game_card",
  name: "Black Lotus",
  game: "MTG",
  set: "Alpha",
  rarity: "rare",
  foil: false
}</code></pre>
            </div>

            <div class="point">
                <strong>Anti-Patterns (avoid these)</strong>
                <ul>
                    <li><strong>Massive arrays</strong> &mdash; unbounded arrays slow down reads and hit 16MB limit</li>
                    <li><strong>Unnecessary normalization</strong> &mdash; don't split data into 10 collections like MySQL</li>
                    <li><strong>No indexes on referenced fields</strong> &mdash; $lookup without index = full collection scan</li>
                    <li><strong>Storing blobs in document</strong> &mdash; use GridFS or S3 for files</li>
                </ul>
            </div>

            <div class="point">
                <strong>$lookup (JOIN equivalent)</strong>
<pre><code>// Get patient with their lab reports from separate collection
db.patients.aggregate([
  { $match: { name: "Rajesh" } },
  { $lookup: {
      from: "lab_reports",
      localField: "_id",
      foreignField: "patientId",
      as: "labs"
  }}
])</code></pre>
                <p>$lookup is expensive. If you need it frequently, consider embedding or extended reference pattern instead.</p>
            </div>

            <div class="point">
                <strong>Schema Validation</strong>
<pre><code>db.createCollection("patients", {
  validator: {
    $jsonSchema: {
      bsonType: "object",
      required: ["name", "age"],
      properties: {
        name: { bsonType: "string", description: "required" },
        age:  { bsonType: "int", minimum: 0, maximum: 150 }
      }
    }
  },
  validationLevel: "strict",    // or "moderate"
  validationAction: "error"     // or "warn"
})</code></pre>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How did you design the schema for medical records?</div>
                    <div class="answer">Used <strong>embedding</strong> for patient + encounter data (vitals, ICD codes, NLP entities) since they're always read together. Used <strong>bucket pattern</strong> for time-series vitals to reduce doc count. Used <strong>referencing</strong> for lab reports since they can be accessed independently and grow unbounded.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Embed or Reference? How do you decide?</div>
                    <div class="answer">Ask 3 questions: (1) Is data read together? &rarr; embed. (2) Does it grow unbounded? &rarr; reference. (3) Is it shared by multiple parents? &rarr; reference. Default to embedding unless there's a reason not to. MongoDB is optimized for single-doc reads.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What schema pattern did you use for the eCommerce collectibles platform?</div>
                    <div class="answer"><strong>Polymorphic pattern</strong> &mdash; sports cards, game cards, and collectibles all in one "products" collection with different field shapes. This enabled a single search index across all product types. Also used <strong>extended reference</strong> &mdash; copied seller name + rating into product doc to avoid $lookup on every product listing.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is the Bucket Pattern and when to use it?</div>
                    <div class="answer">Groups time-series data into buckets (e.g., 1 doc = 1 hour). Instead of millions of individual readings, you get thousands of bucket docs. Reduces index size, improves write performance. Used for patient vitals monitoring &mdash; each bucket holds readings for one hour with pre-computed averages.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How do you handle Many-to-Many in MongoDB?</div>
                    <div class="answer">Two approaches: (1) <strong>Array of references</strong> in both sides &mdash; student has courseIds[], course has studentIds[]. (2) <strong>Junction collection</strong> for unbounded or with extra attributes &mdash; enrollment { studentId, courseId, grade, date }. Choose based on array growth.</div>
                </div>

                <div class="q">
                    <div class="question">Q: $lookup performance &mdash; is it like a SQL JOIN?</div>
                    <div class="answer">Similar but <strong>much slower</strong>. $lookup does a full collection scan on "from" collection unless the foreignField is indexed. It doesn't use hash joins or merge joins. Best practice: (1) always index foreignField, (2) if $lookup is frequent, consider denormalizing with embedding or extended reference pattern.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is Schema Validation in MongoDB?</div>
                    <div class="answer">MongoDB supports <strong>$jsonSchema</strong> validation on collections. Define required fields, types, min/max, enums. Set <code>validationLevel: "strict"</code> to enforce on all writes, or <code>"moderate"</code> to only validate new inserts. Set <code>validationAction: "error"</code> to reject or <code>"warn"</code> to just log.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What are common schema design anti-patterns?</div>
                    <div class="answer">(1) <strong>Unbounded arrays</strong> &mdash; arrays that grow forever slow reads, hit 16MB. (2) <strong>Over-normalization</strong> &mdash; splitting into too many collections like SQL, causing excessive $lookups. (3) <strong>Bloated documents</strong> &mdash; embedding data that's rarely accessed. (4) <strong>No validation</strong> &mdash; flexible schema doesn't mean no rules, use $jsonSchema.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 3 -->
        <div class="concept" id="concept-3">
            <h2>3. Indexing Deep Dive</h2>
            <span class="tag">Performance Critical</span>

            <div class="point">
                <strong>Why Indexes?</strong>
                <p>Without index = <strong>collection scan</strong> (reads every document). With index = B-Tree lookup, jumps directly to matching docs. Like a book's table of contents.</p>
            </div>

            <div class="point">
                <strong>Default Index</strong>
                <p>Every collection has <code>_id</code> index automatically. Cannot be dropped.</p>
            </div>

            <div class="point">
                <strong>Index Types</strong>
                <ul>
                    <li><strong>Single Field</strong> &mdash; index on one field</li>
                    <li><strong>Compound</strong> &mdash; index on multiple fields (order matters!)</li>
                    <li><strong>Multikey</strong> &mdash; auto-created when indexing array fields</li>
                    <li><strong>Text</strong> &mdash; full-text search on string fields</li>
                    <li><strong>Hashed</strong> &mdash; for hash-based sharding</li>
                    <li><strong>Geospatial</strong> &mdash; 2d / 2dsphere for location queries</li>
                    <li><strong>Wildcard</strong> &mdash; index on dynamic/unknown field names</li>
                    <li><strong>TTL</strong> &mdash; auto-delete docs after a time period</li>
                </ul>
            </div>

            <div class="point">
                <strong>Creating Indexes</strong>
<pre><code>// Single field
db.patients.createIndex({ name: 1 })          // 1 = ascending
db.patients.createIndex({ age: -1 })          // -1 = descending

// Compound index (order matters!)
db.patients.createIndex({ status: 1, age: -1 })

// Unique index
db.patients.createIndex({ email: 1 }, { unique: true })

// TTL index - auto-delete after 30 days
db.sessions.createIndex({ createdAt: 1 }, { expireAfterSeconds: 2592000 })

// Text index
db.products.createIndex({ name: "text", description: "text" })

// Partial index - only index active patients
db.patients.createIndex(
  { name: 1 },
  { partialFilterExpression: { status: "active" } }
)

// Wildcard index - for dynamic fields
db.products.createIndex({ "attributes.$**": 1 })</code></pre>
            </div>

            <div class="point">
                <strong>Compound Index - ESR Rule (Equality, Sort, Range)</strong>
                <p>Order fields in compound index as: <strong>Equality</strong> first, then <strong>Sort</strong>, then <strong>Range</strong>.</p>
<pre><code>// Query: find active patients aged > 30, sorted by name
db.patients.find({ status: "active", age: { $gt: 30 } }).sort({ name: 1 })

// Best index (ESR rule):
db.patients.createIndex({ status: 1, name: 1, age: 1 })
//                        Equality     Sort     Range</code></pre>
            </div>

            <div class="point">
                <strong>Covered Query (fastest possible)</strong>
                <p>Query answered entirely from index, no need to fetch document. All queried + returned fields must be in the index.</p>
<pre><code>// Index
db.patients.createIndex({ status: 1, name: 1 })

// Covered query - returns only indexed fields, _id excluded
db.patients.find(
  { status: "active" },
  { name: 1, _id: 0 }
)
// explain() will show: totalDocsExamined: 0</code></pre>
            </div>

            <div class="point">
                <strong>explain() - How to Check Index Usage</strong>
<pre><code>db.patients.find({ name: "Rajesh" }).explain("executionStats")

// Key things to check:
// winningPlan.stage: "IXSCAN" = using index, "COLLSCAN" = bad!
// totalKeysExamined: how many index entries scanned
// totalDocsExamined: how many docs fetched
// executionTimeMillis: query time

// Goal: keysExamined ≈ docsExamined ≈ nReturned</code></pre>
            </div>

            <div class="point">
                <strong>Index Properties</strong>
                <ul>
                    <li><strong>Unique</strong> &mdash; no duplicate values allowed</li>
                    <li><strong>Sparse</strong> &mdash; only index docs that have the field (skip nulls)</li>
                    <li><strong>Partial</strong> &mdash; index only docs matching a filter (more flexible than sparse)</li>
                    <li><strong>TTL</strong> &mdash; auto-expire documents after X seconds</li>
                    <li><strong>Hidden</strong> &mdash; exists but not used by planner (for testing before dropping)</li>
                </ul>
            </div>

            <div class="point">
                <strong>Multikey Index (arrays)</strong>
<pre><code>// Document
{ name: "Rajesh", codes: ["E11", "I10", "M19"] }

// Index on array field - MongoDB creates index entry for EACH element
db.patients.createIndex({ codes: 1 })

// Now this is fast:
db.patients.find({ codes: "E11" })

// Limitation: compound index can have only ONE array field</code></pre>
            </div>

            <div class="point">
                <strong>Index Costs (trade-offs)</strong>
                <ul>
                    <li>Each index <strong>slows down writes</strong> &mdash; every insert/update must update all indexes</li>
                    <li>Indexes use <strong>RAM</strong> &mdash; working set should fit in memory</li>
                    <li>Too many indexes = slow writes + wasted memory</li>
                    <li>Rule: max <strong>5-10 indexes</strong> per collection for write-heavy workloads</li>
                </ul>
            </div>

            <div class="point">
                <strong>Manage Indexes</strong>
<pre><code>db.patients.getIndexes()                    // list all
db.patients.dropIndex("name_1")             // drop by name
db.patients.dropIndexes()                   // drop all except _id
db.patients.hideIndex("name_1")             // hide (test before drop)
db.patients.unhideIndex("name_1")           // unhide</code></pre>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How did you optimize query performance for healthcare data?</div>
                    <div class="answer">Used <strong>compound indexes</strong> following ESR rule. For the most common query (find patients by status + date range sorted by name), created index <code>{ status: 1, name: 1, admitDate: 1 }</code>. Used <strong>partial indexes</strong> to only index active records, saving 60% index memory. Used <strong>explain()</strong> to verify IXSCAN and minimize docsExamined.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is the ESR rule?</div>
                    <div class="answer">Order compound index fields as: <strong>Equality</strong> (exact match fields first), <strong>Sort</strong> (sort fields next), <strong>Range</strong> ($gt, $lt, $in last). This lets MongoDB traverse the index optimally &mdash; narrow by equality, walk in sort order, then filter range.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is a Covered Query?</div>
                    <div class="answer">A query answered <strong>entirely from the index</strong> without touching documents. All query conditions AND projected fields must be in the index. Fastest possible query. In explain(), <code>totalDocsExamined: 0</code>. Must exclude _id with <code>{ _id: 0 }</code> unless _id is part of the index.</div>
                </div>

                <div class="q">
                    <div class="question">Q: TTL Index - how did you use it?</div>
                    <div class="answer">Used TTL index on session and temporary token collections to <strong>auto-delete expired docs</strong>. <code>createIndex({ createdAt: 1 }, { expireAfterSeconds: 86400 })</code> deletes docs 24 hours after creation. MongoDB background thread checks every 60 seconds. No cron job needed.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Multikey index limitation?</div>
                    <div class="answer">MongoDB creates one index entry <strong>per array element</strong>. Limitation: a compound index can have <strong>at most one array field</strong>. If both fields are arrays, MongoDB rejects the index (Cartesian product too large). For the medical codes array at CorroHealth, we index <code>{ codes: 1 }</code> to quickly find patients by ICD code.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How many indexes is too many?</div>
                    <div class="answer">Each index slows writes and uses RAM. Rule of thumb: <strong>5-10 indexes</strong> for write-heavy, up to 15-20 for read-heavy. Monitor with <code>db.collection.stats()</code> &mdash; check <code>totalIndexSize</code> vs available RAM. If working set doesn't fit in memory, performance degrades sharply.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Partial vs Sparse index?</div>
                    <div class="answer"><strong>Sparse</strong> only indexes docs where the field exists (skips null/missing). <strong>Partial</strong> is more flexible &mdash; index docs matching any filter expression like <code>{ status: "active" }</code>. Partial is preferred. Example: index only active patients saves 70% space if 70% of records are archived.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to safely remove an unused index in production?</div>
                    <div class="answer">Step 1: <code>hideIndex()</code> &mdash; makes planner ignore it without dropping. Step 2: Monitor for a week &mdash; if no performance regression, it's safe. Step 3: <code>dropIndex()</code>. Never drop directly in production &mdash; hiding is reversible, dropping is not.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 4 -->
        <div class="concept" id="concept-4">
            <h2>4. Aggregation Framework</h2>
            <span class="tag">Heavy Interview Topic</span>

            <div class="point">
                <strong>What is it?</strong>
                <p>Pipeline of stages that process documents. Each stage transforms data and passes result to next stage. Like Unix pipes: <code>data | $match | $group | $sort | result</code></p>
            </div>

            <div class="point">
                <strong>Core Stages (must know)</strong>
                <ul>
                    <li><strong>$match</strong> &mdash; filter docs (like WHERE). Put early to reduce data.</li>
                    <li><strong>$group</strong> &mdash; group by field + apply accumulators (like GROUP BY)</li>
                    <li><strong>$project</strong> &mdash; reshape docs, include/exclude/compute fields (like SELECT)</li>
                    <li><strong>$sort</strong> &mdash; order results</li>
                    <li><strong>$limit / $skip</strong> &mdash; pagination</li>
                    <li><strong>$unwind</strong> &mdash; flatten an array (1 doc per array element)</li>
                    <li><strong>$lookup</strong> &mdash; join with another collection</li>
                    <li><strong>$addFields</strong> &mdash; add new computed fields (keeps existing)</li>
                    <li><strong>$count</strong> &mdash; count total docs</li>
                    <li><strong>$facet</strong> &mdash; run multiple pipelines in parallel on same data</li>
                </ul>
            </div>

            <div class="point">
                <strong>$group Accumulators</strong>
                <ul>
                    <li><strong>$sum</strong>, <strong>$avg</strong>, <strong>$min</strong>, <strong>$max</strong> &mdash; math operations</li>
                    <li><strong>$first</strong>, <strong>$last</strong> &mdash; first/last value in group</li>
                    <li><strong>$push</strong> &mdash; collect values into array</li>
                    <li><strong>$addToSet</strong> &mdash; collect unique values into array</li>
                    <li><strong>$count</strong> &mdash; count docs in group</li>
                </ul>
            </div>

            <div class="point">
                <strong>Example 1: Patient stats by diagnosis</strong>
<pre><code>db.patients.aggregate([
  { $match: { status: "active" } },
  { $unwind: "$codes" },
  { $group: {
      _id: "$codes",
      count: { $sum: 1 },
      avgAge: { $avg: "$age" },
      patients: { $push: "$name" }
  }},
  { $sort: { count: -1 } },
  { $limit: 10 }
])
// Output: top 10 diagnosis codes with patient count and avg age</code></pre>
            </div>

            <div class="point">
                <strong>Example 2: Monthly revenue (eCommerce)</strong>
<pre><code>db.orders.aggregate([
  { $match: { status: "completed", year: 2024 } },
  { $group: {
      _id: { month: "$month", category: "$category" },
      revenue: { $sum: "$total" },
      orders: { $sum: 1 },
      avgOrder: { $avg: "$total" }
  }},
  { $sort: { "_id.month": 1 } },
  { $project: {
      month: "$_id.month",
      category: "$_id.category",
      revenue: 1,
      orders: 1,
      avgOrder: { $round: ["$avgOrder", 2] },
      _id: 0
  }}
])</code></pre>
            </div>

            <div class="point">
                <strong>$unwind (flatten arrays)</strong>
<pre><code>// Before unwind
{ name: "Rajesh", codes: ["E11", "I10", "M19"] }

// After { $unwind: "$codes" }
{ name: "Rajesh", codes: "E11" }
{ name: "Rajesh", codes: "I10" }
{ name: "Rajesh", codes: "M19" }

// Preserve empty arrays (don't lose docs without the field)
{ $unwind: { path: "$codes", preserveNullAndEmptyArrays: true } }</code></pre>
            </div>

            <div class="point">
                <strong>$lookup (JOIN)</strong>
<pre><code>// Simple lookup
{ $lookup: {
    from: "doctors",
    localField: "doctorId",
    foreignField: "_id",
    as: "doctor"
}}

// Pipeline lookup (more powerful - add conditions)
{ $lookup: {
    from: "lab_reports",
    let: { pid: "$_id" },
    pipeline: [
      { $match: { $expr: { $eq: ["$patientId", "$$pid"] } } },
      { $match: { type: "blood_test" } },
      { $sort: { date: -1 } },
      { $limit: 5 }
    ],
    as: "recentLabs"
}}</code></pre>
            </div>

            <div class="point">
                <strong>$facet (multiple pipelines at once)</strong>
<pre><code>// Get total count + paginated results + category breakdown in ONE query
db.products.aggregate([
  { $match: { status: "active" } },
  { $facet: {
      totalCount: [{ $count: "count" }],
      results: [{ $skip: 20 }, { $limit: 10 }],
      byCategory: [
        { $group: { _id: "$category", count: { $sum: 1 } } }
      ]
  }}
])</code></pre>
            </div>

            <div class="point">
                <strong>$bucket / $bucketAuto (histogram)</strong>
<pre><code>// Group patients by age ranges
db.patients.aggregate([
  { $bucket: {
      groupBy: "$age",
      boundaries: [0, 18, 30, 50, 70, 120],
      default: "unknown",
      output: {
        count: { $sum: 1 },
        names: { $push: "$name" }
      }
  }}
])
// Output: { _id: 0, count: 5 }, { _id: 18, count: 12 }, ...</code></pre>
            </div>

            <div class="point">
                <strong>Performance Tips</strong>
                <ul>
                    <li><strong>$match early</strong> &mdash; filter first, reduces data for later stages</li>
                    <li><strong>$match + $sort at start</strong> can use indexes</li>
                    <li>After $group/$project, indexes can't be used</li>
                    <li><strong>allowDiskUse: true</strong> for large datasets (default 100MB RAM limit per stage)</li>
                    <li>Use <strong>$project</strong> early to drop unnecessary fields</li>
                </ul>
<pre><code>// Allow disk use for large aggregations
db.patients.aggregate([...], { allowDiskUse: true })</code></pre>
            </div>

            <div class="point">
                <strong>$out / $merge (save results)</strong>
<pre><code>// $out - replace entire collection with results
{ $out: "monthly_reports" }

// $merge - upsert results into existing collection (better)
{ $merge: {
    into: "monthly_reports",
    on: "_id",
    whenMatched: "replace",
    whenNotMatched: "insert"
}}</code></pre>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How did you use aggregation for clickstream intelligence at Beckett?</div>
                    <div class="answer">Built pipelines to analyze user browsing patterns: <code>$match</code> by date range, <code>$group</code> by productId to count views/clicks, <code>$sort</code> by engagement score, <code>$lookup</code> to join product details. Used <code>$facet</code> to get trending products + category breakdown + hourly patterns in a single query. Results saved with <code>$merge</code> into a precomputed analytics collection.</div>
                </div>

                <div class="q">
                    <div class="question">Q: $match at beginning vs end - does it matter?</div>
                    <div class="answer">Yes, hugely. <strong>$match at the start</strong> can use indexes and reduces docs flowing through the pipeline. $match after $group or $project <strong>cannot use indexes</strong> and processes all docs. Always filter as early as possible. Same for $sort &mdash; at the start it uses indexes, later it sorts in memory.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is $facet and when to use it?</div>
                    <div class="answer">Runs <strong>multiple sub-pipelines in parallel</strong> on the same input data. Perfect for search result pages &mdash; get paginated results + total count + category filters in ONE database call instead of three. Each sub-pipeline gets the full input and produces independent output.</div>
                </div>

                <div class="q">
                    <div class="question">Q: $unwind with preserveNullAndEmptyArrays?</div>
                    <div class="answer">By default, $unwind <strong>drops documents</strong> where the array is empty or missing. <code>preserveNullAndEmptyArrays: true</code> keeps those docs with the field set to null. Important when you don't want to lose data &mdash; e.g., patients without diagnosis codes should still appear in reports.</div>
                </div>

                <div class="q">
                    <div class="question">Q: $out vs $merge - which to use?</div>
                    <div class="answer"><code>$out</code> replaces the entire target collection (destructive). <code>$merge</code> does upsert &mdash; update existing docs, insert new ones. <strong>Always prefer $merge</strong> in production. $merge also works with sharded collections and allows configuring whenMatched/whenNotMatched behavior.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Pipeline vs simple lookup - what's the difference?</div>
                    <div class="answer">Simple lookup matches localField to foreignField (basic equality join). <strong>Pipeline lookup</strong> lets you add $match, $sort, $limit inside the join &mdash; e.g., "get only last 5 blood tests for this patient." Pipeline lookup is more flexible but slightly slower. Use it when you need filtered/sorted joins.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What happens when aggregation exceeds memory?</div>
                    <div class="answer">Each stage has <strong>100MB RAM limit</strong> by default. Exceeded = error. Fix: (1) <code>allowDiskUse: true</code> to spill to disk. (2) $match early to reduce data. (3) $project to drop fields. Note: allowDiskUse is slower since it uses disk I/O. For production analytics, consider precomputing with $merge.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Write an aggregation for "top 5 doctors by patient count this month"</div>
                    <div class="answer"><pre><code>db.patients.aggregate([
  { $match: {
      admitDate: {
        $gte: ISODate("2024-12-01"),
        $lt: ISODate("2025-01-01")
      }
  }},
  { $group: { _id: "$doctorId", count: { $sum: 1 } } },
  { $sort: { count: -1 } },
  { $limit: 5 },
  { $lookup: {
      from: "doctors",
      localField: "_id",
      foreignField: "_id",
      as: "doctor"
  }},
  { $unwind: "$doctor" },
  { $project: {
      name: "$doctor.name",
      patientCount: "$count", _id: 0
  }}
])</code></pre></div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 5 -->
        <div class="concept" id="concept-5">
            <h2>5. Sharding & Horizontal Scaling</h2>
            <span class="tag">Architecture Level</span>

            <div class="point">
                <strong>What is Sharding?</strong>
                <p>Splitting data across multiple servers (shards). Each shard holds a subset of data. Enables <strong>horizontal scaling</strong> &mdash; add more machines instead of bigger machine.</p>
            </div>

            <div class="point">
                <strong>Sharded Cluster Components</strong>
                <ul>
                    <li><strong>Shard</strong> &mdash; each shard is a replica set holding a portion of data</li>
                    <li><strong>mongos</strong> &mdash; query router. App connects to mongos, not directly to shards</li>
                    <li><strong>Config Servers</strong> &mdash; store metadata (which data is on which shard). Also a replica set</li>
                </ul>
<pre><code>// Architecture:
App &rarr; mongos (router) &rarr; Shard 1 (replica set)
                         &rarr; Shard 2 (replica set)
                         &rarr; Shard 3 (replica set)
         Config Servers (replica set) - stores chunk mappings</code></pre>
            </div>

            <div class="point">
                <strong>Shard Key (most important decision)</strong>
                <p>The field used to distribute data across shards. <strong>Cannot be changed after sharding</strong> (until MongoDB 5.0 resharding). Choose carefully!</p>
<pre><code>// Shard a collection
sh.enableSharding("healthcare_db")
sh.shardCollection("healthcare_db.patients", { region: 1 })
//                                             ^ shard key</code></pre>
            </div>

            <div class="point">
                <strong>Sharding Strategies</strong>
                <ul>
                    <li><strong>Ranged Sharding</strong> &mdash; consecutive values go to same shard. Good for range queries. Risk: hotspot if writes are sequential (e.g., timestamps)</li>
                    <li><strong>Hashed Sharding</strong> &mdash; hash of shard key distributes evenly. Good write distribution. Bad for range queries (scatter-gather)</li>
                    <li><strong>Zone Sharding</strong> &mdash; assign data ranges to specific shards. Good for geo/compliance (e.g., India data stays on India shard)</li>
                </ul>
<pre><code>// Hashed sharding
sh.shardCollection("db.orders", { orderId: "hashed" })

// Zone sharding (geo-based)
sh.addShardTag("shard1", "INDIA")
sh.addShardTag("shard2", "US")
sh.addTagRange("db.patients", { region: "IN" }, { region: "IN\uffff" }, "INDIA")
sh.addTagRange("db.patients", { region: "US" }, { region: "US\uffff" }, "US")</code></pre>
            </div>

            <div class="point">
                <strong>Good Shard Key Properties</strong>
                <ul>
                    <li><strong>High cardinality</strong> &mdash; many unique values (NOT status: "active/inactive")</li>
                    <li><strong>Even distribution</strong> &mdash; data spread equally across shards</li>
                    <li><strong>Query isolation</strong> &mdash; most queries include shard key (targeted queries)</li>
                    <li><strong>Not monotonically increasing</strong> &mdash; avoid _id, timestamps (causes hotspot on last shard)</li>
                </ul>
            </div>

            <div class="point">
                <strong>Targeted vs Scatter-Gather Queries</strong>
                <ul>
                    <li><strong>Targeted</strong> &mdash; query includes shard key, mongos routes to ONE shard. Fast.</li>
                    <li><strong>Scatter-Gather</strong> &mdash; query without shard key, mongos asks ALL shards, merges results. Slow.</li>
                </ul>
<pre><code>// Shard key: { region: 1 }

// TARGETED - includes shard key
db.patients.find({ region: "IN", name: "Rajesh" })  // goes to 1 shard

// SCATTER-GATHER - no shard key
db.patients.find({ name: "Rajesh" })  // asks ALL shards</code></pre>
            </div>

            <div class="point">
                <strong>Chunks & Balancer</strong>
                <ul>
                    <li><strong>Chunk</strong> &mdash; contiguous range of shard key values. Default max size: 128MB</li>
                    <li><strong>Splitting</strong> &mdash; when chunk exceeds max size, MongoDB splits it into two</li>
                    <li><strong>Balancer</strong> &mdash; background process that moves chunks between shards to keep data even</li>
                    <li>Balancer runs on config server, can be scheduled to off-peak hours</li>
                </ul>
            </div>

            <div class="point">
                <strong>When to Shard</strong>
                <ul>
                    <li>Single server can't handle data volume (disk full)</li>
                    <li>Single server can't handle throughput (CPU/RAM maxed)</li>
                    <li>Working set doesn't fit in RAM</li>
                    <li>Typically at <strong>1TB+</strong> data or <strong>50K+ ops/sec</strong></li>
                </ul>
                <p>Don't shard too early &mdash; adds complexity. Start with replica set, shard when needed.</p>
            </div>

            <div class="point">
                <strong>Sharding Limitations</strong>
                <ul>
                    <li>Unique indexes must include the shard key</li>
                    <li>$lookup only works on unsharded or same-shard collections (before 5.1)</li>
                    <li>Transactions across shards are slower than single-shard</li>
                    <li>Resharding (changing shard key) possible since 5.0 but expensive</li>
                </ul>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How would you shard the healthcare patient collection?</div>
                    <div class="answer">Use <strong>compound shard key</strong>: <code>{ region: 1, patientId: "hashed" }</code>. Region provides query isolation (most queries filter by hospital/region) and enables zone sharding for data compliance. Hashed patientId ensures even distribution within each region. Avoids hotspot since patientId is random.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Why not use _id as shard key?</div>
                    <div class="answer">ObjectId is <strong>monotonically increasing</strong> (starts with timestamp). All new writes go to the last shard &mdash; creating a <strong>hotspot</strong>. One shard gets all the writes while others sit idle. Use hashed _id if you must: <code>{ _id: "hashed" }</code>, but then range queries on _id become scatter-gather.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Targeted vs Scatter-Gather query?</div>
                    <div class="answer"><strong>Targeted</strong>: query includes shard key, goes to one shard. O(1) routing. <strong>Scatter-Gather</strong>: no shard key in query, mongos asks all shards, merges. O(n) where n = number of shards. Always design queries to include shard key. If 80% of queries use region, shard by region.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Ranged vs Hashed sharding &mdash; when to use each?</div>
                    <div class="answer"><strong>Ranged</strong>: when you need range queries on shard key (date ranges, alphabetical). Risk: uneven distribution. <strong>Hashed</strong>: when you need even write distribution. Risk: range queries become scatter-gather. For eCommerce orders, used hashed on orderId for even writes. For analytics by date, used ranged on date.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is the Balancer and can it cause issues?</div>
                    <div class="answer">Background process that <strong>migrates chunks</strong> between shards for even distribution. Can cause performance impact during migration (network + disk I/O). Best practice: schedule balancer window during off-peak hours using <code>sh.setBalancerState()</code> and balancer window configuration. Monitor with <code>sh.status()</code>.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is Zone Sharding and when did you use it?</div>
                    <div class="answer">Assigns data ranges to specific shards based on tags. Used for <strong>data residency compliance</strong> &mdash; India patient data must stay on India servers. Tag shard1 as "INDIA", define zone range for region "IN". Also useful for tiered storage &mdash; hot data on SSD shards, cold data on HDD shards.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Can you change the shard key after sharding?</div>
                    <div class="answer">Before 5.0: <strong>No</strong>. Had to create new collection, reshard, migrate. Since MongoDB 5.0: <strong>reshardCollection</strong> command allows changing shard key online. It creates a temp collection, copies data, does a fast cutover. Still expensive on large collections &mdash; plan during maintenance window.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How did you design sharding for the eCommerce collectibles platform?</div>
                    <div class="answer">Products collection: <code>{ category: 1, _id: "hashed" }</code> &mdash; category gives targeted queries (users browse by category), hashed _id distributes evenly within category. Orders collection: <code>{ userId: "hashed" }</code> &mdash; even distribution, user's orders go to same shard (targeted lookup). Clickstream: <code>{ date: 1 }</code> ranged &mdash; analytics queries always filter by date range.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 6 -->
        <div class="concept" id="concept-6">
            <h2>6. Replication & High Availability</h2>
            <span class="tag">SRE / Reliability</span>

            <div class="point">
                <strong>What is a Replica Set?</strong>
                <p>Group of mongod instances with the same data. One <strong>Primary</strong> (reads+writes) and multiple <strong>Secondaries</strong> (replicate from primary). Automatic failover.</p>
            </div>

            <div class="point">
                <strong>Replica Set Members</strong>
                <ul>
                    <li><strong>Primary</strong> &mdash; receives all writes. Only one at a time.</li>
                    <li><strong>Secondary</strong> &mdash; replicates from primary's oplog. Can serve reads if configured.</li>
                    <li><strong>Arbiter</strong> &mdash; votes in elections but holds NO data. Use to break tie (avoid if possible).</li>
                    <li><strong>Hidden</strong> &mdash; invisible to app, won't become primary. Used for backups/analytics.</li>
                    <li><strong>Delayed</strong> &mdash; replicates with a time delay (e.g., 1 hour behind). Protection against accidental deletes.</li>
                </ul>
<pre><code>// Typical setup: 3 members (1 primary + 2 secondaries)
// Minimum for automatic failover: 3 voting members

rs.initiate({
  _id: "healthcareRS",
  members: [
    { _id: 0, host: "mongo1:27017" },
    { _id: 1, host: "mongo2:27017" },
    { _id: 2, host: "mongo3:27017" }
  ]
})</code></pre>
            </div>

            <div class="point">
                <strong>Oplog (Operations Log)</strong>
                <ul>
                    <li>Capped collection on primary: <code>local.oplog.rs</code></li>
                    <li>Records every write operation</li>
                    <li>Secondaries tail the oplog and replay operations</li>
                    <li>Idempotent &mdash; safe to replay multiple times</li>
                    <li>Size matters &mdash; if secondary falls behind more than oplog window, needs full resync</li>
                </ul>
            </div>

            <div class="point">
                <strong>Election & Failover</strong>
                <ul>
                    <li>If primary goes down, secondaries hold an <strong>election</strong></li>
                    <li>Needs <strong>majority vote</strong> (2 out of 3, 3 out of 5)</li>
                    <li>Election takes <strong>~10-12 seconds</strong> (configurable with electionTimeoutMillis)</li>
                    <li>During election, <strong>no writes</strong> are accepted</li>
                    <li>Priority setting controls which member is preferred as primary</li>
                </ul>
<pre><code>// Set priority (higher = preferred primary)
cfg = rs.conf()
cfg.members[0].priority = 10  // strongly prefer this node
cfg.members[1].priority = 5
cfg.members[2].priority = 1
rs.reconfig(cfg)

// Check replica set status
rs.status()    // shows who is primary, replication lag
rs.printReplicationInfo()  // oplog size and window</code></pre>
            </div>

            <div class="point">
                <strong>Write Concern (how many nodes confirm write)</strong>
                <ul>
                    <li><code>w: 1</code> &mdash; primary only (fast, risk of data loss on failover)</li>
                    <li><code>w: "majority"</code> &mdash; majority of replica set (safe, slightly slower)</li>
                    <li><code>w: 3</code> &mdash; all 3 members confirm</li>
                    <li><code>j: true</code> &mdash; wait for journal write (most durable)</li>
                </ul>
<pre><code>// Healthcare: maximum durability
db.patients.insertOne(
  { name: "Rajesh", codes: ["E11"] },
  { writeConcern: { w: "majority", j: true, wtimeout: 5000 } }
)</code></pre>
            </div>

            <div class="point">
                <strong>Read Concern (consistency level for reads)</strong>
                <ul>
                    <li><strong>"local"</strong> &mdash; returns most recent data on node (may be rolled back)</li>
                    <li><strong>"available"</strong> &mdash; like local, for sharded collections</li>
                    <li><strong>"majority"</strong> &mdash; returns data acknowledged by majority (won't be rolled back)</li>
                    <li><strong>"linearizable"</strong> &mdash; strongest. Waits for all prior majority writes. Slow.</li>
                    <li><strong>"snapshot"</strong> &mdash; for multi-document transactions</li>
                </ul>
            </div>

            <div class="point">
                <strong>Replication Lag</strong>
                <p>Time difference between primary's latest write and secondary's latest applied op. Monitor with:</p>
<pre><code>rs.printSecondaryReplicationInfo()
// Output shows lag in seconds for each secondary

// Or check in Atlas: Replication Lag chart
// Alert threshold: typically > 10 seconds = warning</code></pre>
            </div>

            <div class="point">
                <strong>Rollback</strong>
                <ul>
                    <li>Happens when old primary had writes not replicated to majority before failover</li>
                    <li>When old primary rejoins, it rolls back those unreplicated writes</li>
                    <li>Rolled-back data saved to <code>rollback/</code> directory</li>
                    <li><strong>Prevention:</strong> use <code>w: "majority"</code> for all critical writes</li>
                </ul>
            </div>

            <div class="point">
                <strong>Best Practices</strong>
                <ul>
                    <li>Always odd number of voting members (3, 5, 7 &mdash; max 7 voters)</li>
                    <li>Max 50 members total in a replica set</li>
                    <li>Spread members across data centers / AZs</li>
                    <li>Use <code>w: "majority"</code> for important data</li>
                    <li>Monitor replication lag, set alerts</li>
                    <li>Use hidden member for backups, delayed member for disaster recovery</li>
                </ul>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How did you ensure high availability for healthcare systems?</div>
                    <div class="answer">Used <strong>3-member replica set</strong> across AWS availability zones. Write concern <code>w: "majority", j: true</code> for all patient data. Hidden member for nightly backups. Delayed member (1 hour) for accidental delete recovery. Monitoring replication lag with alerts at >5 seconds. Achieved <strong>99.99% uptime</strong>.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What happens when the primary goes down?</div>
                    <div class="answer">Remaining secondaries hold an <strong>election</strong> (needs majority vote). New primary elected in ~10-12 seconds. During election, writes are rejected (app retries). Reads can continue from secondaries if readPreference is secondaryPreferred. Once new primary is elected, app reconnects automatically via connection string.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is the oplog and why does its size matter?</div>
                    <div class="answer">Oplog is a <strong>capped collection</strong> recording all writes. Secondaries tail it to stay in sync. If a secondary is offline longer than the <strong>oplog window</strong> (time it takes for oplog to fill), it can't catch up and needs full resync (hours/days for large datasets). Size oplog to cover at least 24-48 hours of writes.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Write Concern "majority" vs w:1 &mdash; trade-off?</div>
                    <div class="answer"><code>w:1</code>: fast (acknowledged by primary only) but risk of <strong>data loss</strong> if primary crashes before replicating. <code>w:"majority"</code>: slightly slower (~1-5ms more) but data <strong>survives any single node failure</strong>. For healthcare/financial data, always use majority. For logs/analytics, w:1 is acceptable.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is a rollback and how to prevent it?</div>
                    <div class="answer">Rollback occurs when old primary had writes <strong>not replicated to majority</strong> before failover. When it rejoins as secondary, those writes are rolled back. <strong>Prevention:</strong> use <code>w: "majority"</code> &mdash; guarantees data is on majority before acknowledging. Rolled-back data is saved to files for manual recovery.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Why use a Hidden or Delayed member?</div>
                    <div class="answer"><strong>Hidden:</strong> never becomes primary, invisible to app. Perfect for running backups, analytics queries, or reporting without impacting production. <strong>Delayed:</strong> replicates X hours behind. If someone drops a collection, you have a window to recover from the delayed member before the drop propagates.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Arbiter vs adding a third data-bearing member?</div>
                    <div class="answer">Arbiter only votes, holds no data. Cheaper but <strong>risky</strong> &mdash; if one data member fails, you have only 1 copy of data. Always prefer 3 data-bearing members. Arbiter is a last resort for cost-sensitive environments. MongoDB docs recommend against arbiters for production.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to handle replica set across regions/AZs?</div>
                    <div class="answer">Spread members across AZs: primary + 1 secondary in primary region, 1 secondary in DR region. Set <strong>priority</strong> so primary stays in main region. If main region goes down, DR secondary gets elected. Use <code>readPreference: "nearest"</code> for geo-distributed reads. For healthcare, this ensures data survives entire AZ outage.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 7 -->
        <div class="concept" id="concept-7">
            <h2>7. MongoDB Atlas & Cloud</h2>
            <span class="tag">Cloud / Managed Service</span>

            <div class="point">
                <strong>What is Atlas?</strong>
                <p>MongoDB's fully managed cloud database service. Handles provisioning, patching, backups, scaling &mdash; you focus on code. Available on AWS, Azure, GCP.</p>
            </div>

            <div class="point">
                <strong>Cluster Tiers</strong>
                <p><strong>M0/M2/M5:</strong> Free/Shared tiers for dev/POC. <strong>M10+:</strong> Dedicated clusters with full features. <strong>M30+:</strong> For production. <strong>Serverless:</strong> Pay-per-operation, auto-scales to zero.</p>
                <pre>// Connection string pattern
mongodb+srv://user:pass@cluster0.abc123.mongodb.net/myDB?retryWrites=true&w=majority</pre>
            </div>

            <div class="point">
                <strong>Atlas Search (Lucene-based)</strong>
                <p>Full-text search engine built into Atlas. No separate Elasticsearch needed. Create search indexes, query with <code>$search</code> aggregation stage.</p>
                <pre>// Create search index (Atlas UI or API)
{
  "mappings": {
    "dynamic": true,
    "fields": {
      "diagnosis": { "type": "string", "analyzer": "lucene.standard" }
    }
  }
}

// Query with $search
db.patients.aggregate([
  { $search: {
      index: "default",
      text: { query: "diabetes type 2", path: "diagnosis" }
  }},
  { $limit: 10 },
  { $project: { diagnosis: 1, score: { $meta: "searchScore" } } }
])</pre>
            </div>

            <div class="point">
                <strong>Change Streams</strong>
                <p>Real-time event-driven pipeline. Watch insert/update/delete on collection, DB, or entire deployment. Built on oplog. Perfect for syncing caches, triggering notifications.</p>
                <pre>// Watch a collection for changes
const pipeline = [{ $match: { operationType: "insert" } }];
const changeStream = db.collection("orders").watch(pipeline);

changeStream.on("change", (event) => {
  console.log("New order:", event.fullDocument);
  // Trigger notification, update cache, sync to Kafka
});</pre>
            </div>

            <div class="point">
                <strong>Atlas Triggers</strong>
                <p>Serverless functions that fire on DB events (insert/update/delete), scheduled (cron), or authentication events. Written in JS, run on Atlas App Services.</p>
                <pre>// Database Trigger example - runs on every new patient insert
exports = function(changeEvent) {
  const patient = changeEvent.fullDocument;
  if (patient.riskScore > 80) {
    // Send alert to care team via external API
    context.http.post({ url: "https://alerts.api/notify", body: patient });
  }
};</pre>
            </div>

            <div class="point">
                <strong>Online Archive</strong>
                <p>Automatically tier cold data from hot cluster to cheaper S3-backed storage. Query both with single connection string. Saves cost on large historical datasets.</p>
                <pre>// Archive rule: move records older than 365 days
{
  "dbName": "hospital",
  "collName": "visits",
  "partitionFields": [
    { "fieldName": "visitDate", "order": 0 }
  ],
  "criteria": {
    "type": "DATE",
    "dateField": "visitDate",
    "dateFormat": "ISODATE",
    "expireAfterDays": 365
  }
}</pre>
            </div>

            <div class="point">
                <strong>Backup & Restore</strong>
                <p><strong>Cloud Backup:</strong> Continuous backup with point-in-time restore (M10+). Snapshots every 6 hours (configurable). Restore to any second within retention window. <strong>M0-M5:</strong> Daily snapshots only.</p>
            </div>

            <div class="point">
                <strong>Performance Advisor & Monitoring</strong>
                <p><strong>Performance Advisor:</strong> Suggests missing indexes based on slow queries. <strong>Real-Time Performance Panel:</strong> Shows ops/sec, connections, network. <strong>Alerts:</strong> Configure thresholds for CPU, disk, oplog window, replication lag.</p>
            </div>

            <!-- Interview Q&A -->
            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: Why choose Atlas over self-managed MongoDB?</div>
                    <div class="answer">Atlas handles infra: automated backups, patching, scaling, monitoring, security (encryption at rest + in transit by default). For healthcare project, Atlas ensured <strong>HIPAA compliance</strong> with encryption, VPC peering, and audit logs out of the box. Team focused on features instead of ops. Self-managed makes sense only when you need full control or have strict on-prem requirements.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Atlas Search vs Elasticsearch &mdash; when to pick which?</div>
                    <div class="answer">Atlas Search: <strong>simpler architecture</strong> (no separate cluster), Lucene-powered, good for 80% of search use cases. Use when search is secondary feature. Elasticsearch: better for <strong>heavy analytics, log aggregation, complex scoring</strong>. In our project, we used Atlas Search for patient diagnosis lookup &mdash; avoided running separate ES cluster, reduced cost and complexity.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How do Change Streams work internally?</div>
                    <div class="answer">Built on top of the <strong>oplog</strong> (replication log). Atlas tails the oplog and pushes matching events to your cursor. Requires replica set or sharded cluster. Supports <strong>resume tokens</strong> &mdash; if your app crashes, reconnect with last token and pick up where you left off. No events lost.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Explain Atlas connection string &mdash; what does +srv do?</div>
                    <div class="answer"><code>mongodb+srv://</code> uses <strong>DNS SRV records</strong> to discover all replica set members automatically. Client doesn't need to list every host. If Atlas adds/removes nodes, DNS updates and client discovers new topology. The <code>retryWrites=true</code> ensures transient failures auto-retry. <code>w=majority</code> ensures writes acknowledged by majority.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How would you handle cost optimization in Atlas?</div>
                    <div class="answer"><strong>1)</strong> Use Online Archive to move cold data to cheap storage. <strong>2)</strong> Right-size cluster tier based on actual workload. <strong>3)</strong> Use auto-scaling (M10+) to scale up during peak, down at off-peak. <strong>4)</strong> Use serverless instances for sporadic workloads. <strong>5)</strong> Optimize indexes &mdash; fewer indexes = less storage and faster writes. <strong>6)</strong> Compress data with zstd (WiredTiger default).</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is Atlas Data Federation?</div>
                    <div class="answer">Query data across Atlas clusters, S3 buckets, and Atlas Online Archive using <strong>single MQL endpoint</strong>. No ETL needed. Use for cross-source analytics &mdash; e.g., join live patient data with archived historical records in one aggregation pipeline.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to ensure high availability in Atlas across regions?</div>
                    <div class="answer">Atlas supports <strong>multi-region clusters</strong>. Configure preferred region for primary, add electable nodes in other regions. If primary region fails, automatic failover to another region. Use <code>readPreference: "nearest"</code> for low-latency reads from closest node. For healthcare, this ensures zero downtime even during regional cloud outages.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Describe Atlas Triggers vs Change Streams &mdash; difference?</div>
                    <div class="answer"><strong>Change Streams:</strong> Client-side cursor that your app code listens to. You manage the listener process. <strong>Atlas Triggers:</strong> Serverless &mdash; Atlas runs your function automatically, no infrastructure to manage. Use Triggers for simple reactions (send email, call API). Use Change Streams when you need complex processing in your own app/service (e.g., sync to Kafka).</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 8 -->
        <div class="concept" id="concept-8">
            <h2>8. Performance Tuning</h2>
            <span class="tag">Production Critical</span>

            <div class="point">
                <strong>The Performance Checklist</strong>
                <ul>
                    <li><strong>1. Indexes</strong> &mdash; right indexes for your queries (ESR rule)</li>
                    <li><strong>2. Schema</strong> &mdash; embed vs reference, avoid unbounded arrays</li>
                    <li><strong>3. Working Set</strong> &mdash; hot data must fit in RAM</li>
                    <li><strong>4. Query Patterns</strong> &mdash; avoid COLLSCAN, use projections</li>
                    <li><strong>5. Connection Pooling</strong> &mdash; reuse connections, don't open/close per request</li>
                    <li><strong>6. Write Concern</strong> &mdash; tune based on data criticality</li>
                </ul>
            </div>

            <div class="point">
                <strong>Profiler &mdash; Find Slow Queries</strong>
                <p>MongoDB profiler logs slow operations. Level 0 = off, Level 1 = slow ops only, Level 2 = all ops.</p>
<pre><code>// Enable profiler for queries > 100ms
db.setProfilingLevel(1, { slowms: 100 })

// Check slow queries
db.system.profile.find().sort({ ts: -1 }).limit(5).pretty()

// Key fields to check:
// millis - execution time
// planSummary - IXSCAN or COLLSCAN
// nscanned vs nreturned - ratio should be close to 1
// command.filter - what query was run</code></pre>
            </div>

            <div class="point">
                <strong>explain() Deep Dive</strong>
<pre><code>db.patients.find({ status: "active", age: { $gt: 30 } })
  .sort({ name: 1 })
  .explain("executionStats")

// What to look for:
// 1. winningPlan.stage = "IXSCAN" (good) vs "COLLSCAN" (bad)
// 2. totalKeysExamined / nReturned &asymp; 1.0 (efficient)
// 3. totalDocsExamined / nReturned &asymp; 1.0 (no wasted reads)
// 4. executionTimeMillis - should be < 100ms for most queries
// 5. rejectedPlans - shows other plans MongoDB considered

// "allPlansExecution" mode shows all candidate plans
db.patients.find({...}).explain("allPlansExecution")</code></pre>
            </div>

            <div class="point">
                <strong>Working Set &amp; Memory</strong>
                <p>WiredTiger cache holds frequently accessed data + indexes. If working set exceeds cache, MongoDB reads from disk = <strong>massive slowdown</strong>.</p>
<pre><code>// Check cache usage
db.serverStatus().wiredTiger.cache
// "bytes currently in the cache"
// "maximum bytes configured" (default: 50% of RAM - 1GB)

// Check if data fits in RAM
db.patients.stats()
// dataSize: total data
// indexSize: total indexes
// Rule: dataSize + indexSize < available RAM = fast</code></pre>
            </div>

            <div class="point">
                <strong>Connection Pooling</strong>
                <p>Each connection uses ~1MB RAM on server. Don't open/close per request. Use driver's built-in pool.</p>
<pre><code>// Node.js - connection pool settings
const client = new MongoClient(uri, {
  maxPoolSize: 50,       // max connections in pool
  minPoolSize: 10,       // keep minimum ready
  maxIdleTimeMS: 30000,  // close idle connections after 30s
  waitQueueTimeoutMS: 5000  // timeout if pool exhausted
});

// WRONG: creating new client per request
app.get("/api", async (req, res) => {
  const client = new MongoClient(uri); // BAD!
  await client.connect();
  // ...
});

// RIGHT: reuse single client
const client = new MongoClient(uri);
await client.connect();  // once at startup
app.get("/api", async (req, res) => {
  const db = client.db("mydb");  // reuse
  // ...
});</code></pre>
            </div>

            <div class="point">
                <strong>Projection &mdash; Fetch Only What You Need</strong>
<pre><code>// BAD: fetches entire document (all fields)
db.patients.find({ status: "active" })

// GOOD: fetch only needed fields
db.patients.find(
  { status: "active" },
  { name: 1, age: 1, diagnosis: 1, _id: 0 }
)
// Less data over network, less memory, faster</code></pre>
            </div>

            <div class="point">
                <strong>Bulk Operations</strong>
                <p>Batch writes reduce network round trips. Use <code>bulkWrite()</code> for mixed operations or <code>insertMany()</code> for batch inserts.</p>
<pre><code>// Instead of 1000 individual inserts
// Use insertMany (1 round trip)
db.patients.insertMany([doc1, doc2, ..., doc1000], { ordered: false })

// bulkWrite for mixed operations
db.patients.bulkWrite([
  { insertOne: { document: { name: "A", age: 30 } } },
  { updateOne: { filter: { _id: id1 }, update: { $set: { status: "active" } } } },
  { deleteOne: { filter: { _id: id2 } } }
], { ordered: false })  // ordered:false = parallel execution</code></pre>
            </div>

            <div class="point">
                <strong>Read Preference Tuning</strong>
                <ul>
                    <li><strong>primary</strong> &mdash; all reads from primary (default, strongest consistency)</li>
                    <li><strong>primaryPreferred</strong> &mdash; primary, fallback to secondary</li>
                    <li><strong>secondary</strong> &mdash; read from secondaries (offload primary)</li>
                    <li><strong>secondaryPreferred</strong> &mdash; secondary, fallback to primary</li>
                    <li><strong>nearest</strong> &mdash; lowest latency node (best for geo-distributed)</li>
                </ul>
<pre><code>// Offload analytics reads to secondary
db.patients.find({ status: "active" })
  .readPref("secondaryPreferred")</code></pre>
            </div>

            <div class="point">
                <strong>Compression</strong>
                <ul>
                    <li><strong>snappy</strong> &mdash; default for collections. Good balance of speed + compression</li>
                    <li><strong>zstd</strong> &mdash; better compression ratio, slightly more CPU. Best for storage-heavy</li>
                    <li><strong>zlib</strong> &mdash; legacy, use zstd instead</li>
                    <li>Network compression: <code>compressors=zstd</code> in connection string</li>
                </ul>
            </div>

            <!-- Interview Q&A -->
            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How did you tune performance for healthcare data at scale?</div>
                    <div class="answer"><strong>1)</strong> Profiled slow queries (>100ms) with profiler level 1. <strong>2)</strong> Added compound indexes following ESR rule &mdash; reduced query time from 800ms to 15ms. <strong>3)</strong> Ensured working set fits in RAM by archiving old records. <strong>4)</strong> Used projections in all API responses. <strong>5)</strong> Connection pooling with maxPoolSize=100. <strong>6)</strong> Bulk writes for batch patient imports (10K docs/batch).</div>
                </div>

                <div class="q">
                    <div class="question">Q: A query is slow &mdash; walk through your debugging steps.</div>
                    <div class="answer"><strong>Step 1:</strong> Run <code>explain("executionStats")</code> &mdash; check if COLLSCAN or IXSCAN. <strong>Step 2:</strong> If COLLSCAN, add appropriate index. <strong>Step 3:</strong> Check ratio of keysExamined to nReturned. If >>1, index isn't selective enough. <strong>Step 4:</strong> Check if sort is in-memory (avoid). <strong>Step 5:</strong> Check projection &mdash; are we fetching unnecessary fields? <strong>Step 6:</strong> Check profiler for pattern of slow queries.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is the working set and why does it matter?</div>
                    <div class="answer">Working set = <strong>frequently accessed data + indexes</strong>. WiredTiger keeps it in cache (default 50% of RAM - 1GB). If working set > cache, MongoDB reads from disk (page faults) &mdash; 1000x slower. Monitor with <code>db.serverStatus().wiredTiger.cache</code>. Fix: add RAM, archive cold data, or shard to distribute across servers.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Connection pooling &mdash; what happens without it?</div>
                    <div class="answer">Each connection = ~1MB RAM + TCP handshake + auth. Without pooling, every request opens a new connection &mdash; at 1000 req/sec, that's 1000 connections = 1GB RAM wasted + connection overhead. With pooling, 50 connections are <strong>reused</strong> across all requests. Always initialize client once at app startup, reuse across requests.</div>
                </div>

                <div class="q">
                    <div class="question">Q: ordered vs unordered bulk writes?</div>
                    <div class="answer"><strong>ordered: true</strong> (default) &mdash; executes sequentially, stops on first error. <strong>ordered: false</strong> &mdash; executes in parallel, continues on errors, reports all errors at end. Use <code>ordered: false</code> for batch imports where individual failures are acceptable (e.g., importing patient records &mdash; don't stop 10K inserts because one has a duplicate key).</div>
                </div>

                <div class="q">
                    <div class="question">Q: When to read from secondary?</div>
                    <div class="answer">Use <code>secondaryPreferred</code> for: <strong>1)</strong> Analytics/reporting queries that can tolerate slight staleness. <strong>2)</strong> Offloading read-heavy workloads from primary. <strong>3)</strong> Geo-distributed reads with <code>nearest</code>. <strong>Never</strong> use secondary reads for data that must be up-to-the-second current (e.g., checking patient's current medication). Reads from secondary may be milliseconds to seconds behind.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to handle large batch imports without impacting production?</div>
                    <div class="answer"><strong>1)</strong> Use <code>insertMany</code> with <code>ordered: false</code> in batches of 5K-10K. <strong>2)</strong> Set lower write concern <code>w:1</code> for imports (not majority). <strong>3)</strong> Run during off-peak hours. <strong>4)</strong> Rate-limit inserts with small delays between batches. <strong>5)</strong> Build indexes AFTER bulk import (faster than indexing during insert). <strong>6)</strong> Use <code>mongoimport --numInsertionWorkers</code> for parallel import.</div>
                </div>

                <div class="q">
                    <div class="question">Q: snappy vs zstd compression &mdash; which to choose?</div>
                    <div class="answer"><strong>snappy</strong> (default): faster compression/decompression, ~3-4x compression ratio. Good for balanced workloads. <strong>zstd</strong>: better compression (~5-7x), slightly more CPU. Choose zstd for storage-heavy, read-heavy workloads where disk I/O is the bottleneck. For our healthcare system with large text documents, zstd saved ~40% storage vs snappy.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 9 -->
        <div class="concept" id="concept-9">
            <h2>9. Transactions</h2>
            <span class="tag">ACID Compliance</span>

            <div class="point">
                <strong>Single Document = Atomic (always)</strong>
                <p>MongoDB guarantees <strong>atomic writes on a single document</strong> &mdash; even for embedded arrays/subdocuments. No transaction needed for single-doc operations. This covers 80%+ of use cases.</p>
            </div>

            <div class="point">
                <strong>Multi-Document Transactions (4.0+)</strong>
                <p>When you need atomicity across <strong>multiple documents or collections</strong>. All-or-nothing &mdash; either all operations commit or all roll back.</p>
<pre><code>const session = client.startSession();
try {
  session.startTransaction({
    readConcern: { level: "snapshot" },
    writeConcern: { w: "majority" }
  });

  // Debit from patient wallet
  await db.wallets.updateOne(
    { patientId: "p1" },
    { $inc: { balance: -500 } },
    { session }
  );

  // Create payment record
  await db.payments.insertOne(
    { patientId: "p1", amount: 500, type: "consultation" },
    { session }
  );

  // Update appointment status
  await db.appointments.updateOne(
    { _id: appointmentId },
    { $set: { paymentStatus: "paid" } },
    { session }
  );

  await session.commitTransaction();
} catch (error) {
  await session.abortTransaction();
  throw error;
} finally {
  session.endSession();
}</code></pre>
            </div>

            <div class="point">
                <strong>Transaction Requirements</strong>
                <ul>
                    <li>Requires <strong>replica set</strong> (or sharded cluster 4.2+)</li>
                    <li>Default timeout: <strong>60 seconds</strong> (maxTransactionLockRequestTimeoutMillis)</li>
                    <li>Operations must use the same <strong>session</strong> object</li>
                    <li>Best practice: use <code>readConcern: "snapshot"</code> + <code>writeConcern: "majority"</code></li>
                </ul>
            </div>

            <div class="point">
                <strong>withTransaction() Helper (recommended)</strong>
                <p>Handles retry logic automatically for transient errors and unknown commit results.</p>
<pre><code>const session = client.startSession();
try {
  await session.withTransaction(async () => {
    await db.wallets.updateOne(
      { patientId: "p1" },
      { $inc: { balance: -500 } },
      { session }
    );
    await db.payments.insertOne(
      { patientId: "p1", amount: 500 },
      { session }
    );
  }, {
    readConcern: { level: "snapshot" },
    writeConcern: { w: "majority" }
  });
  // Auto-committed if no errors
} finally {
  session.endSession();
}</code></pre>
            </div>

            <div class="point">
                <strong>ACID Properties in MongoDB</strong>
                <ul>
                    <li><strong>Atomicity</strong> &mdash; all ops in transaction succeed or all roll back</li>
                    <li><strong>Consistency</strong> &mdash; data moves from one valid state to another</li>
                    <li><strong>Isolation</strong> &mdash; snapshot isolation &mdash; transaction sees consistent snapshot of data</li>
                    <li><strong>Durability</strong> &mdash; with <code>w: "majority", j: true</code>, committed data survives failures</li>
                </ul>
            </div>

            <div class="point">
                <strong>Transaction Limitations</strong>
                <ul>
                    <li><strong>Performance</strong> &mdash; slower than single-doc writes (~2-5x overhead)</li>
                    <li><strong>60 second timeout</strong> &mdash; long-running transactions are killed</li>
                    <li>Cannot create collections/indexes inside a transaction</li>
                    <li>Cross-shard transactions (4.2+) are even slower &mdash; need two-phase commit</li>
                    <li><strong>Write conflicts</strong> &mdash; if two transactions modify same doc, one retries or aborts</li>
                    <li>Oplog entry limit: 16MB per transaction</li>
                </ul>
            </div>

            <div class="point">
                <strong>When to Use vs When to Avoid</strong>
                <ul>
                    <li><strong>Use:</strong> Financial transfers, payment + inventory update, multi-collection consistency</li>
                    <li><strong>Avoid:</strong> If you can model as single document (embed instead). Transactions are a safety net, not a design pattern.</li>
                    <li><strong>Rule:</strong> If you need transactions frequently, rethink your schema &mdash; maybe data should be embedded</li>
                </ul>
            </div>

            <div class="point">
                <strong>Retryable Writes &amp; Reads</strong>
                <ul>
                    <li><strong>retryWrites=true</strong> (default in drivers) &mdash; auto-retries once on network errors</li>
                    <li>Works for insertOne, updateOne, deleteOne, findOneAndUpdate</li>
                    <li>Does NOT work for: insertMany(ordered), updateMany, deleteMany</li>
                    <li><strong>retryReads=true</strong> &mdash; auto-retries read operations on transient failures</li>
                </ul>
            </div>

            <!-- Interview Q&A -->
            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: Does MongoDB support ACID transactions?</div>
                    <div class="answer"><strong>Yes, fully ACID.</strong> Single-doc operations are always atomic (since v1). Multi-document transactions added in <strong>4.0</strong> (replica sets) and <strong>4.2</strong> (sharded clusters). Use snapshot read concern + majority write concern for full ACID guarantees across multiple documents/collections.</div>
                </div>

                <div class="q">
                    <div class="question">Q: When did you use transactions in your projects?</div>
                    <div class="answer">Healthcare payment flow: debit patient wallet + create payment record + update appointment status. All three must succeed or none. Also used for <strong>inventory management</strong> in eCommerce &mdash; decrement stock + create order must be atomic to prevent overselling. Both cases involved multiple collections.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Why not just use transactions everywhere like SQL?</div>
                    <div class="answer">Transactions add <strong>~2-5x overhead</strong> (locking, snapshot management, two-phase commit on shards). MongoDB is designed for <strong>single-document atomicity</strong> &mdash; embed related data to avoid transactions. Only use transactions when data genuinely spans multiple documents/collections. If you need transactions for every write, your schema needs redesign.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is snapshot isolation?</div>
                    <div class="answer">Transaction sees a <strong>consistent snapshot</strong> of data at the time it started. Other concurrent writes are invisible to the transaction. This prevents dirty reads and non-repeatable reads. Uses <code>readConcern: "snapshot"</code>. If another transaction modifies the same document, one gets a <strong>write conflict</strong> error and must retry.</div>
                </div>

                <div class="q">
                    <div class="question">Q: withTransaction() vs manual startTransaction &mdash; which to use?</div>
                    <div class="answer"><strong>Always use withTransaction()</strong> in production. It automatically handles: (1) <strong>TransientTransactionError</strong> &mdash; retries the entire transaction. (2) <strong>UnknownTransactionCommitResult</strong> &mdash; retries the commit. Manual start/commit doesn't handle these. You'd have to write retry loops yourself.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What happens during write conflict?</div>
                    <div class="answer">If Transaction A and Transaction B both try to modify the same document, <strong>first writer wins</strong>. Second transaction gets a <strong>WriteConflict</strong> error. With <code>withTransaction()</code>, it auto-retries. In manual mode, you must catch the error and retry. This is MongoDB's <strong>optimistic concurrency control</strong> &mdash; no locks held until write.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Transactions on sharded cluster &mdash; any difference?</div>
                    <div class="answer">Added in 4.2. Uses <strong>two-phase commit</strong> protocol across shards. Slower than single-shard transactions due to cross-shard coordination. Best practice: design shard key so most transactions target a <strong>single shard</strong> (e.g., shard by userId so all user-related ops hit one shard). Cross-shard transactions should be the exception.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Can you create a collection inside a transaction?</div>
                    <div class="answer"><strong>No</strong> (before 4.4). Since MongoDB 4.4, you can implicitly create collections inside transactions. But you still <strong>cannot create indexes</strong> inside a transaction. Best practice: ensure collections exist before starting transactions. Use <code>db.createCollection()</code> during deployment/migration, not at runtime.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 10 -->
        <div class="concept" id="concept-10">
            <h2>10. Security</h2>
            <span class="tag">Enterprise / Compliance</span>

            <div class="point">
                <strong>Security Layers (Defense in Depth)</strong>
                <ul>
                    <li><strong>Authentication</strong> &mdash; who are you? (SCRAM, x.509, LDAP, Kerberos)</li>
                    <li><strong>Authorization</strong> &mdash; what can you do? (RBAC &mdash; Role-Based Access Control)</li>
                    <li><strong>Encryption</strong> &mdash; protect data in transit (TLS) and at rest (AES-256)</li>
                    <li><strong>Auditing</strong> &mdash; track who did what (Enterprise feature)</li>
                    <li><strong>Network</strong> &mdash; IP whitelisting, VPC peering, private endpoints</li>
                </ul>
            </div>

            <div class="point">
                <strong>Authentication Methods</strong>
                <ul>
                    <li><strong>SCRAM-SHA-256</strong> &mdash; default. Username/password with salted challenge-response. Good for most apps.</li>
                    <li><strong>x.509 Certificates</strong> &mdash; certificate-based auth. Used for inter-node communication in replica sets and for client auth.</li>
                    <li><strong>LDAP</strong> &mdash; authenticate against Active Directory / LDAP server (Enterprise)</li>
                    <li><strong>Kerberos</strong> &mdash; single sign-on for enterprise environments (Enterprise)</li>
                </ul>
<pre><code>// Create user with SCRAM authentication
db.createUser({
  user: "appUser",
  pwd: "securePassword123",
  roles: [
    { role: "readWrite", db: "healthcare_db" },
    { role: "read", db: "analytics_db" }
  ]
})

// Enable authentication in mongod.conf
// security:
//   authorization: enabled</code></pre>
            </div>

            <div class="point">
                <strong>Role-Based Access Control (RBAC)</strong>
                <ul>
                    <li><strong>read</strong> &mdash; read-only on a database</li>
                    <li><strong>readWrite</strong> &mdash; read + write on a database</li>
                    <li><strong>dbAdmin</strong> &mdash; index management, stats, compact</li>
                    <li><strong>userAdmin</strong> &mdash; create/modify users and roles</li>
                    <li><strong>clusterAdmin</strong> &mdash; manage replica sets, sharding</li>
                    <li><strong>root</strong> &mdash; superuser (avoid in production)</li>
                </ul>
<pre><code>// Custom role: read + write on patients, read-only on audit
db.createRole({
  role: "healthcareAppRole",
  privileges: [
    { resource: { db: "healthcare_db", collection: "patients" },
      actions: ["find", "insert", "update"] },
    { resource: { db: "healthcare_db", collection: "audit_log" },
      actions: ["find"] }
  ],
  roles: []
})

// Assign custom role to user
db.createUser({
  user: "apiService",
  pwd: "secret",
  roles: [{ role: "healthcareAppRole", db: "healthcare_db" }]
})</code></pre>
            </div>

            <div class="point">
                <strong>Encryption at Rest</strong>
                <ul>
                    <li>WiredTiger encrypts data files with <strong>AES-256-CBC</strong> (Enterprise)</li>
                    <li>Atlas: encryption at rest <strong>enabled by default</strong> (AWS KMS / Azure Key Vault / GCP KMS)</li>
                    <li>Protects against physical theft of disk / unauthorized file access</li>
                </ul>
            </div>

            <div class="point">
                <strong>Encryption in Transit (TLS/SSL)</strong>
<pre><code>// mongod.conf - enable TLS
net:
  tls:
    mode: requireTLS
    certificateKeyFile: /etc/ssl/mongodb.pem
    CAFile: /etc/ssl/ca.pem

// Connection string with TLS
mongodb://user:pass@host:27017/db?tls=true&tlsCAFile=/path/ca.pem</code></pre>
                <p>Atlas: TLS <strong>always on</strong>, cannot be disabled.</p>
            </div>

            <div class="point">
                <strong>Client-Side Field Level Encryption (CSFLE)</strong>
                <p>Encrypt sensitive fields <strong>before</strong> they reach the server. Server never sees plaintext. Even DBA can't read encrypted fields.</p>
<pre><code>// Fields encrypted client-side before sending to MongoDB
{
  name: "Rajesh",                     // plaintext
  ssn: Binary("encrypted_data"),      // encrypted - server can't read
  diagnosis: Binary("encrypted_data") // encrypted
}

// Two modes:
// Deterministic: same input = same ciphertext (allows equality queries)
// Random: same input = different ciphertext (more secure, no queries)</code></pre>
            </div>

            <div class="point">
                <strong>Network Security</strong>
                <ul>
                    <li><strong>IP Whitelist</strong> &mdash; allow only known IPs (Atlas default)</li>
                    <li><strong>VPC Peering</strong> &mdash; private connection between your VPC and Atlas (no public internet)</li>
                    <li><strong>Private Endpoints</strong> &mdash; AWS PrivateLink / Azure Private Link for zero public exposure</li>
                    <li><strong>bindIp</strong> &mdash; bind mongod to specific interface (never 0.0.0.0 in production)</li>
                </ul>
            </div>

            <div class="point">
                <strong>Auditing (Enterprise / Atlas)</strong>
<pre><code>// mongod.conf - enable audit log
auditLog:
  destination: file
  format: JSON
  path: /var/log/mongodb/audit.json
  filter: '{ atype: { $in: ["authCheck", "dropCollection", "createUser"] } }'

// Tracks: authentication attempts, CRUD operations,
// schema changes, user management, config changes</code></pre>
            </div>

            <!-- Interview Q&A -->
            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How did you secure MongoDB in healthcare (HIPAA)?</div>
                    <div class="answer"><strong>1)</strong> TLS for all connections (encryption in transit). <strong>2)</strong> AES-256 encryption at rest via Atlas KMS. <strong>3)</strong> CSFLE for PII fields (SSN, diagnosis). <strong>4)</strong> RBAC &mdash; each microservice gets least-privilege role. <strong>5)</strong> VPC peering (no public internet). <strong>6)</strong> Audit logging for all data access. <strong>7)</strong> IP whitelist for Atlas cluster access.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is RBAC and how did you implement it?</div>
                    <div class="answer">Role-Based Access Control &mdash; assign permissions via roles, not directly to users. Created <strong>custom roles</strong>: API service gets readWrite on patients but read-only on audit logs. Analytics service gets read-only everywhere. No service uses root. Principle of <strong>least privilege</strong> &mdash; each user/service gets minimum permissions needed.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Client-Side Field Level Encryption &mdash; why use it?</div>
                    <div class="answer">Server <strong>never sees plaintext</strong> of sensitive data. Even if someone breaches the database or a DBA goes rogue, encrypted fields are unreadable. For healthcare: SSN, diagnosis codes, patient names encrypted client-side. Two modes: <strong>deterministic</strong> (allows equality queries on encrypted field) and <strong>random</strong> (most secure, no queries possible).</div>
                </div>

                <div class="q">
                    <div class="question">Q: Encryption at rest vs in transit vs CSFLE &mdash; differences?</div>
                    <div class="answer"><strong>At rest:</strong> encrypts data files on disk &mdash; protects against physical theft. <strong>In transit:</strong> TLS encrypts data between client and server &mdash; protects against network sniffing. <strong>CSFLE:</strong> encrypts specific fields before they leave the client &mdash; protects against server compromise, DBA access. All three together provide <strong>defense in depth</strong>.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to prevent unauthorized access in production?</div>
                    <div class="answer"><strong>1)</strong> Enable <code>authorization: enabled</code> (never run without auth). <strong>2)</strong> Use strong passwords + SCRAM-SHA-256. <strong>3)</strong> IP whitelist (never allow 0.0.0.0). <strong>4)</strong> VPC peering / private endpoints. <strong>5)</strong> Disable unused ports. <strong>6)</strong> Don't expose mongod to public internet. <strong>7)</strong> Regular rotation of credentials.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is the security checklist for production MongoDB?</div>
                    <div class="answer"><strong>1)</strong> Enable auth. <strong>2)</strong> Enable TLS. <strong>3)</strong> Use RBAC with custom roles. <strong>4)</strong> Enable encryption at rest. <strong>5)</strong> Network restriction (VPC/IP whitelist). <strong>6)</strong> Enable audit log. <strong>7)</strong> Disable HTTP interface and REST API. <strong>8)</strong> Keep MongoDB version updated. <strong>9)</strong> Use CSFLE for PII. <strong>10)</strong> Monitor and alert on suspicious access patterns.</div>
                </div>

                <div class="q">
                    <div class="question">Q: VPC Peering vs Private Endpoint?</div>
                    <div class="answer"><strong>VPC Peering:</strong> connects two VPCs &mdash; all traffic stays in private network. Simple setup but CIDR blocks can't overlap. <strong>Private Endpoint:</strong> (AWS PrivateLink) creates an endpoint in your VPC that routes to Atlas via AWS backbone. More secure &mdash; one-directional, no CIDR constraints. We used Private Endpoint for healthcare to ensure <strong>zero public internet exposure</strong>.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Can MongoDB run without authentication?</div>
                    <div class="answer">Yes, by default mongod starts <strong>without auth</strong> (localhost exception). This is a <strong>critical security risk</strong>. In 2017, thousands of MongoDB instances were ransomed because they were exposed without auth. Always enable <code>security.authorization: enabled</code> in production. Atlas enforces auth by default &mdash; cannot disable.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 11 -->
        <div class="concept" id="concept-11">
            <h2>11. Kafka + MongoDB</h2>
            <span class="tag">Event-Driven Architecture</span>

            <div class="point">
                <strong>Why Kafka + MongoDB?</strong>
                <p>Kafka = distributed event streaming. MongoDB = flexible document store. Together: <strong>event-driven microservices</strong> where Kafka moves data between services and MongoDB stores it. Decouples producers from consumers.</p>
            </div>

            <div class="point">
                <strong>Common Patterns</strong>
                <ul>
                    <li><strong>CDC (Change Data Capture):</strong> MongoDB Change Streams push events to Kafka topics. Any change in MongoDB automatically flows to downstream consumers.</li>
                    <li><strong>Event Sourcing:</strong> Kafka stores all events as source of truth. MongoDB stores current state (materialized view).</li>
                    <li><strong>CQRS:</strong> Write to MongoDB via Kafka commands. Read from MongoDB directly. Separate write and read paths.</li>
                    <li><strong>Data Pipeline:</strong> Kafka ingests from multiple sources &rarr; transforms &rarr; sinks into MongoDB for querying.</li>
                </ul>
            </div>

            <div class="point">
                <strong>MongoDB Kafka Connector (Source)</strong>
                <p>Reads Change Streams from MongoDB and publishes to Kafka topics automatically.</p>
<pre><code>// MongoDB Source Connector config
{
  "name": "mongo-source",
  "config": {
    "connector.class": "com.mongodb.kafka.connect.MongoSourceConnector",
    "connection.uri": "mongodb+srv://user:pass@cluster.mongodb.net",
    "database": "healthcare_db",
    "collection": "patients",
    "pipeline": "[{\"$match\": {\"operationType\": {\"$in\": [\"insert\", \"update\"]}}}]",
    "publish.full.document.only": true,
    "topic.prefix": "mongo"
  }
}
// Every insert/update on patients → Kafka topic "mongo.healthcare_db.patients"</code></pre>
            </div>

            <div class="point">
                <strong>MongoDB Kafka Connector (Sink)</strong>
                <p>Consumes from Kafka topics and writes to MongoDB collections.</p>
<pre><code>// MongoDB Sink Connector config
{
  "name": "mongo-sink",
  "config": {
    "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",
    "connection.uri": "mongodb+srv://user:pass@cluster.mongodb.net",
    "database": "analytics_db",
    "collection": "events",
    "topics": "user-clicks,page-views,orders",
    "writemodel.strategy": "com.mongodb.kafka.connect.sink.writemodel.strategy.UpsertAsPartOfConditionStrategy",
    "document.id.strategy": "com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInKeyStrategy"
  }
}</code></pre>
            </div>

            <div class="point">
                <strong>Manual Integration (Node.js)</strong>
<pre><code>// Producer: Push MongoDB changes to Kafka
const changeStream = db.collection("orders").watch();
changeStream.on("change", async (event) => {
  await producer.send({
    topic: "order-events",
    messages: [{
      key: event.documentKey._id.toString(),
      value: JSON.stringify(event.fullDocument)
    }]
  });
});

// Consumer: Read from Kafka, write to MongoDB
await consumer.subscribe({ topic: "order-events" });
await consumer.run({
  eachMessage: async ({ message }) => {
    const order = JSON.parse(message.value);
    await db.collection("order_analytics").updateOne(
      { _id: order._id },
      { $set: order },
      { upsert: true }
    );
  }
});</code></pre>
            </div>

            <div class="point">
                <strong>Key Kafka Concepts</strong>
                <ul>
                    <li><strong>Topic:</strong> Named stream of events (like a collection)</li>
                    <li><strong>Partition:</strong> Topic split for parallelism. Order guaranteed within partition.</li>
                    <li><strong>Consumer Group:</strong> Multiple consumers share work. Each partition read by one consumer in group.</li>
                    <li><strong>Offset:</strong> Position in partition. Consumer tracks where it is. Can replay from any offset.</li>
                    <li><strong>At-least-once:</strong> Default delivery. Handle duplicates with idempotent writes (upsert in MongoDB).</li>
                </ul>
            </div>

            <!-- Interview Q&A -->
            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: How did you use Kafka with MongoDB in your projects?</div>
                    <div class="answer"><strong>eCommerce (Beckett):</strong> Kafka ingested real-time price updates from multiple vendors. Consumers processed and wrote to MongoDB for the product catalog. Change Streams on order collection pushed events to Kafka for inventory sync across 5 storefronts. <strong>Healthcare:</strong> Medical records ingested via Kafka, NLP service consumed and wrote coded results back to MongoDB.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Change Streams vs Kafka Connector &mdash; when to use which?</div>
                    <div class="answer"><strong>Change Streams:</strong> Use when your app directly needs to react to DB changes (cache invalidation, notifications). Runs in your app process. <strong>Kafka Connector:</strong> Use for system-to-system data flow, decoupled architecture, replay capability. Managed by Kafka Connect framework, no custom code needed. We used Connector for cross-service sync, Change Streams for real-time UI updates.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to handle duplicate messages from Kafka?</div>
                    <div class="answer">Kafka guarantees <strong>at-least-once</strong> delivery by default. Duplicates happen on retries. Solution: make MongoDB writes <strong>idempotent</strong>. Use <code>updateOne with upsert</code> instead of insertOne. Or use Kafka message key as MongoDB <code>_id</code> &mdash; duplicate inserts fail gracefully. We used this pattern for all event consumers in the collectibles platform.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to guarantee ordering in Kafka + MongoDB?</div>
                    <div class="answer">Kafka guarantees order <strong>within a partition</strong>. Use the MongoDB document <code>_id</code> as Kafka message key &mdash; all events for same document go to same partition, processed in order. For our order processing: orderId as key ensured all events for one order (created → paid → shipped) were processed sequentially.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What happens if Kafka consumer crashes mid-processing?</div>
                    <div class="answer">Consumer offset is committed <strong>after processing</strong>. If crash before commit, Kafka re-delivers the message on restart (at-least-once). With MongoDB upsert, re-processing is safe. For critical flows: use Kafka transactions + MongoDB transactions together to ensure exactly-once semantics (more complex, use only when needed).</div>
                </div>

                <div class="q">
                    <div class="question">Q: CDC pattern &mdash; explain with a real example.</div>
                    <div class="answer"><strong>CDC (Change Data Capture):</strong> Capture every change in source DB and propagate. In our healthcare system: when a patient record was updated in MongoDB, Change Stream captured it → published to Kafka topic → analytics service consumed and updated dashboards → notification service sent alerts to care team. Source system didn't need to know about consumers. Fully decoupled.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to scale Kafka consumers reading into MongoDB?</div>
                    <div class="answer">Increase Kafka partitions for the topic, then add consumers to the consumer group (max = partition count). Each consumer writes to MongoDB independently. Use <strong>bulkWrite</strong> in consumer for batching. For our clickstream pipeline: 12 partitions, 12 consumers, each bulk-writing 1000 events to MongoDB per batch. Handled 50K events/sec.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 12 -->
        <div class="concept" id="concept-12">
            <h2>12. MongoDB vs Elasticsearch</h2>
            <span class="tag">Technology Comparison</span>

            <div class="point">
                <strong>Core Difference</strong>
                <ul>
                    <li><strong>MongoDB:</strong> General-purpose document database. Primary data store. ACID transactions. Rich query language (MQL). Best for CRUD + aggregation.</li>
                    <li><strong>Elasticsearch:</strong> Search &amp; analytics engine (Lucene-based). Inverted index. Best for full-text search, log analytics, faceted search. Not ideal as primary data store.</li>
                </ul>
            </div>

            <div class="point">
                <strong>When to Use MongoDB</strong>
                <ul>
                    <li>Primary database for application data</li>
                    <li>Complex queries, joins ($lookup), aggregations</li>
                    <li>ACID transactions across documents</li>
                    <li>Flexible schema with validation</li>
                    <li>Real-time CRUD operations</li>
                    <li>Sharding for horizontal scale</li>
                </ul>
            </div>

            <div class="point">
                <strong>When to Use Elasticsearch</strong>
                <ul>
                    <li>Full-text search with relevance scoring</li>
                    <li>Log aggregation &amp; analytics (ELK stack)</li>
                    <li>Autocomplete, fuzzy matching, synonyms</li>
                    <li>Faceted search (filters with counts)</li>
                    <li>Geo-spatial search at scale</li>
                    <li>Near real-time analytics dashboards</li>
                </ul>
            </div>

            <div class="point">
                <strong>Side-by-Side Comparison</strong>
<pre><code>Feature          | MongoDB              | Elasticsearch
-----------------+----------------------+---------------------
Data Model       | BSON documents       | JSON documents
Query Language   | MQL + Aggregation    | Query DSL (JSON)
Indexing         | B-Tree, Compound     | Inverted Index
Transactions     | Full ACID (4.0+)     | No transactions
Joins            | $lookup              | No native joins
Full-Text Search | Atlas Search / text  | Core strength
Scoring/Ranking  | Basic                | Advanced (BM25, TF-IDF)
Sharding         | Native               | Native
Replication      | Replica Sets         | Primary/Replica shards
Consistency      | Strong (primary)     | Eventually consistent
Use Case         | Primary DB, CRUD     | Search, Analytics</code></pre>
            </div>

            <div class="point">
                <strong>Using Both Together (Common Pattern)</strong>
<pre><code>// Architecture: MongoDB (source of truth) + ES (search layer)
// 1. App writes to MongoDB
await db.collection("products").insertOne(product);

// 2. Sync to Elasticsearch (via Kafka or Change Streams)
changeStream.on("change", async (event) => {
  await esClient.index({
    index: "products",
    id: event.documentKey._id.toString(),
    body: event.fullDocument
  });
});

// 3. Search queries go to ES
const results = await esClient.search({
  index: "products",
  body: { query: { multi_match: { query: "rare vintage card", fields: ["title", "description"] } } }
});

// 4. Detail/write queries go to MongoDB
const product = await db.collection("products").findOne({ _id: id });</code></pre>
            </div>

            <div class="point">
                <strong>Atlas Search vs Elasticsearch</strong>
                <ul>
                    <li><strong>Atlas Search:</strong> Built into MongoDB Atlas. Lucene-based. No separate cluster. Use <code>$search</code> in aggregation pipeline. Simpler ops. Good for 80% of search needs.</li>
                    <li><strong>Elasticsearch:</strong> Separate cluster to manage. More powerful for complex scoring, analytics, log aggregation. Better for dedicated search-heavy workloads.</li>
                    <li><strong>Rule:</strong> If search is secondary feature → Atlas Search. If search IS the product → Elasticsearch.</li>
                </ul>
            </div>

            <!-- Interview Q&A -->
            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: Why did you use both MongoDB and Elasticsearch in Beckett?</div>
                    <div class="answer">MongoDB was <strong>source of truth</strong> for product catalog (10M+ cards), orders, user data &mdash; needed ACID transactions for purchases. Elasticsearch powered <strong>search</strong>: autocomplete on card names, faceted filtering (year, sport, brand, price range), relevance scoring. Users searched in ES, then fetched full product details from MongoDB. Kafka synced data between them.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Can MongoDB replace Elasticsearch entirely?</div>
                    <div class="answer">For <strong>basic search</strong>: yes, with Atlas Search or text indexes. For <strong>advanced use cases</strong> (complex relevance tuning, custom analyzers, synonyms, log analytics, Kibana dashboards): no. MongoDB is a database first, search second. Elasticsearch is search first. In healthcare, we used Atlas Search for patient lookup &mdash; sufficient. In eCommerce, we needed ES for full-featured product search.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to keep MongoDB and Elasticsearch in sync?</div>
                    <div class="answer"><strong>Option 1:</strong> MongoDB Kafka Source Connector → Kafka → ES Sink Connector (recommended, decoupled). <strong>Option 2:</strong> Change Streams → custom code writes to ES. <strong>Option 3:</strong> Dual-write in app code (risky &mdash; if ES write fails, data inconsistent). We used Option 1 with Kafka. Lag was under 500ms. For conflict resolution: MongoDB is always source of truth, ES is rebuilt from MongoDB if needed.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Inverted index vs B-Tree index &mdash; difference?</div>
                    <div class="answer"><strong>B-Tree (MongoDB):</strong> Stores values in sorted tree. Great for exact match, range queries, sorting. O(log n) lookup. <strong>Inverted Index (ES):</strong> Maps every term to list of documents containing it. Great for "find all docs containing word X". O(1) for term lookup. That's why ES is faster for text search but MongoDB is faster for exact field queries and range scans.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Performance comparison for typical operations?</div>
                    <div class="answer"><strong>CRUD by _id:</strong> MongoDB faster (B-Tree direct lookup). <strong>Full-text search:</strong> ES much faster (inverted index optimized). <strong>Aggregation:</strong> Both capable, MongoDB better for complex pipelines with $lookup. ES better for metric aggregations (avg, percentile). <strong>Write speed:</strong> MongoDB faster (ES needs to update inverted index). <strong>Bulk analytics:</strong> ES wins with Kibana integration.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What is eventual consistency in ES and how does it affect architecture?</div>
                    <div class="answer">ES has a <strong>refresh interval</strong> (default 1 second). New documents aren't searchable until refreshed. This means: write to ES → search immediately → may not find it. For our eCommerce: after adding a product, we showed a success page from MongoDB data (consistent), not from ES search results. Search results caught up within 1 second. Design for this lag.</div>
                </div>

                <div class="q">
                    <div class="question">Q: When would you NOT use Elasticsearch?</div>
                    <div class="answer"><strong>1)</strong> As primary data store (no ACID, data loss risk). <strong>2)</strong> When you need transactions. <strong>3)</strong> Simple CRUD apps without search needs. <strong>4)</strong> When team doesn't have ES expertise (operational overhead is high). <strong>5)</strong> Cost-sensitive projects (ES clusters are expensive). In these cases, MongoDB alone with text indexes or Atlas Search is sufficient.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 13 -->
        <div class="concept" id="concept-13">
            <h2>13. Project Scenarios</h2>
            <span class="tag">Real-World Application</span>

            <div class="point">
                <strong>Scenario 1: Healthcare Medical Coding Platform</strong>
                <p><strong>Problem:</strong> Process 100K+ clinical documents daily, extract medical codes (ICD-10, CPT) using NLP, store results for auditors to review.</p>
<pre><code>// Architecture:
// Ingestion → Kafka → NLP Service → MongoDB → Dashboard

// Patient document schema
{
  _id: ObjectId,
  mrn: "MRN-12345",
  documents: [{
    type: "progress_note",       // polymorphic
    content: "Patient presents with...",
    receivedAt: ISODate,
    nlpResults: {
      codes: [
        { code: "E11.9", desc: "Type 2 diabetes", confidence: 0.95 },
        { code: "I10", desc: "Hypertension", confidence: 0.87 }
      ],
      processedAt: ISODate,
      modelVersion: "v3.2"
    },
    auditStatus: "pending_review"
  }]
}

// Key indexes
db.patients.createIndex({ "documents.auditStatus": 1, "documents.receivedAt": -1 })
db.patients.createIndex({ mrn: 1 }, { unique: true })

// Aggregation: Daily coding accuracy report
db.patients.aggregate([
  { $unwind: "$documents" },
  { $match: { "documents.receivedAt": { $gte: today } } },
  { $group: {
      _id: "$documents.auditStatus",
      count: { $sum: 1 },
      avgConfidence: { $avg: { $max: "$documents.nlpResults.codes.confidence" } }
  }}
])</code></pre>
            </div>

            <div class="point">
                <strong>Scenario 2: eCommerce Collectibles Marketplace</strong>
                <p><strong>Problem:</strong> 10M+ product listings, real-time pricing, search with facets, inventory sync across 5 storefronts.</p>
<pre><code>// Product schema (MongoDB - source of truth)
{
  _id: ObjectId,
  title: "1986 Fleer Michael Jordan #57",
  sport: "Basketball",
  year: 1986,
  brand: "Fleer",
  player: "Michael Jordan",
  prices: {
    raw: { low: 3000, mid: 8000, high: 25000 },
    graded: { psa10: 450000, psa9: 45000 }
  },
  inventory: [
    { storefront: "us", qty: 12, updatedAt: ISODate },
    { storefront: "eu", qty: 3, updatedAt: ISODate }
  ],
  priceHistory: []  // moved to separate collection (subset pattern)
}

// Sharding: by sport (zone-based)
sh.shardCollection("marketplace.products", { sport: 1, _id: 1 })
// Zone: "basketball" → shard1, "baseball" → shard2

// Kafka flow:
// Price update event → Kafka "price-updates" topic
// Consumer updates MongoDB → Change Stream → Kafka → ES sync
// Inventory event → Kafka "inventory-sync" → all 5 storefronts</code></pre>
            </div>

            <div class="point">
                <strong>Scenario 3: Enterprise DAM (Digital Asset Management)</strong>
                <p><strong>Problem:</strong> 500K+ digital assets across 8 brands, metadata taxonomy, approval workflows, CDN delivery.</p>
<pre><code>// Asset schema
{
  _id: ObjectId,
  filename: "dabur-honey-campaign-2024.jpg",
  brand: "Dabur Honey",
  type: "image",
  metadata: {
    campaign: "Summer 2024",
    tags: ["honey", "health", "summer"],
    dimensions: { w: 1920, h: 1080 },
    format: "jpeg",
    size: 2400000
  },
  workflow: {
    status: "approved",         // draft → review → approved → published
    submittedBy: "marketing_in",
    approvedBy: "brand_manager",
    publishedAt: ISODate
  },
  acl: {
    brand: "dabur_honey",       // multi-tenant isolation
    roles: ["marketing", "brand_manager"]
  },
  cdnUrl: "https://cdn.dabur.com/assets/..."
}

// Search with Elasticsearch
// Facets: brand, type, campaign, status, tags
// Autocomplete on filename and tags</code></pre>
            </div>

            <div class="point">
                <strong>Scenario 4: Real-Time Analytics Dashboard</strong>
<pre><code>// Clickstream events collected via Kafka
// Stored in MongoDB with TTL index (auto-delete after 90 days)
{
  userId: "u123",
  event: "product_view",
  productId: "p456",
  timestamp: ISODate,
  sessionId: "sess-789",
  metadata: { source: "search", device: "mobile" }
}

db.clickstream.createIndex({ timestamp: 1 }, { expireAfterSeconds: 7776000 })

// Real-time dashboard aggregation
db.clickstream.aggregate([
  { $match: { timestamp: { $gte: lastHour } } },
  { $group: {
      _id: { event: "$event", minute: { $dateTrunc: { date: "$timestamp", unit: "minute" } } },
      count: { $sum: 1 },
      uniqueUsers: { $addToSet: "$userId" }
  }},
  { $project: { event: "$_id.event", minute: "$_id.minute", count: 1, uniqueUsers: { $size: "$uniqueUsers" } } },
  { $merge: { into: "dashboard_metrics" } }  // materialized view
])</code></pre>
            </div>

            <!-- Interview Q&A -->
            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q: Walk through your healthcare platform architecture end-to-end.</div>
                    <div class="answer"><strong>Ingestion:</strong> Clinical docs received via HL7/FHIR APIs → Kafka topic. <strong>Processing:</strong> NLP service consumes, extracts ICD-10/CPT codes, writes results to MongoDB Atlas. <strong>Storage:</strong> Polymorphic schema handles 20+ doc types. Field-level encryption for PHI. <strong>Query:</strong> Aggregation pipelines for accuracy reports, Atlas Search for diagnosis lookup. <strong>Dashboard:</strong> React frontend reads from MongoDB via Node.js API. <strong>Infra:</strong> AWS, MongoDB Atlas M30, Kafka on MSK, Datadog monitoring.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How did you handle schema for 20+ document types?</div>
                    <div class="answer"><strong>Polymorphic pattern.</strong> Common fields at top level (mrn, receivedAt, auditStatus). Type-specific fields in nested objects. Discriminator field <code>type</code> identifies the document type. Single collection with compound index on <code>{type: 1, auditStatus: 1}</code>. Avoided 20 separate collections &mdash; would have complicated aggregation and reporting across types.</div>
                </div>

                <div class="q">
                    <div class="question">Q: Why zone-based sharding for the marketplace?</div>
                    <div class="answer">Different sports have different query volumes (basketball = 40% of traffic, baseball = 30%). Zone sharding lets us place <strong>hot data on powerful shards</strong>. Also, regional storefronts mostly query their sport &mdash; EU users search football, US users search baseball/basketball. Zone sharding + read preference "nearest" minimized cross-shard queries. Most queries were <strong>targeted to single shard</strong>.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How did you handle data consistency across MongoDB and ES?</div>
                    <div class="answer">MongoDB = <strong>source of truth</strong>. ES = read-optimized copy. Sync via Kafka (MongoDB Source Connector → Kafka → ES Sink Connector). If ES gets out of sync: full re-index from MongoDB. Designed frontend to handle ES eventual consistency &mdash; after writes, show data from MongoDB directly, not from ES search. Sync lag was typically under 500ms.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How would you design a notification system with MongoDB + Kafka?</div>
                    <div class="answer"><strong>1)</strong> Change Stream on "orders" collection captures status changes. <strong>2)</strong> Publish to Kafka "notification-events" topic. <strong>3)</strong> Multiple consumers: email service, SMS service, push notification service &mdash; each in separate consumer group. <strong>4)</strong> Store notification history in MongoDB with TTL index. <strong>5)</strong> Idempotent sends &mdash; use orderId + eventType as dedup key. Fully decoupled &mdash; adding a new notification channel = adding a new consumer.</div>
                </div>

                <div class="q">
                    <div class="question">Q: How to handle data migration from MySQL to MongoDB?</div>
                    <div class="answer"><strong>Phase 1:</strong> Analyze MySQL schema, identify embedding opportunities (normalize 200 tables → ~30 collections). <strong>Phase 2:</strong> Write migration scripts (Python + pymongo), transform relational rows to documents. <strong>Phase 3:</strong> Dual-write period &mdash; app writes to both MySQL and MongoDB. <strong>Phase 4:</strong> Validate data parity. <strong>Phase 5:</strong> Switch reads to MongoDB. <strong>Phase 6:</strong> Decommission MySQL. Took 6 months for Beckett. Result: 60% less schema complexity, 8x faster API responses.</div>
                </div>

                <div class="q">
                    <div class="question">Q: If you were designing from scratch today, what would you change?</div>
                    <div class="answer"><strong>1)</strong> Use MongoDB Atlas Serverless for variable workloads (pay-per-op). <strong>2)</strong> Atlas Search instead of separate ES cluster (simpler ops). <strong>3)</strong> Atlas Triggers instead of custom Kafka consumers for simple event handling. <strong>4)</strong> CSFLE from day one for all PII. <strong>5)</strong> Time-series collections for clickstream instead of regular collections. <strong>6)</strong> Online Archive for historical data from day one. Basically: leverage more Atlas-native features to reduce operational overhead.</div>
                </div>

                <div class="q">
                    <div class="question">Q: What's your approach when something goes wrong in production?</div>
                    <div class="answer"><strong>1) Detect:</strong> Datadog alerts on latency spike / error rate. <strong>2) Triage:</strong> Check MongoDB Atlas metrics (connections, opcounters, replication lag). <strong>3) Diagnose:</strong> Profiler for slow queries, <code>currentOp()</code> for blocked operations, oplog for replication issues. <strong>4) Fix:</strong> Kill long-running queries, add missing indexes, scale up if needed. <strong>5) Prevent:</strong> Add monitoring for the new pattern, update runbook. Real example: found COLLSCAN on 10M collection during peak &mdash; added compound index, response dropped from 12s to 8ms.</div>
                </div>
            </div>
        </div>

    </main>
</div>

<script>
    // Mobile menu toggle
    function toggleMenu() {
        document.querySelector('.sidebar').classList.toggle('open');
        document.querySelector('.sidebar-overlay').classList.toggle('show');
    }

    // Close sidebar on link click (mobile)
    document.querySelectorAll('.sidebar nav a').forEach(a => {
        a.addEventListener('click', () => {
            if (window.innerWidth <= 768) {
                document.querySelector('.sidebar').classList.remove('open');
                document.querySelector('.sidebar-overlay').classList.remove('show');
            }
        });
    });

    // Highlight active sidebar link on scroll
    const links = document.querySelectorAll('.sidebar nav a');
    const sections = document.querySelectorAll('.concept');
    window.addEventListener('scroll', () => {
        let current = '';
        sections.forEach(s => {
            if (window.scrollY >= s.offsetTop - 100) current = s.id;
        });
        links.forEach(a => {
            a.classList.remove('active');
            if (a.getAttribute('href') === '#' + current) a.classList.add('active');
        });
    });
</script>
</body>
</html>