<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AWS Interview Prep</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code:wght@400;500&display=swap');
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Inter', sans-serif; background: #f8f9fa; color: #1e293b; line-height: 1.6; }

        .layout { display: flex; min-height: 100vh; }

        /* Sidebar */
        .sidebar {
            width: 260px;
            min-width: 260px;
            background: #fff;
            border-right: 1px solid #e2e8f0;
            padding: 24px 0;
            position: fixed;
            top: 0;
            left: 0;
            height: 100vh;
            overflow-y: auto;
        }
        .sidebar h2 { font-size: 15px; color: #0f766e; padding: 0 20px; margin-bottom: 4px; }
        .sidebar .sub { font-size: 11px; color: #94a3b8; padding: 0 20px; margin-bottom: 20px; }
        .sidebar nav a {
            display: block;
            padding: 9px 20px;
            font-size: 13px;
            color: #475569;
            text-decoration: none;
            border-left: 3px solid transparent;
            transition: all 0.2s;
        }
        .sidebar nav a:hover { background: #f0fdf4; color: #0f766e; border-left-color: #0f766e; }
        .sidebar nav a.active { background: #f0fdf4; color: #0f766e; border-left-color: #0f766e; font-weight: 600; }
        .sidebar nav a.pending { color: #cbd5e1; }

        /* Main content */
        .main { margin-left: 260px; padding: 30px 40px; max-width: 820px; }

        /* Concept block */
        .concept { background: #fff; border-radius: 10px; padding: 28px; margin-bottom: 24px; border: 1px solid #e2e8f0; }
        .concept h2 { font-size: 20px; color: #0f766e; margin-bottom: 4px; }
        .concept .tag { display: inline-block; background: #f0fdf4; color: #0f766e; font-size: 11px; padding: 2px 10px; border-radius: 20px; margin-bottom: 14px; font-weight: 500; }

        /* Simple points */
        .point { margin-bottom: 14px; }
        .point strong { color: #0f766e; font-size: 13px; }
        .point p { font-size: 13px; color: #475569; margin-top: 2px; }
        .point ul { padding-left: 18px; margin-top: 4px; }
        .point li { font-size: 12.5px; color: #475569; margin-bottom: 3px; }
        .point li strong { color: #1e293b; }

        /* Code */
        pre { background: #f1f5f9; border-radius: 8px; padding: 14px; overflow-x: auto; margin: 10px 0; border: 1px solid #e2e8f0; }
        code { font-family: 'Fira Code', monospace; font-size: 12px; color: #0f766e; }

        /* Q&A */
        .qa h3 { font-size: 13px; color: #b45309; margin: 18px 0 10px; text-transform: uppercase; letter-spacing: 1px; border-top: 1px solid #e2e8f0; padding-top: 16px; }
        .q { background: #fffbeb; border-radius: 8px; padding: 12px 14px; margin-bottom: 8px; border: 1px solid #fde68a; }
        .q .question { font-size: 13px; font-weight: 600; color: #1e293b; margin-bottom: 6px; }
        .q .answer { font-size: 12.5px; color: #57534e; }
        .q .answer strong { color: #1e293b; }

        /* Project context block */
        .project-block { background: #eff6ff; border-radius: 10px; padding: 16px 18px; margin: 16px 0; border: 1px solid #bfdbfe; border-left: 4px solid #2563eb; }
        .project-block h4 { font-size: 13px; color: #1e40af; margin-bottom: 8px; font-weight: 700; letter-spacing: 0.3px; }
        .project-block p { font-size: 12.5px; color: #1e3a5f; margin-bottom: 6px; line-height: 1.6; }
        .project-block p strong { color: #1e40af; }
        .project-block pre { background: #dbeafe; border: 1px solid #93c5fd; color: #1e3a5f; font-size: 11.5px; }
        .project-block ul { padding-left: 18px; margin: 4px 0 6px; }
        .project-block li { font-size: 12.5px; color: #1e3a5f; margin-bottom: 3px; }
        .project-block li strong { color: #1e40af; }
        .project-block .arch-label { font-size: 11px; color: #6b7280; font-weight: 600; text-transform: uppercase; letter-spacing: 0.5px; margin-bottom: 4px; }

        /* Comparison table */
        .compare-table { width: 100%; border-collapse: collapse; margin: 10px 0; font-size: 12px; }
        .compare-table th { background: #e0f2fe; color: #1e40af; padding: 8px 10px; text-align: left; border: 1px solid #bfdbfe; }
        .compare-table td { padding: 6px 10px; border: 1px solid #e2e8f0; color: #475569; }

        /* Mobile menu button */
        .menu-btn {
            display: none;
            position: fixed;
            top: 12px;
            left: 12px;
            z-index: 1001;
            background: #0f766e;
            color: #fff;
            border: none;
            border-radius: 8px;
            width: 40px;
            height: 40px;
            cursor: pointer;
            font-size: 20px;
            align-items: center;
            justify-content: center;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }
        .menu-btn span { line-height: 1; }

        /* Overlay */
        .sidebar-overlay {
            display: none;
            position: fixed;
            top: 0; left: 0; right: 0; bottom: 0;
            background: rgba(0,0,0,0.4);
            z-index: 999;
        }
        .sidebar-overlay.show { display: block; }

        /* Responsive */
        @media (max-width: 768px) {
            .menu-btn { display: flex; }
            .sidebar {
                display: block;
                transform: translateX(-100%);
                transition: transform 0.3s ease;
                z-index: 1000;
            }
            .sidebar.open { transform: translateX(0); }
            .main { margin-left: 0; padding: 60px 16px 20px; }
            .concept { padding: 18px; }
            pre { padding: 10px; font-size: 11px; }
        }
    </style>
</head>
<body>
<button class="menu-btn" onclick="toggleMenu()" aria-label="Toggle menu"><span>&#9776;</span></button>
<div class="sidebar-overlay" onclick="toggleMenu()"></div>

<div class="layout">

    <!-- Sidebar -->
    <aside class="sidebar">
        <h2>AWS Prep</h2>
        <p class="sub">Cloud Architect Guide</p>
        <nav>
            <a href="#concept-1" class="active">1. EC2 & Compute</a>
            <a href="#concept-2">2. VPC & Networking</a>
            <a href="#concept-3">3. S3 & Storage</a>
            <a href="#concept-4">4. IAM & Security</a>
            <a href="#concept-5">5. ELB & Auto Scaling</a>
            <a href="#concept-6">6. Route 53 & DNS</a>
            <a href="#concept-7">7. RDS & Aurora</a>
            <a href="#concept-8">8. DynamoDB</a>
            <a href="#concept-9" class="pending">9. Lambda & Serverless</a>
            <a href="#concept-10" class="pending">10. API Gateway</a>
            <a href="#concept-11" class="pending">11. SQS, SNS & EventBridge</a>
            <a href="#concept-12" class="pending">12. ECS & EKS</a>
            <a href="#concept-13" class="pending">13. CloudFront & CDN</a>
            <a href="#concept-14" class="pending">14. CloudWatch & Monitoring</a>
            <a href="#concept-15" class="pending">15. CloudFormation & IaC</a>
            <a href="#concept-16" class="pending">16. ElastiCache</a>
            <a href="#concept-17" class="pending">17. Kinesis & Streaming</a>
            <a href="#concept-18" class="pending">18. Security Deep Dive</a>
            <a href="#concept-19" class="pending">19. Well-Architected Framework</a>
            <a href="#concept-20" class="pending">20. Architecture Scenarios</a>
        </nav>
    </aside>

    <!-- Main Content -->
    <main class="main">

        <!-- CONCEPT 1 -->
        <div class="concept" id="concept-1">
            <h2>1. EC2 & Compute</h2>
            <span class="tag">Core Compute</span>

            <div class="point">
                <strong>What is EC2?</strong>
                <p>Elastic Compute Cloud &mdash; virtual servers in the cloud. You choose instance type (CPU, RAM, storage, network), OS (AMI), and pay for what you use. Foundation of AWS compute.</p>
            </div>

            <div class="point">
                <strong>Instance Families</strong>
                <ul>
                    <li><strong>General Purpose (T3, M5, M6i):</strong> Balanced CPU/memory. Web servers, small DBs. T3 has burstable CPU with credits.</li>
                    <li><strong>Compute Optimized (C5, C6i):</strong> High CPU. Batch processing, ML inference, gaming servers.</li>
                    <li><strong>Memory Optimized (R5, R6i, X1):</strong> High RAM. In-memory databases (Redis, SAP HANA), real-time analytics.</li>
                    <li><strong>Storage Optimized (I3, D2):</strong> High sequential read/write. Data warehousing, distributed file systems.</li>
                    <li><strong>Accelerated (P4, G5, Inf1):</strong> GPU/custom chips. ML training, video encoding, graphics rendering.</li>
                </ul>
            </div>

            <div class="point">
                <strong>Pricing Models</strong>
                <ul>
                    <li><strong>On-Demand:</strong> Pay per second/hour. No commitment. Best for unpredictable, short-term workloads.</li>
                    <li><strong>Reserved (1yr / 3yr):</strong> Up to 72% discount. Commit to instance type in a region. Standard or Convertible.</li>
                    <li><strong>Savings Plans:</strong> Commit to $/hour spend (not instance type). More flexible than Reserved. Compute or EC2 plans.</li>
                    <li><strong>Spot Instances:</strong> Up to 90% discount. AWS can reclaim with 2-min notice. Best for batch jobs, CI/CD, stateless workers.</li>
                    <li><strong>Dedicated Hosts:</strong> Physical server dedicated to you. For licensing (Windows Server, Oracle) or compliance.</li>
                </ul>
            </div>

            <div class="point">
                <strong>AMI (Amazon Machine Image)</strong>
                <p>Template containing OS, software, config. Launch instances from AMI. You can create custom AMIs for fast, consistent deployments. Region-specific &mdash; copy across regions for DR.</p>
<pre><code># Create AMI from running instance
aws ec2 create-image --instance-id i-1234567890abcdef0 \
  --name "my-app-v2.1" --no-reboot

# Launch from custom AMI
aws ec2 run-instances --image-id ami-0abcdef1234567890 \
  --instance-type t3.medium --key-name my-key \
  --security-group-ids sg-903004f8 --count 1</code></pre>
            </div>

            <div class="point">
                <strong>EBS (Elastic Block Store) vs Instance Store</strong>
                <ul>
                    <li><strong>EBS:</strong> Network-attached persistent storage. Survives instance stop/terminate. Snapshot to S3 for backup. Types: gp3 (general SSD), io2 (provisioned IOPS), st1 (throughput HDD), sc1 (cold HDD).</li>
                    <li><strong>Instance Store:</strong> Physically attached to host. Highest I/O performance. Data lost on stop/terminate. Use for temp data, caches, buffers.</li>
                    <li><strong>gp3 (default):</strong> 3000 IOPS baseline, 125 MB/s. Scale IOPS/throughput independently. Most workloads.</li>
                    <li><strong>io2 Block Express:</strong> Up to 256,000 IOPS. Mission-critical databases.</li>
                </ul>
            </div>

            <div class="point">
                <strong>Placement Groups</strong>
                <ul>
                    <li><strong>Cluster:</strong> All instances in same rack. Lowest latency (10 Gbps). HPC, tightly coupled workloads.</li>
                    <li><strong>Spread:</strong> Each instance on different hardware. Max 7 per AZ. Critical apps that need isolation.</li>
                    <li><strong>Partition:</strong> Instances spread across partitions (racks). Big data (Hadoop, Kafka, Cassandra).</li>
                </ul>
            </div>

            <div class="point">
                <strong>User Data & Metadata</strong>
<pre><code># User Data: bootstrap script runs on first launch
#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
echo "Hello from $(hostname)" > /var/www/html/index.html

# Instance Metadata: query info about running instance
curl http://169.254.169.254/latest/meta-data/instance-id
curl http://169.254.169.254/latest/meta-data/public-ipv4
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/role-name
# IMDSv2 (recommended - token-based, more secure)
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")
curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/</code></pre>
            </div>

            <div class="point">
                <strong>Security Groups vs NACLs (quick ref)</strong>
                <ul>
                    <li><strong>Security Group:</strong> Instance-level firewall. <strong>Stateful</strong> (return traffic auto-allowed). Allow rules only. Default: deny all inbound, allow all outbound.</li>
                    <li><strong>NACL:</strong> Subnet-level firewall. <strong>Stateless</strong> (must define both directions). Allow + Deny rules. Evaluated by rule number (lowest first).</li>
                </ul>
            </div>

            <div class="project-block">
                <h4>In Your Project: MedCode AI Platform</h4>
                <p class="arch-label">How EC2 fits in the Medical Coding Pipeline (250 Processing Units)</p>
                <pre>
  FULL PIPELINE:
  Client Upload â†’ S3 â†’ SQS-1 â†’ Lambda(Textract) â†’ SQS-2 â†’ Java App â†’ AI â†’ DB â†’ Downstream

  WHERE EC2 RUNS:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  EC2: Java Sectionizer App (250 units)                       â”‚
  â”‚  Instance: c5.xlarge (4 vCPU, 8GB) Ã— 250 units in ASG       â”‚
  â”‚  Picks from SQS-2 â†’ sectionizes medical text (HPI,          â”‚
  â”‚  Assessment, Plan) â†’ sends sections to AI for coding         â”‚
  â”‚  Spot Instances for cost savings (SQS retries on interrupt)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  EC2: Coder Review Dashboard                                 â”‚
  â”‚  Instance: t3.large (2 vCPU, 8GB) in ASG behind ALB         â”‚
  â”‚  Coders review AI-suggested ICD-10, CPT, HCPCS codes        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  EC2: Bastion Host (t3.micro) for admin access               â”‚
  â”‚  SSH tunnel to Aurora/ElastiCache in private subnet           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</pre>
                <ul>
                    <li><strong>Java sectionizer (250 units):</strong> c5.xlarge Spot Instances in ASG. Each unit polls SQS-2, receives Textract output, sectionizes into medical sections (HPI, ROS, Assessment, Plan), sends to AI service for ICD-10/CPT coding</li>
                    <li><strong>Spot Instances for Java app:</strong> 70% cost savings. Safe because SQS-2 retries messages if a Spot instance is interrupted &mdash; visibility timeout returns message to queue</li>
                    <li><strong>Dashboard servers:</strong> t3.large On-Demand in ASG (min 2). Coders review AI-generated codes, approve/reject, finalize encounters</li>
                    <li><strong>User data script:</strong> Bootstrap installs CloudWatch agent, pulls Java app JAR from S3, configures SQS consumer threads, connects to AI endpoint</li>
                    <li><strong>Bastion host:</strong> t3.micro in public subnet for SSH tunneling to Aurora in private subnet &mdash; HIPAA audit requires all DB access logged</li>
                </ul>
            </div>

            <!-- Interview Q&A -->
            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q1: How do you choose the right EC2 instance type?</div>
                    <div class="answer"><strong>Step 1:</strong> Profile your workload &mdash; CPU-bound? Memory-bound? I/O-bound? <strong>Step 2:</strong> Start with general purpose (T3/M5). <strong>Step 3:</strong> Monitor CloudWatch (CPU utilization, memory, network). <strong>Step 4:</strong> Right-size based on data. For our healthcare NLP processing: started with M5.xlarge, found it was CPU-bound, switched to C5.2xlarge &mdash; 40% faster, same cost.</div>
                </div>

                <div class="q">
                    <div class="question">Q2: Spot vs Reserved vs Savings Plans &mdash; when to use each?</div>
                    <div class="answer"><strong>Reserved:</strong> Steady-state workloads (production DB, always-on API servers) &mdash; know exact instance type. <strong>Savings Plans:</strong> Steady spend but may change instance types/regions &mdash; more flexibility. <strong>Spot:</strong> Fault-tolerant, stateless workloads (batch processing, CI/CD, data pipelines). We used Spot for NLP batch processing (70% savings) + Reserved for production API servers.</div>
                </div>

                <div class="q">
                    <div class="question">Q3: What happens when a Spot instance is interrupted?</div>
                    <div class="answer">AWS sends a <strong>2-minute warning</strong> via instance metadata and CloudWatch Events. Your app should: <strong>1)</strong> Check <code>http://169.254.169.254/latest/meta-data/spot/termination-time</code> periodically. <strong>2)</strong> Save state / checkpoint work. <strong>3)</strong> Deregister from load balancer. Best practice: use <strong>Spot Fleet</strong> with multiple instance types and AZs to reduce interruption risk. Combine with On-Demand for baseline capacity.</div>
                </div>

                <div class="q">
                    <div class="question">Q4: EBS gp3 vs gp2 &mdash; why gp3 is better?</div>
                    <div class="answer"><strong>gp2:</strong> IOPS scales with volume size (3 IOPS/GB, max 16K). Need 5.3TB volume to get max IOPS. <strong>gp3:</strong> 3000 IOPS + 125 MB/s baseline <strong>regardless of size</strong>. Scale IOPS (up to 16K) and throughput (up to 1000 MB/s) independently. 20% cheaper than gp2 at baseline. Always use gp3 for new volumes.</div>
                </div>

                <div class="q">
                    <div class="question">Q5: How to make EC2 highly available?</div>
                    <div class="answer"><strong>1)</strong> Launch instances across <strong>multiple AZs</strong>. <strong>2)</strong> Use <strong>Auto Scaling Group</strong> with min/max/desired capacity. <strong>3)</strong> Put behind <strong>ALB</strong> for load distribution + health checks. <strong>4)</strong> Use <strong>Launch Template</strong> (not Launch Config) for consistent deployments. <strong>5)</strong> Store state externally (RDS, ElastiCache, S3) &mdash; instances should be stateless. <strong>6)</strong> Custom AMI for fast boot (pre-baked vs user-data bootstrapping).</div>
                </div>

                <div class="q">
                    <div class="question">Q6: IMDSv1 vs IMDSv2 &mdash; why enforce v2?</div>
                    <div class="answer">IMDSv1: simple GET request to <code>169.254.169.254</code>. Vulnerable to <strong>SSRF attacks</strong> &mdash; if attacker tricks your app into making a request to metadata endpoint, they can steal IAM role credentials. <strong>IMDSv2:</strong> Requires PUT request first to get a token, then use token in subsequent requests. Blocks SSRF because attacker can't get the token. Always enforce IMDSv2 in production: <code>--metadata-options HttpTokens=required</code>.</div>
                </div>

                <div class="q">
                    <div class="question">Q7: When to use Cluster vs Spread placement group?</div>
                    <div class="answer"><strong>Cluster:</strong> All instances same rack, same AZ. <strong>10 Gbps network</strong> between instances. Use for HPC, tightly-coupled apps where inter-node latency matters (MPI, distributed ML training). Risk: rack failure takes all instances. <strong>Spread:</strong> Each instance on different hardware, across AZs. Max 7 per AZ. Use for critical apps (primary DB replicas, Kafka brokers) where you need <strong>hardware isolation</strong>.</div>
                </div>

                <div class="q">
                    <div class="question">Q8: How do you handle EC2 cost optimization?</div>
                    <div class="answer"><strong>1)</strong> Right-size: use AWS Compute Optimizer to find underutilized instances. <strong>2)</strong> Reserved/Savings Plans for baseline (60-72% savings). <strong>3)</strong> Spot for batch/dev (up to 90% savings). <strong>4)</strong> Schedule: stop dev/staging instances nights/weekends (Instance Scheduler). <strong>5)</strong> Use ARM-based Graviton instances (M6g, C6g) &mdash; 20% cheaper, 40% better price-performance. <strong>6)</strong> Delete unused EBS volumes and unattached Elastic IPs.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 2 -->
        <div class="concept" id="concept-2">
            <h2>2. VPC & Networking</h2>
            <span class="tag">Networking Foundation</span>

            <div class="point">
                <strong>What is VPC?</strong>
                <p>Virtual Private Cloud &mdash; your own isolated network in AWS. You control IP ranges (CIDR), subnets, route tables, gateways. Every resource (EC2, RDS, Lambda) lives inside a VPC. Think of it as your private data center in the cloud.</p>
            </div>

            <div class="point">
                <strong>CIDR & Subnets</strong>
                <ul>
                    <li><strong>CIDR Block:</strong> IP range for VPC. Example: <code>10.0.0.0/16</code> = 65,536 IPs. Choose wisely &mdash; can't change later.</li>
                    <li><strong>Public Subnet:</strong> Has route to Internet Gateway. Resources get public IP. For: ALBs, NAT Gateways, bastion hosts.</li>
                    <li><strong>Private Subnet:</strong> No direct internet access. For: App servers, databases, internal services.</li>
                    <li><strong>Best Practice:</strong> At least 2 AZs, each with public + private subnet. /24 per subnet (256 IPs, 251 usable &mdash; AWS reserves 5).</li>
                </ul>
<pre><code># Typical 3-tier VPC layout
VPC: 10.0.0.0/16

Public Subnets (ALB, NAT):
  10.0.1.0/24  (AZ-a)    10.0.2.0/24  (AZ-b)

Private Subnets (App servers):
  10.0.10.0/24 (AZ-a)    10.0.20.0/24 (AZ-b)

Data Subnets (RDS, ElastiCache):
  10.0.100.0/24 (AZ-a)   10.0.200.0/24 (AZ-b)</code></pre>
            </div>

            <div class="point">
                <strong>Internet Gateway (IGW) & NAT Gateway</strong>
                <ul>
                    <li><strong>IGW:</strong> Allows public subnet resources to reach the internet (and internet to reach them). One per VPC. Free.</li>
                    <li><strong>NAT Gateway:</strong> Allows private subnet resources to reach the internet (outbound only &mdash; for updates, API calls). Internet can't initiate connection in. Managed, scales automatically. ~$0.045/hr + data charges. Place in public subnet.</li>
                    <li><strong>NAT Instance:</strong> EC2 instance doing NAT. Cheaper but you manage it. Use for dev/cost-sensitive environments.</li>
                </ul>
            </div>

            <div class="point">
                <strong>Route Tables</strong>
<pre><code># Public Subnet Route Table
Destination      Target
10.0.0.0/16      local          # VPC internal traffic
0.0.0.0/0        igw-xxxxx      # All other traffic â†’ Internet Gateway

# Private Subnet Route Table
Destination      Target
10.0.0.0/16      local          # VPC internal traffic
0.0.0.0/0        nat-xxxxx      # All other traffic â†’ NAT Gateway</code></pre>
            </div>

            <div class="point">
                <strong>Security Groups vs NACLs</strong>
<pre><code>Feature          | Security Group         | NACL
-----------------+------------------------+-------------------------
Level            | Instance (ENI)         | Subnet
State            | Stateful               | Stateless
Rules            | Allow only             | Allow + Deny
Evaluation       | All rules evaluated    | Rules by number (lowâ†’high)
Default          | Deny all in, Allow out | Allow all in + out
Return traffic   | Auto-allowed           | Must explicitly allow</code></pre>
                <ul>
                    <li><strong>SG tip:</strong> Reference other SGs instead of IPs. Example: ALB SG allows 443 from 0.0.0.0/0. App SG allows 8080 from ALB-SG only.</li>
                    <li><strong>NACL tip:</strong> Use as extra defense layer. Block known bad IPs at subnet level. Remember: must allow ephemeral ports (1024-65535) for return traffic.</li>
                </ul>
            </div>

            <div class="point">
                <strong>VPC Peering</strong>
                <p>Private connection between two VPCs (same or different accounts/regions). Traffic stays on AWS backbone. <strong>Not transitive</strong> &mdash; if A peers with B and B peers with C, A can't talk to C. CIDR blocks must not overlap.</p>
            </div>

            <div class="point">
                <strong>Transit Gateway</strong>
                <p>Hub-and-spoke model. Connect multiple VPCs, on-prem networks, VPNs through a single gateway. Scales to thousands of VPCs. Supports transitive routing. Replaces complex peering mesh.</p>
<pre><code># Without Transit Gateway: N VPCs need N*(N-1)/2 peering connections
# 10 VPCs = 45 peering connections ğŸ˜©

# With Transit Gateway: N VPCs need N connections
# 10 VPCs = 10 connections to TGW âœ…
# Plus: centralized routing, monitoring, security</code></pre>
            </div>

            <div class="point">
                <strong>VPN & Direct Connect</strong>
                <ul>
                    <li><strong>Site-to-Site VPN:</strong> Encrypted tunnel over public internet. Quick setup (~minutes). Bandwidth limited by internet connection. For: dev, backup, low-traffic hybrid.</li>
                    <li><strong>Direct Connect:</strong> Dedicated physical fiber from your data center to AWS. 1 Gbps or 10 Gbps. Consistent latency, no internet. For: high-throughput production workloads, compliance. Takes weeks to provision.</li>
                    <li><strong>Direct Connect Gateway:</strong> Access VPCs in multiple regions via single Direct Connect.</li>
                </ul>
            </div>

            <div class="point">
                <strong>VPC Endpoints</strong>
                <ul>
                    <li><strong>Gateway Endpoint:</strong> Free. For S3 and DynamoDB. Add entry to route table. Traffic stays in AWS network.</li>
                    <li><strong>Interface Endpoint (PrivateLink):</strong> ENI in your subnet. For 100+ AWS services (SQS, SNS, KMS, CloudWatch). Costs ~$0.01/hr. Private DNS resolves service to private IP.</li>
                    <li><strong>Why?</strong> Keep traffic off the internet. Required for private subnets without NAT. Better security + lower data transfer costs.</li>
                </ul>
            </div>

            <div class="point">
                <strong>VPC Flow Logs</strong>
                <p>Capture IP traffic metadata (source, dest, port, action, bytes). Attach to VPC, subnet, or ENI. Send to CloudWatch Logs or S3. For: troubleshooting connectivity, security analysis, compliance auditing.</p>
<pre><code># Flow Log format example:
# version account-id eni-id srcaddr dstaddr srcport dstport protocol packets bytes start end action log-status
2 123456789012 eni-abc123 10.0.1.5 52.94.133.10 49761 443 6 12 1080 1580000000 1580000060 ACCEPT OK
2 123456789012 eni-abc123 203.0.113.99 10.0.1.5 0 0 1 4 336 1580000000 1580000060 REJECT OK</code></pre>
            </div>

            <div class="project-block">
                <h4>In Your Project: MedCode AI Platform</h4>
                <p class="arch-label">VPC Architecture for 250-Unit Processing Pipeline</p>
                <pre>
  VPC: 10.0.0.0/16 (MedCode Production)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                              â”‚
  â”‚  Public Subnet 10.0.1.0/24          (us-east-1a/b/c)        â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
  â”‚  â”‚   ALB    â”‚  â”‚ Bastion  â”‚  â”‚   NAT    â”‚                   â”‚
  â”‚  â”‚(Dashboardâ”‚  â”‚  Host    â”‚  â”‚ Gateway  â”‚                   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
  â”‚                                                              â”‚
  â”‚  Private Subnet 10.0.10.0/24        (Processing Layer)       â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
  â”‚  â”‚  Java Sectionizer App (250 EC2 units in ASG)     â”‚        â”‚
  â”‚  â”‚  Polls SQS-2 â†’ sectionize â†’ send to AI          â”‚        â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
  â”‚  â”‚Dashboard â”‚  â”‚ Lambda   â”‚  â† VPC Lambda for Textract       â”‚
  â”‚  â”‚ EC2 App  â”‚  â”‚(Textract)â”‚    needs access to S3 + SQS     â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
  â”‚                                                              â”‚
  â”‚  Isolated Subnet 10.0.20.0/24      (Data Layer)              â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
  â”‚  â”‚  Aurora   â”‚  â”‚ElastiCacheâ”‚  â”‚ DynamoDB â”‚ â† via VPC        â”‚
  â”‚  â”‚  MySQL   â”‚  â”‚  Redis   â”‚  â”‚ Endpoint â”‚   Endpoint        â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
  â”‚                                                              â”‚
  â”‚  VPC Endpoints: S3 (Gateway), SQS, DynamoDB, Textract        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</pre>
                <ul>
                    <li><strong>Public subnet:</strong> Only ALB (coder dashboard) + Bastion + NAT Gateway. No processing servers exposed to internet</li>
                    <li><strong>Private subnet:</strong> 250 Java sectionizer EC2 instances in ASG, Lambda (VPC-enabled for Textract + S3 access), Dashboard app servers. Outbound via NAT Gateway</li>
                    <li><strong>Isolated subnet:</strong> Aurora MySQL + ElastiCache Redis + DynamoDB (via VPC endpoint). No internet route &mdash; HIPAA: patient PDF data never touches public internet</li>
                    <li><strong>VPC Endpoints:</strong> S3 Gateway (free, Lambda reads PDFs), SQS Interface (Lambda polls SQS-1, Java polls SQS-2), DynamoDB Gateway (pipeline tracking), Textract Interface (OCR calls stay in AWS network)</li>
                    <li><strong>250 units network:</strong> Each unit consumes ~5 Mbps during Textract output processing. Private subnet sized /20 (4,096 IPs) to handle all units + future growth</li>
                    <li><strong>Flow Logs:</strong> All subnets â†’ CloudWatch Logs. HIPAA audit: track all network access to PHI data. Retained 1 year</li>
                </ul>
            </div>

            <!-- Interview Q&A -->
            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q1: Design a VPC for a 3-tier web application.</div>
                    <div class="answer"><strong>VPC:</strong> 10.0.0.0/16 across 2 AZs. <strong>Tier 1 (Public):</strong> 2 public subnets for ALB + NAT Gateway. <strong>Tier 2 (Private):</strong> 2 private subnets for app servers (EC2/ECS). <strong>Tier 3 (Data):</strong> 2 isolated subnets for RDS Multi-AZ + ElastiCache. SGs: ALB accepts 443 from internet â†’ App accepts 8080 from ALB-SG only â†’ DB accepts 3306 from App-SG only. NAT Gateway for outbound (updates, external APIs). VPC endpoints for S3 and DynamoDB.</div>
                </div>

                <div class="q">
                    <div class="question">Q2: How does a private subnet access the internet?</div>
                    <div class="answer">Via <strong>NAT Gateway</strong> placed in a public subnet. Private subnet route table has <code>0.0.0.0/0 â†’ nat-gw</code>. NAT Gateway has a route to IGW via public subnet route table. Traffic flow: Private instance â†’ NAT GW (translates private IP to NAT's Elastic IP) â†’ IGW â†’ internet. Response comes back same path. Internet <strong>cannot</strong> initiate connections to private instances. For AWS service access without internet: use <strong>VPC Endpoints</strong>.</div>
                </div>

                <div class="q">
                    <div class="question">Q3: VPC Peering vs Transit Gateway &mdash; when to use which?</div>
                    <div class="answer"><strong>VPC Peering:</strong> Simple, free data transfer (same region), low latency. Use for 2-3 VPCs that need direct communication. Limitation: not transitive, full mesh gets complex. <strong>Transit Gateway:</strong> Hub-and-spoke, supports transitive routing, connects 1000s of VPCs + on-prem. Use when: >3 VPCs, need centralized routing/monitoring, hybrid connectivity. We used TGW at CorroHealth to connect prod VPC + staging VPC + on-prem via VPN &mdash; single point of management.</div>
                </div>

                <div class="q">
                    <div class="question">Q4: What are VPC Endpoints and why use them?</div>
                    <div class="answer">VPC Endpoints let you access AWS services <strong>without going through the internet</strong>. Two types: <strong>Gateway</strong> (free, for S3/DynamoDB, route table entry) and <strong>Interface/PrivateLink</strong> (ENI in your subnet, for 100+ services). Benefits: <strong>1)</strong> Better security &mdash; traffic never leaves AWS network. <strong>2)</strong> No need for NAT Gateway (saves cost). <strong>3)</strong> Required for private subnets accessing S3/DynamoDB. <strong>4)</strong> Lower latency. We used Gateway Endpoint for S3 access from private app servers.</div>
                </div>

                <div class="q">
                    <div class="question">Q5: Direct Connect vs VPN &mdash; when to use each?</div>
                    <div class="answer"><strong>VPN:</strong> Quick setup (minutes), encrypted over internet, up to 1.25 Gbps per tunnel (use multiple for more). Good for: dev, backup, low-traffic hybrid. <strong>Direct Connect:</strong> Dedicated fiber, consistent latency, 1-10 Gbps, takes weeks to provision. For: production workloads needing consistent performance, large data transfers, compliance (data must not traverse public internet). Best practice: use both &mdash; <strong>Direct Connect primary + VPN as failover</strong>.</div>
                </div>

                <div class="q">
                    <div class="question">Q6: How to troubleshoot "instance can't reach the internet"?</div>
                    <div class="answer">Checklist: <strong>1)</strong> Is instance in public subnet? Check route table for IGW route. <strong>2)</strong> Does instance have public/Elastic IP? <strong>3)</strong> Security Group: outbound rule allows traffic? <strong>4)</strong> NACL: allows outbound + ephemeral port inbound? <strong>5)</strong> If private subnet: NAT Gateway exists? Route table has 0.0.0.0/0 â†’ NAT? <strong>6)</strong> NAT Gateway in public subnet with IGW route? <strong>7)</strong> Check VPC Flow Logs for REJECT entries. Usually it's a missing route or SG rule.</div>
                </div>

                <div class="q">
                    <div class="question">Q7: Stateful vs Stateless firewalls &mdash; explain with SG and NACL.</div>
                    <div class="answer"><strong>Stateful (SG):</strong> If you allow inbound on port 443, the response is <strong>automatically allowed</strong> regardless of outbound rules. It tracks connection state. <strong>Stateless (NACL):</strong> Each direction evaluated independently. If you allow inbound 443, you MUST also allow outbound on <strong>ephemeral ports (1024-65535)</strong> for the response. Forgetting this is the #1 NACL mistake. NACLs process rules by number &mdash; first match wins (unlike SGs which evaluate all rules).</div>
                </div>

                <div class="q">
                    <div class="question">Q8: How to design VPC for HIPAA compliance?</div>
                    <div class="answer"><strong>1)</strong> All resources in <strong>private subnets</strong> &mdash; no public IPs on app/data tier. <strong>2)</strong> VPC Endpoints for AWS services (no internet traversal). <strong>3)</strong> <strong>NACLs + SGs</strong> with least-privilege rules. <strong>4)</strong> <strong>VPC Flow Logs</strong> enabled and shipped to S3/CloudWatch for audit. <strong>5)</strong> <strong>Direct Connect or VPN</strong> for on-prem connectivity. <strong>6)</strong> <strong>PrivateLink</strong> for third-party service access. <strong>7)</strong> Encrypt all traffic with TLS. We implemented all of these at CorroHealth for PHI data handling.</div>
                </div>
            </div>
        </div>

        <!-- CONCEPT 3 -->
        <div class="concept" id="concept-3">
            <h2>3. S3 & Storage</h2>
            <span class="tag">Object Storage</span>

            <div class="point">
                <strong>What is S3?</strong>
                <p>Simple Storage Service &mdash; unlimited object storage. Store any file (up to 5TB per object). Objects stored in <strong>buckets</strong> (globally unique name). Flat structure (no real folders &mdash; prefixes simulate folders). 11 9's durability (99.999999999%). Used for: static assets, backups, data lakes, logs, ML datasets.</p>
            </div>

            <div class="point">
                <strong>Storage Classes</strong>
                <ul>
                    <li><strong>S3 Standard:</strong> Default. Frequently accessed. 3 AZ replication. Lowest latency. Most expensive storage.</li>
                    <li><strong>S3 Intelligent-Tiering:</strong> Auto-moves objects between tiers based on access patterns. No retrieval fees. Best when access patterns are unknown.</li>
                    <li><strong>S3 Standard-IA (Infrequent Access):</strong> Lower storage cost, retrieval fee. Min 30 days. For: backups accessed occasionally.</li>
                    <li><strong>S3 One Zone-IA:</strong> Single AZ. 20% cheaper than Standard-IA. For: reproducible data, secondary backups.</li>
                    <li><strong>S3 Glacier Instant Retrieval:</strong> Millisecond retrieval. Cheapest for data accessed once per quarter.</li>
                    <li><strong>S3 Glacier Flexible:</strong> Minutes to hours retrieval (Expedited: 1-5 min, Standard: 3-5 hr, Bulk: 5-12 hr). For: archives.</li>
                    <li><strong>S3 Glacier Deep Archive:</strong> Cheapest. 12-48 hour retrieval. For: compliance archives, 7+ year retention.</li>
                </ul>
            </div>

            <div class="point">
                <strong>Lifecycle Policies</strong>
                <p>Automatically transition objects between storage classes or delete after expiry.</p>
<pre><code>{
  "Rules": [{
    "ID": "ArchiveOldLogs",
    "Status": "Enabled",
    "Filter": { "Prefix": "logs/" },
    "Transitions": [
      { "Days": 30, "StorageClass": "STANDARD_IA" },
      { "Days": 90, "StorageClass": "GLACIER" },
      { "Days": 365, "StorageClass": "DEEP_ARCHIVE" }
    ],
    "Expiration": { "Days": 2555 }  // Delete after 7 years
  }]
}</code></pre>
            </div>

            <div class="point">
                <strong>Versioning</strong>
                <ul>
                    <li>Keep multiple versions of an object. Protects against accidental deletes/overwrites.</li>
                    <li>Delete adds a <strong>delete marker</strong> (soft delete). Previous versions remain accessible.</li>
                    <li>Combine with lifecycle to expire old versions: <code>"NoncurrentVersionExpiration": { "NoncurrentDays": 30 }</code></li>
                    <li><strong>MFA Delete:</strong> Require MFA to permanently delete versions. Extra protection for critical data.</li>
                </ul>
            </div>

            <div class="point">
                <strong>Encryption</strong>
                <ul>
                    <li><strong>SSE-S3:</strong> AWS manages keys. Default encryption. AES-256. Zero effort.</li>
                    <li><strong>SSE-KMS:</strong> AWS KMS manages keys. Audit trail via CloudTrail. Key rotation. Use for: compliance (HIPAA, PCI).</li>
                    <li><strong>SSE-C:</strong> You provide encryption key with each request. AWS encrypts/decrypts but doesn't store key.</li>
                    <li><strong>Client-Side:</strong> Encrypt before upload. AWS never sees plaintext. Maximum control.</li>
                    <li><strong>In Transit:</strong> Always use HTTPS (TLS). Enforce with bucket policy: <code>"aws:SecureTransport": "false"</code> â†’ Deny.</li>
                </ul>
            </div>

            <div class="point">
                <strong>Replication</strong>
                <ul>
                    <li><strong>CRR (Cross-Region Replication):</strong> Replicate to bucket in different region. For: DR, compliance, latency reduction.</li>
                    <li><strong>SRR (Same-Region Replication):</strong> Replicate within same region. For: log aggregation, live replica between accounts.</li>
                    <li>Requires versioning enabled on both source and destination. Can replicate to different account.</li>
                </ul>
            </div>

            <div class="point">
                <strong>Presigned URLs & S3 Select</strong>
<pre><code># Presigned URL: Temporary access to private objects
aws s3 presign s3://my-bucket/private-report.pdf --expires-in 3600
# Returns: https://my-bucket.s3.amazonaws.com/private-report.pdf?X-Amz-...
# Anyone with URL can download for 1 hour. No AWS credentials needed.

# S3 Select: Query CSV/JSON/Parquet with SQL without downloading entire file
aws s3api select-object-content \
  --bucket my-bucket --key data.csv \
  --expression "SELECT * FROM s3object WHERE age > 30" \
  --expression-type SQL \
  --input-serialization '{"CSV": {"FileHeaderInfo": "USE"}}' \
  --output-serialization '{"CSV": {}}' output.csv
# Saves bandwidth and cost â€” only transfers matching rows</code></pre>
            </div>

            <div class="point">
                <strong>S3 Performance & Transfer</strong>
                <ul>
                    <li><strong>Multipart Upload:</strong> Required for >5GB. Recommended for >100MB. Parallel upload of parts. Resume on failure.</li>
                    <li><strong>Transfer Acceleration:</strong> Uses CloudFront edge locations. Upload to nearest edge â†’ AWS backbone to S3. 50-500% faster for long-distance uploads.</li>
                    <li><strong>S3 Byte-Range Fetches:</strong> Download specific byte ranges in parallel. Speed up large file downloads.</li>
                    <li><strong>Performance:</strong> 3,500 PUT/POST + 5,500 GET per second <strong>per prefix</strong>. Spread across prefixes for more throughput.</li>
                </ul>
            </div>

            <div class="point">
                <strong>Bucket Policies & ACLs</strong>
<pre><code>// Bucket Policy: Allow public read on /public/* prefix only
{
  "Version": "2012-10-17",
  "Statement": [{
    "Sid": "PublicRead",
    "Effect": "Allow",
    "Principal": "*",
    "Action": "s3:GetObject",
    "Resource": "arn:aws:s3:::my-bucket/public/*"
  }]
}

// Block Public Access (account-level recommended)
// S3 Block Public Access â†’ ON (all 4 settings)
// Only open specific buckets via bucket policy when needed</code></pre>
            </div>

            <div class="project-block">
                <h4>In Your Project: MedCode AI Platform</h4>
                <p class="arch-label">S3 â€” The Entry Point of the Entire Pipeline</p>
                <pre>
  PIPELINE STARTS HERE:
  Client uploads PDF â†’ S3 â†’ Event Notification â†’ SQS-1 (triggers everything)

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  S3: medcode-raw-pdfs (ENTRY POINT)                         â”‚
  â”‚  /client-C001/2024/03/encounter-12345.pdf                   â”‚
  â”‚  SSE-KMS encrypted â”‚ Versioning ON â”‚ Block Public Access    â”‚
  â”‚                                                             â”‚
  â”‚  S3 Event: ObjectCreated â†’ SQS-1 (Textract Queue)          â”‚
  â”‚  This single event kicks off the entire 6-step pipeline     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚  Lambda reads PDF from here via VPC Endpoint
        â”‚  Textract processes â†’ extracted text back to:
        v
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  S3: medcode-textract-output                                â”‚
  â”‚  /output/encounter-12345/page-001.json                      â”‚
  â”‚  Raw Textract JSON (per page), used by Java sectionizer     â”‚
  â”‚  TTL Lifecycle: auto-delete after 30 days (data in DB)      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        v
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  S3: medcode-audit-logs                                     â”‚
  â”‚  CloudTrail + S3 access logs + processing audit trail       â”‚
  â”‚  Object Lock (WORM) â”‚ 7-year retention â”‚ Glacier Deep       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  250 units processing = ~500 S3 GET/PUT per second peak
  S3 prefix design: /client-{id}/yyyy/mm/ avoids hot partitions</pre>
                <ul>
                    <li><strong>S3 Event â†’ SQS-1:</strong> Every new PDF upload triggers S3 Event Notification â†’ sends message to SQS-1. Lambda polls SQS-1, reads PDF from S3, calls Textract for OCR</li>
                    <li><strong>Presigned URLs:</strong> Client portal generates 15-min presigned upload URLs. No S3 credentials exposed to client browser. Scoped to specific prefix per client</li>
                    <li><strong>Textract output bucket:</strong> Lambda stores raw Textract JSON here. Java sectionizer reads from this bucket. Auto-deleted after 30 days (final data lives in DB)</li>
                    <li><strong>Lifecycle policy:</strong> Raw PDFs â†’ S3-IA after 90 days, Glacier after 1 year, Deep Archive after 3 years. HIPAA requires 7-year retention</li>
                    <li><strong>Performance:</strong> 250 units = ~500 GET/PUT per second. S3 prefix <code>/client-{id}/yyyy/mm/</code> distributes across partitions, avoids throttling</li>
                    <li><strong>Audit bucket:</strong> S3 Object Lock in Compliance mode. No one (not even root) can delete for 7 years. Every PDF access logged</li>
                </ul>
            </div>

            <!-- Interview Q&A -->
            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q9: How to design cost-effective S3 storage for healthcare data?</div>
                    <div class="answer"><strong>Active patient docs (0-30 days):</strong> S3 Standard. <strong>Recent archives (30-90 days):</strong> S3 Standard-IA via lifecycle rule. <strong>Compliance archives (90+ days):</strong> Glacier Flexible. <strong>7-year retention (legal):</strong> Glacier Deep Archive. Enable versioning for accidental delete protection. SSE-KMS encryption for HIPAA. Object Lock (WORM) for compliance. We saved ~60% on storage costs with this tiered approach at CorroHealth.</div>
                </div>

                <div class="q">
                    <div class="question">Q10: S3 vs EBS vs EFS &mdash; when to use which?</div>
                    <div class="answer"><strong>S3:</strong> Object storage. Unlimited. HTTP API access. For: static files, backups, data lakes, media. <strong>EBS:</strong> Block storage. Attached to single EC2. For: OS disk, databases, app data. <strong>EFS:</strong> File system (NFS). Shared across multiple EC2 instances. For: CMS content, shared config, container storage. <strong>Rule:</strong> S3 for objects, EBS for single-instance disk, EFS for shared file system.</div>
                </div>

                <div class="q">
                    <div class="question">Q11: How to secure an S3 bucket properly?</div>
                    <div class="answer"><strong>1)</strong> Enable <strong>S3 Block Public Access</strong> at account level. <strong>2)</strong> Bucket policy with least privilege. <strong>3)</strong> SSE-KMS encryption (audit trail). <strong>4)</strong> Enable versioning + MFA Delete. <strong>5)</strong> Enforce HTTPS via bucket policy (<code>SecureTransport</code>). <strong>6)</strong> VPC Gateway Endpoint for private access. <strong>7)</strong> Access logging to separate bucket. <strong>8)</strong> IAM policies &mdash; no <code>s3:*</code> wildcards. <strong>9)</strong> Object Lock for WORM compliance.</div>
                </div>

                <div class="q">
                    <div class="question">Q12: Explain S3 event notifications &mdash; real use case.</div>
                    <div class="answer">S3 can trigger events on <strong>PUT, POST, DELETE, Restore</strong>. Targets: Lambda, SQS, SNS, EventBridge. <strong>Our use case:</strong> Medical document uploaded to S3 â†’ S3 Event triggers Lambda â†’ Lambda sends message to SQS â†’ NLP worker picks up, processes document, writes results to MongoDB. Fully serverless ingestion pipeline. Also used for: image thumbnail generation, virus scanning on upload, ETL triggers.</div>
                </div>

                <div class="q">
                    <div class="question">Q13: What is S3 Object Lock and when to use it?</div>
                    <div class="answer">Object Lock enforces <strong>WORM (Write Once Read Many)</strong>. Two modes: <strong>Governance</strong> (users with special permissions can override) and <strong>Compliance</strong> (nobody can delete, not even root, until retention expires). Use for: <strong>regulatory compliance</strong> (HIPAA, SEC, FINRA), audit logs, legal hold documents. We used Compliance mode for medical records requiring 7-year retention at CorroHealth.</div>
                </div>

                <div class="q">
                    <div class="question">Q14: How to optimize S3 costs?</div>
                    <div class="answer"><strong>1)</strong> Lifecycle policies to auto-transition to cheaper tiers. <strong>2)</strong> S3 Intelligent-Tiering for unknown access patterns. <strong>3)</strong> Delete incomplete multipart uploads (<code>AbortIncompleteMultipartUpload: 7 days</code>). <strong>4)</strong> Expire old object versions. <strong>5)</strong> S3 Analytics to identify transition candidates. <strong>6)</strong> Compress before upload (gzip). <strong>7)</strong> Use VPC Gateway Endpoint (no data transfer charges). <strong>8)</strong> S3 Select instead of downloading full objects.</div>
                </div>

                <div class="q">
                    <div class="question">Q15: CRR vs SRR &mdash; when to use each?</div>
                    <div class="answer"><strong>CRR (Cross-Region):</strong> Disaster recovery (data survives entire region failure), compliance (data must exist in specific regions), latency reduction (users read from nearby region). <strong>SRR (Same-Region):</strong> Log aggregation from multiple accounts into one bucket, live replica between prod/analytics accounts, data sovereignty (keep in same region but different account). Both need versioning enabled. Replication is <strong>asynchronous</strong> &mdash; most objects replicate within 15 minutes.</div>
                </div>

                <div class="q">
                    <div class="question">Q16: How does S3 Transfer Acceleration work?</div>
                    <div class="answer">Client uploads to the <strong>nearest CloudFront edge location</strong> (150+ globally). From edge, data travels over <strong>AWS's optimized backbone</strong> network to the S3 bucket region. 50-500% faster than regular internet upload for long distances. Use when: uploading from globally distributed clients, large file uploads from far-away regions. Enable per bucket. Uses endpoint: <code>mybucket.s3-accelerate.amazonaws.com</code>. Small extra cost but significant speed gain.</div>
                </div>
            </div>
        </div>

        <!-- Concept 4: IAM & Security -->
        <div class="concept" id="concept-4">
            <h2>4. IAM & Security</h2>
            <span class="tag">Security</span>

            <div class="point">
                <h3>IAM Core Building Blocks</h3>
                <p><strong>Users</strong> &mdash; individual identities (person or application). <strong>Groups</strong> &mdash; collection of users (e.g., Developers, Admins); cannot nest groups inside groups. <strong>Roles</strong> &mdash; temporary credentials assumed by users, services, or accounts; no permanent password/keys. <strong>Policies</strong> &mdash; JSON documents defining permissions (Allow/Deny on Actions for Resources with optional Conditions).</p>
                <pre>// Policy structure
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Action": "s3:GetObject",
    "Resource": "arn:aws:s3:::my-bucket/*",
    "Condition": {
      "IpAddress": { "aws:SourceIp": "10.0.0.0/8" }
    }
  }]
}</pre>
                <p>Best practice: Attach policies to <strong>groups</strong>, not individual users. Users inherit permissions from all groups they belong to.</p>
            </div>

            <div class="point">
                <h3>Policy Types & Evaluation Logic</h3>
                <p><strong>Identity-based policies:</strong> Attached to users/groups/roles (managed or inline). <strong>Resource-based policies:</strong> Attached to resources (S3 bucket policy, SQS queue policy) &mdash; specify a <code>Principal</code>. <strong>Service Control Policies (SCPs):</strong> Organization-level guardrails &mdash; restrict what accounts <em>can</em> do (don't grant permissions, only limit). <strong>Permission boundaries:</strong> Maximum permissions an IAM entity can have &mdash; intersection of boundary + identity policy = effective permissions.</p>
                <p><strong>Evaluation order:</strong> Explicit Deny &rarr; SCP &rarr; Resource-based policy &rarr; Permission boundary &rarr; Session policy &rarr; Identity-based policy. <strong>Explicit Deny always wins.</strong></p>
            </div>

            <div class="point">
                <h3>Least Privilege & Access Advisor</h3>
                <p>Start with <strong>zero permissions</strong> and add only what's needed. Use <strong>IAM Access Analyzer</strong> to generate policies based on actual CloudTrail activity. <strong>Access Advisor</strong> shows last-accessed timestamps per service &mdash; remove unused permissions. Use <strong>aws:RequestedRegion</strong> condition to restrict actions to specific regions. Regularly audit with <strong>IAM Credential Report</strong> (account-level CSV of all users and credential status).</p>
            </div>

            <div class="point">
                <h3>IAM Roles & Cross-Account Access</h3>
                <p>Roles have a <strong>trust policy</strong> (who can assume) + <strong>permissions policy</strong> (what they can do). For cross-account: Account B creates role with trust policy allowing Account A, Account A users call <code>sts:AssumeRole</code> to get temporary credentials.</p>
                <pre>// Trust policy (on role in Account B)
{
  "Effect": "Allow",
  "Principal": { "AWS": "arn:aws:iam::111111111111:root" },
  "Action": "sts:AssumeRole",
  "Condition": { "StringEquals": { "sts:ExternalId": "unique-id-123" } }
}

// Account A assumes the role
aws sts assume-role \
  --role-arn arn:aws:iam::222222222222:role/CrossAccountRole \
  --role-session-name my-session \
  --external-id unique-id-123</pre>
                <p><strong>ExternalId</strong> prevents confused deputy attacks when third parties assume roles in your account.</p>
            </div>

            <div class="point">
                <h3>STS (Security Token Service)</h3>
                <p>Generates <strong>temporary credentials</strong> (access key + secret key + session token) with configurable expiry (15 min to 12 hours). Key API calls: <code>AssumeRole</code> (cross-account, same-account role switch), <code>AssumeRoleWithSAML</code> (enterprise SSO), <code>AssumeRoleWithWebIdentity</code> (federated users from Google/Facebook &mdash; prefer Cognito instead), <code>GetSessionToken</code> (MFA-protected API access), <code>GetCallerIdentity</code> (who am I?).</p>
            </div>

            <div class="point">
                <h3>MFA & Security Best Practices</h3>
                <p>Enable MFA on <strong>root account</strong> (mandatory) and all IAM users. MFA device options: Virtual (Authy, Google Authenticator), Hardware (YubiKey), U2F Security Key. Use <strong>MFA condition</strong> in policies to require MFA for sensitive operations:</p>
                <pre>// Deny all actions unless MFA is present
{
  "Effect": "Deny",
  "Action": "*",
  "Resource": "*",
  "Condition": {
    "BoolIfExists": { "aws:MultiFactorAuthPresent": "false" }
  }
}</pre>
                <p>Root account: enable MFA, delete access keys, use only for billing and account-level tasks. Create individual IAM users &mdash; never share root credentials.</p>
            </div>

            <div class="point">
                <h3>IAM Conditions & Variables</h3>
                <p>Conditions add fine-grained control: <code>aws:SourceIp</code> (restrict by IP range), <code>aws:RequestedRegion</code> (restrict to regions), <code>aws:PrincipalTag</code> (ABAC &mdash; attribute-based access), <code>s3:prefix</code> (restrict S3 access to specific folders), <code>aws:CurrentTime</code> (time-based access).</p>
                <pre>// Allow user to access only their own S3 folder
{
  "Effect": "Allow",
  "Action": "s3:*",
  "Resource": "arn:aws:s3:::company-bucket/home/${aws:username}/*"
}

// ABAC: Allow access based on tags
"Condition": {
  "StringEquals": {
    "aws:PrincipalTag/Department": "Engineering",
    "ec2:ResourceTag/Environment": "dev"
  }
}</pre>
                <p><strong>ABAC</strong> (Attribute-Based Access Control) scales better than traditional RBAC &mdash; no need to update policies when new resources are added.</p>
            </div>

            <div class="point">
                <h3>Permission Boundaries</h3>
                <p>Set <strong>maximum permissions</strong> an IAM entity can have, regardless of identity policies. Use case: Allow team leads to create IAM users but limit what permissions they can grant (prevents privilege escalation). Effective permissions = <strong>intersection</strong> of identity policy AND permission boundary.</p>
                <pre>// Permission boundary: max S3 + CloudWatch only
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Action": ["s3:*", "cloudwatch:*"],
    "Resource": "*"
  }]
}
// Even if identity policy grants "ec2:*",
// effective permissions will NOT include EC2</pre>
                <p>Also useful for: delegating IAM administration safely, limiting Lambda execution roles, enforcing organizational guardrails below SCP level.</p>
            </div>

            <div class="project-block">
                <h4>In Your Project: MedCode AI Platform</h4>
                <p class="arch-label">IAM Roles for Each Pipeline Stage (Least Privilege)</p>
                <pre>
  Pipeline:  Client â†’ S3 â†’ SQS-1 â†’ Lambda â†’ SQS-2 â†’ Java App â†’ AI â†’ DB

  Each stage has its OWN IAM Role â€” minimum permissions only:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                              â”‚
  â”‚  Role: ClientUploadRole (assumed by client portal)           â”‚
  â”‚    â†’ s3:PutObject ONLY on medcode-raw-pdfs/client-{id}/*    â”‚
  â”‚    â†’ No read, no delete, scoped to their prefix only        â”‚
  â”‚                                                              â”‚
  â”‚  Role: LambdaTextractRole (Lambda reads SQS-1, calls OCR)   â”‚
  â”‚    â†’ sqs:ReceiveMessage (SQS-1)                             â”‚
  â”‚    â†’ s3:GetObject (raw-pdfs)                                â”‚
  â”‚    â†’ textract:AnalyzeDocument                               â”‚
  â”‚    â†’ sqs:SendMessage (SQS-2)                                â”‚
  â”‚    â†’ dynamodb:UpdateItem (pipeline-tracker status)           â”‚
  â”‚                                                              â”‚
  â”‚  Role: JavaSectionizerRole (250 EC2 units)                   â”‚
  â”‚    â†’ sqs:ReceiveMessage (SQS-2)                             â”‚
  â”‚    â†’ s3:GetObject (textract-output)                         â”‚
  â”‚    â†’ AI service endpoint (invoke model)                      â”‚
  â”‚    â†’ dynamodb:PutItem (sections + codes)                    â”‚
  â”‚    â†’ rds:Connect (via RDS Proxy, IAM auth)                  â”‚
  â”‚                                                              â”‚
  â”‚  Role: DashboardAppRole (coder review UI)                    â”‚
  â”‚    â†’ rds:Read (via RDS Proxy)                               â”‚
  â”‚    â†’ dynamodb:GetItem, Query (read pipeline status)         â”‚
  â”‚    â†’ s3:GetObject (original PDFs for coder reference)       â”‚
  â”‚                                                              â”‚
  â”‚  Group: MedicalCoders â†’ MFA required, read-only dashboard   â”‚
  â”‚  SCP: Deny outside us-east-1/us-west-2, deny unencrypted    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</pre>
                <ul>
                    <li><strong>Per-stage roles:</strong> Lambda has different role than Java app than Dashboard. Each can only access the services it needs for its specific pipeline step</li>
                    <li><strong>Client upload scoping:</strong> Each client can only PutObject to their own prefix <code>/client-{id}/*</code>. IAM policy variable <code>${aws:PrincipalTag/clientId}</code> enforces this</li>
                    <li><strong>Java app role (250 units):</strong> Can read SQS-2, read Textract output from S3, invoke AI, write results to DynamoDB and Aurora via RDS Proxy with IAM auth</li>
                    <li><strong>Permission boundary:</strong> All pipeline roles have MedCodeBoundary &mdash; limits to S3, SQS, DynamoDB, Textract, RDS only. Even if developer misconfigures, no EC2/IAM/billing access</li>
                    <li><strong>Cross-account:</strong> Hospital partner accounts assume <code>ClientUploadRole</code> with ExternalId to upload PDFs. Prevents confused deputy attack</li>
                    <li><strong>MFA enforcement:</strong> All human users (coders, admins) require MFA. Service roles use instance profiles (no MFA needed)</li>
                </ul>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q17: What is the difference between IAM Users, Groups, Roles, and Policies?</div>
                    <div class="answer"><strong>Users:</strong> Individual identities with long-term credentials (password + access keys). <strong>Groups:</strong> Containers for users to apply permissions in bulk (e.g., all developers get S3 read); cannot be nested. <strong>Roles:</strong> Temporary credential providers &mdash; assumed by users, AWS services (EC2, Lambda), or external accounts; no permanent password. <strong>Policies:</strong> JSON permission documents attached to any of the above; define Effect (Allow/Deny), Actions, Resources, and optional Conditions. Best practice: use groups for users, roles for services, and managed policies for reusability.</div>
                </div>

                <div class="q">
                    <div class="question">Q18: Explain the IAM policy evaluation logic. What happens when there's a conflict?</div>
                    <div class="answer">AWS evaluates in this order: (1) <strong>Explicit Deny</strong> &mdash; if any policy says Deny, it's denied immediately (highest priority). (2) <strong>SCP check</strong> &mdash; must be allowed by organization SCP. (3) <strong>Resource-based policy</strong> &mdash; can grant access even cross-account. (4) <strong>Permission boundary</strong> &mdash; must be within boundary. (5) <strong>Session policy</strong> &mdash; for temporary sessions. (6) <strong>Identity-based policy</strong> &mdash; must explicitly Allow. If no explicit Allow is found, the default is <strong>implicit Deny</strong>. Key rule: Explicit Deny > Allow > Implicit Deny. For cross-account access, both the resource policy (target account) AND the identity policy (source account) must allow.</div>
                </div>

                <div class="q">
                    <div class="question">Q19: How does cross-account access work in AWS?</div>
                    <div class="answer"><strong>Method 1 &mdash; IAM Roles (recommended):</strong> Account B creates a role with a trust policy allowing Account A's principal. Account A users call <code>sts:AssumeRole</code> to get temporary credentials for Account B. Use <code>ExternalId</code> to prevent confused deputy attacks. <strong>Method 2 &mdash; Resource-based policies:</strong> The resource (S3 bucket, SQS queue) in Account B adds a policy with Account A as Principal &mdash; no role assumption needed, the caller keeps their original identity. Roles are preferred because they provide temporary credentials and complete audit trail via CloudTrail.</div>
                </div>

                <div class="q">
                    <div class="question">Q20: What is the confused deputy problem and how does ExternalId prevent it?</div>
                    <div class="answer">A <strong>confused deputy</strong> attack occurs when a third-party service (e.g., auditing tool) that you've granted cross-account access is tricked into acting on behalf of another malicious customer using your role ARN. Without ExternalId, the third party can't distinguish requests. <strong>Solution:</strong> Add a unique <code>ExternalId</code> condition in the trust policy. The third party must provide this secret value when calling AssumeRole. Each customer gets a unique ExternalId, so a malicious customer can't use another customer's ExternalId to access their resources.</div>
                </div>

                <div class="q">
                    <div class="question">Q21: What is the difference between SCP, Permission Boundary, and Identity Policy?</div>
                    <div class="answer"><strong>SCP (Service Control Policy):</strong> Organization-level guardrail applied to accounts/OUs; defines the <em>maximum</em> permissions for the entire account; doesn't grant any permissions. <strong>Permission Boundary:</strong> Set on individual IAM users/roles; defines the <em>maximum</em> permissions for that entity; effective permissions = intersection of boundary AND identity policy. <strong>Identity Policy:</strong> Directly grants Allow/Deny permissions to users/groups/roles. All three are <em>layered</em>: an action must be allowed by SCP AND within the permission boundary AND allowed by the identity policy AND not explicitly denied by any of them.</div>
                </div>

                <div class="q">
                    <div class="question">Q22: How do you implement least privilege effectively in a large AWS organization?</div>
                    <div class="answer"><strong>Step 1:</strong> Start with zero permissions (deny all by default). <strong>Step 2:</strong> Use <strong>IAM Access Analyzer</strong> to analyze CloudTrail logs and auto-generate policies based on actual API calls made over 30-90 days. <strong>Step 3:</strong> Review <strong>Access Advisor</strong> to find and remove unused service permissions. <strong>Step 4:</strong> Use <strong>SCPs</strong> to set organization-wide guardrails (e.g., deny all outside approved regions). <strong>Step 5:</strong> Implement <strong>ABAC</strong> with tags to scale permissions without updating policies. <strong>Step 6:</strong> Regular audit with <strong>IAM Credential Report</strong> (CSV of all users, key ages, MFA status). <strong>Step 7:</strong> Use permission boundaries when delegating IAM admin to teams.</div>
                </div>

                <div class="q">
                    <div class="question">Q23: What is ABAC and how does it differ from traditional RBAC in AWS?</div>
                    <div class="answer"><strong>RBAC (Role-Based):</strong> Create separate policies for each role (dev-s3-policy, prod-s3-policy, staging-s3-policy). When new resources are added, update all policies. Doesn't scale well &mdash; policy explosion. <strong>ABAC (Attribute-Based):</strong> Use tags on both principals and resources. One policy like "allow access if <code>PrincipalTag/Project</code> matches <code>ResourceTag/Project</code>". When new resources are added with proper tags, permissions work automatically. <strong>AWS implementation:</strong> Use <code>aws:PrincipalTag</code>, <code>aws:ResourceTag</code>, and <code>aws:RequestTag</code> conditions. ABAC scales to hundreds of projects without policy changes.</div>
                </div>

                <div class="q">
                    <div class="question">Q24: How would you secure the root account and set up IAM for a new AWS account?</div>
                    <div class="answer"><strong>Root account:</strong> (1) Enable MFA immediately (hardware key preferred). (2) Delete root access keys. (3) Create a strong password and store securely. (4) Use root only for billing/account-level tasks. <strong>IAM setup:</strong> (1) Create an Admin IAM user/role (not root). (2) Enable <strong>IAM password policy</strong> (min length, complexity, rotation). (3) Create groups by function (Admins, Developers, ReadOnly). (4) Enforce MFA for all users via policy condition. (5) Enable <strong>CloudTrail</strong> in all regions for audit. (6) Set up <strong>AWS Organizations</strong> with SCPs. (7) Enable <strong>IAM Access Analyzer</strong> for external access findings. (8) Use SSO/Identity Center for enterprise users instead of individual IAM users.</div>
                </div>
            </div>
        </div>

        <!-- Concept 5: ELB & Auto Scaling -->
        <div class="concept" id="concept-5">
            <h2>5. ELB & Auto Scaling</h2>
            <span class="tag">High Availability</span>

            <div class="point">
                <h3>Elastic Load Balancer Types</h3>
                <p><strong>ALB (Application Load Balancer):</strong> Layer 7 (HTTP/HTTPS). Path-based &amp; host-based routing, WebSocket support, gRPC, native HTTP/2, integrates with WAF. Best for microservices &amp; containers. <strong>NLB (Network Load Balancer):</strong> Layer 4 (TCP/UDP/TLS). Ultra-low latency (~100&mu;s), millions of requests/sec, static IP per AZ, preserves source IP. Best for real-time gaming, IoT, financial apps. <strong>GLB (Gateway Load Balancer):</strong> Layer 3 (IP). Routes traffic through virtual appliances (firewalls, IDS/IPS) using GENEVE protocol. <strong>CLB (Classic):</strong> Legacy &mdash; avoid for new workloads.</p>
            </div>

            <div class="point">
                <h3>Target Groups & Health Checks</h3>
                <p>Targets can be: EC2 instances, IP addresses, Lambda functions (ALB only), or other ALBs. <strong>Health checks:</strong> LB periodically pings targets (HTTP path, TCP port, or gRPC). Configurable: interval (5-300s), threshold (2-10 consecutive checks), timeout. Unhealthy targets stop receiving traffic but remain registered. <strong>Deregistration delay</strong> (default 300s) &mdash; allows in-flight requests to complete before removing target.</p>
                <pre>// ALB health check config
Target Group:
  Health check path: /health
  Healthy threshold: 3
  Unhealthy threshold: 2
  Interval: 30s
  Timeout: 5s
  Success codes: 200-299</pre>
            </div>

            <div class="point">
                <h3>ALB Advanced Features</h3>
                <p><strong>Path-based routing:</strong> <code>/api/*</code> &rarr; API service, <code>/images/*</code> &rarr; image service. <strong>Host-based routing:</strong> <code>api.example.com</code> &rarr; API targets, <code>web.example.com</code> &rarr; web targets. <strong>Sticky sessions:</strong> Cookie-based affinity (application cookie or LB-generated AWSALB cookie), duration 1s-7days. <strong>Cross-zone load balancing:</strong> Distributes evenly across all targets in all AZs (enabled by default on ALB). <strong>SSL/TLS termination:</strong> Offload decryption at ALB using ACM certificates, forward HTTP to backends.</p>
            </div>

            <div class="point">
                <h3>Auto Scaling Groups (ASG)</h3>
                <p>Automatically adjusts EC2 instance count based on demand. Components: <strong>Launch Template</strong> (AMI, instance type, key pair, SGs, user data &mdash; replaces Launch Config), <strong>ASG config</strong> (min/max/desired capacity, VPC/subnets, target group attachment). ASG ensures: minimum instances always running, scales out for demand, scales in to save cost, auto-replaces unhealthy instances.</p>
                <pre>// ASG configuration
Min capacity: 2        # Never below 2
Desired capacity: 4    # Current target
Max capacity: 10       # Never above 10
Default cooldown: 300s # Wait after scaling action
Health check type: ELB  # Use LB health (not just EC2)</pre>
            </div>

            <div class="point">
                <h3>Scaling Policies</h3>
                <p><strong>Target Tracking:</strong> "Keep average CPU at 50%" &mdash; ASG auto-adjusts. Simplest, most common. Predefined metrics: CPU, request count per target, network in/out. <strong>Step Scaling:</strong> Add/remove specific count based on alarm thresholds (e.g., CPU 60-70% add 1, 70-80% add 2, 80%+ add 3). <strong>Scheduled Scaling:</strong> Set capacity for predictable patterns (e.g., min 10 at 9am Mon-Fri, min 2 at 7pm). <strong>Predictive Scaling:</strong> ML-based, analyzes 14-day history to forecast and pre-scale.</p>
            </div>

            <div class="point">
                <h3>Cooldown, Warm-up & Lifecycle Hooks</h3>
                <p><strong>Cooldown period</strong> (default 300s): After a scaling action, ASG ignores new alarms to let metrics stabilize. <strong>Instance warm-up:</strong> New instances excluded from ASG metrics until warm-up completes (prevents premature scale-in). <strong>Lifecycle hooks:</strong> Pause instance at <code>Pending:Wait</code> (scale-out) or <code>Terminating:Wait</code> (scale-in) to run custom actions (install software, drain connections, backup logs). Timeout up to 48h. Integrate with EventBridge/SNS/SQS for automation.</p>
            </div>

            <div class="project-block">
                <h4>In Your Project: MedCode AI Platform</h4>
                <p class="arch-label">ALB for Dashboard + ASG for 250 Java Processing Units</p>
                <pre>
  TWO SCALING STRATEGIES IN THE PIPELINE:

  1. DASHBOARD (Coder-Facing):
  Medical Coders â†’ ALB (HTTPS) â†’ Dashboard EC2 ASG
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ALB: medcode-dashboard-alb                      â”‚
  â”‚  /api/*      â†’ TG: API servers (min:2, max:8)    â”‚
  â”‚  /dashboard  â†’ TG: Frontend    (min:2, max:6)    â”‚
  â”‚  /health     â†’ Fixed 200                         â”‚
  â”‚  WAF attached for HIPAA                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  2. PROCESSING (250 Java Sectionizer Units):
  SQS-2 â†’ Java App ASG (scales based on queue depth)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ASG: medcode-sectionizer                        â”‚
  â”‚  min: 50  â”‚ desired: 250  â”‚ max: 400             â”‚
  â”‚                                                  â”‚
  â”‚  Scaling metric: SQS-2 ApproximateNumberOf       â”‚
  â”‚  Messages / RunningInstances                     â”‚
  â”‚  Target: 10 messages per instance                â”‚
  â”‚                                                  â”‚
  â”‚  When queue backs up â†’ scale OUT to 400          â”‚
  â”‚  When queue drains  â†’ scale IN to 50             â”‚
  â”‚  Spot Instances (70% savings, SQS retries)       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</pre>
                <ul>
                    <li><strong>ALB for dashboard:</strong> Path-based routing (/api vs /dashboard), WAF for HIPAA, HTTPS termination with ACM cert. Scheduled scaling: min 4 at 8am EST, min 2 at 8pm</li>
                    <li><strong>ASG for 250 Java units:</strong> Scales based on <strong>SQS queue depth</strong> (not CPU). Custom metric: ApproximateNumberOfMessages / InService instances. Target: 10 messages per instance. Queue backs up = scale out</li>
                    <li><strong>Spot + SQS resilience:</strong> Java units run on Spot (70% savings). If Spot interrupted, SQS visibility timeout expires â†’ message returns to queue â†’ another instance picks it up. Zero data loss</li>
                    <li><strong>Mixed instances:</strong> Launch Template with c5.xlarge, m5.xlarge, r5.xlarge â€” ASG picks cheapest available Spot pool. Capacity-optimized allocation strategy</li>
                    <li><strong>Lifecycle hooks:</strong> Scale-in: complete current SQS message processing before termination. Scale-out: pull JAR from S3 + register with monitoring before joining</li>
                </ul>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q25: When would you choose ALB vs NLB?</div>
                    <div class="answer"><strong>ALB</strong> when you need: Layer 7 routing (path/host-based), WebSocket/gRPC support, WAF integration, Lambda targets, authentication (Cognito/OIDC), HTTP header-based routing. Best for: microservices, REST APIs, web applications. <strong>NLB</strong> when you need: ultra-low latency (&lt;100&mu;s vs ALB's ~400ms), millions of concurrent connections, static IPs (whitelisting), TCP/UDP passthrough, preserve source IP natively, non-HTTP protocols (MQTT, gaming). Best for: real-time systems, financial trading, IoT ingestion, TCP load balancing.</div>
                </div>

                <div class="q">
                    <div class="question">Q26: How does cross-zone load balancing work and why does it matter?</div>
                    <div class="answer">Without cross-zone: each AZ's LB node distributes only to targets in its own AZ. If AZ-A has 2 instances and AZ-B has 8, each instance in AZ-A gets 25% traffic while AZ-B instances get 6.25% each &mdash; <strong>uneven load</strong>. With cross-zone: LB distributes evenly across ALL targets in ALL AZs regardless of AZ distribution. Each of the 10 instances gets 10%. <strong>ALB:</strong> always enabled, free. <strong>NLB:</strong> disabled by default, charges apply for inter-AZ data. <strong>CLB:</strong> disabled by default, free. Enable it unless inter-AZ data costs are a concern.</div>
                </div>

                <div class="q">
                    <div class="question">Q27: Explain sticky sessions. When are they useful and what are the drawbacks?</div>
                    <div class="answer"><strong>Sticky sessions</strong> bind a user to the same target for the session duration. ALB uses cookies: <strong>Application cookie</strong> (custom name, your app sets it) or <strong>LB cookie</strong> (AWSALB, auto-generated). Duration: 1 second to 7 days. <strong>Use when:</strong> legacy apps storing session in memory, WebSocket connections, shopping carts not in shared cache. <strong>Drawbacks:</strong> uneven load distribution (popular sessions overload one instance), reduced fault tolerance (if instance fails, session is lost), prevents effective auto-scaling. <strong>Better alternative:</strong> externalize session state to ElastiCache Redis or DynamoDB &mdash; eliminates need for sticky sessions.</div>
                </div>

                <div class="q">
                    <div class="question">Q28: What scaling policy would you choose for different scenarios?</div>
                    <div class="answer"><strong>Target Tracking</strong> for most workloads &mdash; "maintain CPU at 50%" or "1000 requests/target." Simplest, auto-creates scale-out and scale-in alarms. <strong>Step Scaling</strong> for fine-grained control &mdash; different scaling amounts at different thresholds (add 1 at 60% CPU, add 3 at 80%). <strong>Scheduled Scaling</strong> for predictable patterns &mdash; office hours (9am scale up, 7pm scale down), Black Friday (pre-scale to 50 instances). <strong>Predictive Scaling</strong> for cyclic workloads &mdash; uses ML to analyze 14-day history, pre-provisions before traffic spike. <strong>Combination:</strong> use Predictive for baseline + Target Tracking for unexpected spikes.</div>
                </div>

                <div class="q">
                    <div class="question">Q29: How do ASG lifecycle hooks work and what are common use cases?</div>
                    <div class="answer">Lifecycle hooks <strong>pause</strong> an instance during launch or termination. <strong>Scale-out hook (Pending:Wait):</strong> instance launches but doesn't join target group until hook completes. Use cases: install monitoring agents, pull configuration from S3, register with service discovery, run health validation. <strong>Scale-in hook (Terminating:Wait):</strong> instance is marked for termination but stays running. Use cases: drain connections gracefully, push logs to S3, deregister from service discovery, take final backup. Hooks send notifications to EventBridge/SNS/SQS. Timeout: default 1 hour, max 48 hours. Call <code>complete-lifecycle-action</code> to proceed or <code>record-lifecycle-action-heartbeat</code> to extend.</div>
                </div>

                <div class="q">
                    <div class="question">Q30: How does ASG handle unhealthy instances and what health check types are available?</div>
                    <div class="answer"><strong>EC2 health check (default):</strong> Only checks EC2 system status (hardware/hypervisor issues). Instance marked unhealthy only on: stopped, terminated, impaired status. <strong>ELB health check:</strong> Uses the LB's health check (HTTP request to /health). More thorough &mdash; catches application-level failures. <strong>Custom health check:</strong> Use <code>set-instance-health</code> API for external monitoring integration. When unhealthy: ASG terminates the instance and launches a replacement to maintain desired capacity. <strong>Grace period</strong> (default 300s): new instances exempt from health checks during startup. Set it long enough for your app to fully boot and pass health checks.</div>
                </div>

                <div class="q">
                    <div class="question">Q31: Design a highly available web application using ALB + ASG.</div>
                    <div class="answer"><strong>Architecture:</strong> (1) ALB spanning 3 AZs with HTTPS listener (ACM cert) + HTTP&rarr;HTTPS redirect. (2) ASG across 3 AZs with Launch Template (Amazon Linux 2, user data for app bootstrap). (3) Min: 2, Desired: 3, Max: 12 for HA across AZs. (4) Target Tracking at 60% CPU + Predictive Scaling. (5) ELB health checks (not just EC2). (6) Connection draining: 30s deregistration delay. (7) SNS notifications on scaling events for ops. (8) WAF on ALB for security. <strong>Why 3 AZs:</strong> if one AZ fails, 2 remain &mdash; with min 2, at least 1 instance per surviving AZ.</div>
                </div>

                <div class="q">
                    <div class="question">Q32: What is the difference between Launch Template and Launch Configuration?</div>
                    <div class="answer"><strong>Launch Configuration (legacy):</strong> Immutable (must create new one for changes), no versioning, limited features, being deprecated. <strong>Launch Template (recommended):</strong> Supports versioning (v1, v2... with default version), mix instance types (capacity-optimized Spot), T2/T3 unlimited, placement groups, capacity reservations, Dedicated Hosts. Can launch On-Demand + Spot in same ASG with mixed instances policy. Supports multiple instance types for Spot flexibility (if c5.large unavailable, try m5.large, r5.large). Always use Launch Templates for new workloads.</div>
                </div>
            </div>
        </div>

        <!-- Concept 6: Route 53 & DNS -->
        <div class="concept" id="concept-6">
            <h2>6. Route 53 & DNS</h2>
            <span class="tag">DNS & Routing</span>

            <div class="point">
                <h3>DNS Fundamentals & Hosted Zones</h3>
                <p><strong>Route 53</strong> is AWS's highly available DNS service (SLA 100%). Named after DNS port 53. <strong>Hosted Zone:</strong> Container for DNS records for a domain. <strong>Public hosted zone:</strong> resolves domain from the internet (<code>example.com</code>). <strong>Private hosted zone:</strong> resolves within VPCs only (<code>internal.corp</code>). Cost: $0.50/month per hosted zone + $0.40 per million queries. Route 53 also acts as domain registrar (buy domains directly).</p>
            </div>

            <div class="point">
                <h3>Record Types</h3>
                <p><strong>A:</strong> Maps hostname to IPv4 address (<code>example.com &rarr; 1.2.3.4</code>). <strong>AAAA:</strong> Maps hostname to IPv6. <strong>CNAME:</strong> Maps hostname to another hostname (<code>app.example.com &rarr; lb.amazonaws.com</code>). <strong>Cannot</strong> be used at zone apex (naked domain). <strong>Alias:</strong> AWS-specific &mdash; maps hostname to AWS resource (ELB, CloudFront, S3, API Gateway). <strong>Free queries, works at zone apex</strong>. Always prefer Alias over CNAME for AWS resources. <strong>MX:</strong> Mail server. <strong>TXT:</strong> Text record (SPF, domain verification). <strong>NS:</strong> Name servers for the hosted zone.</p>
                <pre>// Alias vs CNAME
CNAME: app.example.com â†’ my-lb-123.us-east-1.elb.amazonaws.com
  âŒ Cannot use for example.com (zone apex)
  âŒ Charged per query

Alias: example.com â†’ my-lb-123.us-east-1.elb.amazonaws.com
  âœ… Works at zone apex
  âœ… Free queries for AWS resources
  âœ… Native health check integration</pre>
            </div>

            <div class="point">
                <h3>Routing Policies</h3>
                <p><strong>Simple:</strong> Single resource, no health checks. Returns all values (client picks randomly). <strong>Weighted:</strong> Split traffic by percentage (70% v1, 30% v2). Great for blue-green deployments. Weight 0 = no traffic. <strong>Latency-based:</strong> Routes to region with lowest latency for the user. AWS maintains latency database. <strong>Failover:</strong> Active-passive. Primary with health check &rarr; if fails, routes to secondary. <strong>Geolocation:</strong> Route based on user's location (continent/country/state). Must set default record. <strong>Geoproximity:</strong> Route based on geographic distance with bias to shift traffic. <strong>Multi-value:</strong> Returns up to 8 healthy records (client-side load balancing with health checks).</p>
            </div>

            <div class="point">
                <h3>Health Checks</h3>
                <p>Route 53 health checkers (15 globally) monitor endpoints. Types: <strong>Endpoint check:</strong> HTTP/HTTPS/TCP to IP or domain (configurable path, interval 10s or 30s, threshold). <strong>Calculated check:</strong> Combine multiple checks with AND/OR/NOT logic. <strong>CloudWatch alarm check:</strong> Monitor private resources (can't be reached by public health checkers) via CloudWatch metrics/alarms. Health check passes if &ge;18% of checkers report healthy. Integrate with routing policies to automatically failover unhealthy targets.</p>
            </div>

            <div class="point">
                <h3>Domain Registration & DNS Migration</h3>
                <p><strong>Register domains</strong> directly in Route 53 (auto-creates hosted zone). Transfer domains from other registrars (unlock at source, get auth code, initiate transfer). <strong>DNS migration:</strong> (1) Create hosted zone in Route 53. (2) Copy all records from old provider. (3) Update NS records at registrar to Route 53 nameservers. (4) Lower TTL before migration (300s), restore after. <strong>DNSSEC:</strong> Route 53 supports DNSSEC signing for public hosted zones &mdash; protects against DNS spoofing. Enable via KMS key (must be in us-east-1).</p>
            </div>

            <div class="point">
                <h3>Route 53 + ELB Architecture Patterns</h3>
                <p><strong>Multi-region active-active:</strong> Latency-based routing to ALBs in multiple regions. Each ALB has health check. User routed to lowest-latency healthy region. <strong>Multi-region active-passive:</strong> Failover routing &mdash; primary region (active) with health check, secondary region (passive) receives traffic only on failure. <strong>Weighted canary:</strong> 95% to current version, 5% to new version. Gradually shift weight. <strong>Geo-restricted:</strong> Geolocation routing to serve different content per country (compliance, localization).</p>
            </div>

            <div class="project-block">
                <h4>In Your Project: MedCode AI Platform</h4>
                <p class="arch-label">DNS for Client Portal, Dashboard & Pipeline Services</p>
                <pre>
  Route 53 Hosted Zone: medcode.healthcare.com
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                              â”‚
  â”‚  medcode.healthcare.com (Coder Dashboard)                    â”‚
  â”‚    Alias â†’ ALB (us-east-1) â”‚ Failover Primary               â”‚
  â”‚    Health Check: /health (checks DB + SQS + S3 connectivity) â”‚
  â”‚                                                              â”‚
  â”‚  medcode.healthcare.com (DR)                                 â”‚
  â”‚    Alias â†’ ALB (us-west-2) â”‚ Failover Secondary              â”‚
  â”‚                                                              â”‚
  â”‚  upload.medcode.healthcare.com (Client PDF Upload Portal)    â”‚
  â”‚    Alias â†’ API Gateway â”‚ Clients upload PDFs here            â”‚
  â”‚    Generates presigned S3 URLs for direct upload             â”‚
  â”‚                                                              â”‚
  â”‚  status.medcode.healthcare.com (Pipeline Status API)         â”‚
  â”‚    Alias â†’ API Gateway â”‚ Downstream apps check coding status â”‚
  â”‚                                                              â”‚
  â”‚  Private Hosted Zone: internal.medcode.local                 â”‚
  â”‚    aurora.internal.medcode.local â†’ Aurora writer endpoint     â”‚
  â”‚    redis.internal.medcode.local  â†’ ElastiCache endpoint      â”‚
  â”‚    sqs.internal.medcode.local    â†’ VPC endpoint for SQS      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Client uploads PDF via upload.medcode.healthcare.com
                  â†’ API Gateway â†’ Lambda â†’ presigned URL â†’ S3
                  â†’ S3 Event â†’ SQS-1 â†’ pipeline starts</pre>
                <ul>
                    <li><strong>Client upload domain:</strong> <code>upload.medcode.healthcare.com</code> â†’ API Gateway. Clients hit this to upload PDFs. Gateway generates presigned URL, client uploads directly to S3</li>
                    <li><strong>Dashboard failover:</strong> Primary us-east-1, DR us-west-2. Health check validates full stack (DB + SQS + S3). Auto-failover in ~60s if primary region dies</li>
                    <li><strong>Status API:</strong> Downstream applications poll <code>status.medcode.healthcare.com</code> to check if coding is complete. Reads from DynamoDB pipeline-tracker</li>
                    <li><strong>Private hosted zone:</strong> 250 Java units resolve <code>aurora.internal.medcode.local</code> for DB access. No public DNS exposure for data layer</li>
                    <li><strong>TTL strategy:</strong> 60s for failover records (fast DR switch), 300s for upload/status APIs (stable endpoints)</li>
                </ul>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q33: What is the difference between CNAME and Alias records in Route 53?</div>
                    <div class="answer"><strong>CNAME:</strong> Standard DNS record that maps one hostname to another. Works for non-root domains only (<code>app.example.com</code>). Cannot be used at zone apex (<code>example.com</code>). Charged per query. Works with any DNS target. <strong>Alias:</strong> AWS proprietary extension. Maps to AWS resources (ELB, CloudFront, S3 website, API Gateway, another Route 53 record). Works at zone apex. Free queries for AWS targets. Auto-detects IP changes of the target resource. Supports native health check evaluation. <strong>Rule of thumb:</strong> Always use Alias for AWS resources; CNAME only for non-AWS external targets.</div>
                </div>

                <div class="q">
                    <div class="question">Q34: Explain all Route 53 routing policies with use cases.</div>
                    <div class="answer"><strong>Simple:</strong> Single endpoint, basic DNS (small app, no failover needed). <strong>Weighted:</strong> Percentage-based split &mdash; blue-green (90/10), A/B testing (50/50), gradual migration. <strong>Latency:</strong> Route to lowest-latency AWS region &mdash; global apps with multi-region deployments. <strong>Failover:</strong> Active/passive DR &mdash; primary with health check, auto-failover to standby. <strong>Geolocation:</strong> Continent/country-based &mdash; compliance (EU data stays in EU), localized content, restrict access by region. <strong>Geoproximity:</strong> Distance-based with bias &mdash; shift traffic between regions without moving infrastructure. <strong>Multi-value:</strong> Return up to 8 healthy IPs &mdash; client-side load balancing with health checks (not a replacement for ELB).</div>
                </div>

                <div class="q">
                    <div class="question">Q35: How do Route 53 health checks work for private resources?</div>
                    <div class="answer">Route 53 health checkers are <strong>public</strong> &mdash; they cannot reach private IPs in VPCs. <strong>Solution:</strong> Create a <strong>CloudWatch metric</strong> that monitors the private resource (e.g., custom metric from an agent, or RDS/ElastiCache CloudWatch metrics). Create a <strong>CloudWatch alarm</strong> on that metric. Create a Route 53 <strong>health check of type "CloudWatch Alarm"</strong>. When the alarm triggers (ALARM state), Route 53 marks the endpoint unhealthy and failover routing kicks in. This pattern lets you health-check databases, internal services, or any private infrastructure.</div>
                </div>

                <div class="q">
                    <div class="question">Q36: How would you design multi-region failover with Route 53?</div>
                    <div class="answer"><strong>Active-Passive:</strong> (1) Deploy app in us-east-1 (primary) and eu-west-1 (standby). (2) Create health check on primary ALB. (3) Failover routing: primary record (us-east-1 ALB, health check attached), secondary record (eu-west-1 ALB). (4) On primary failure, traffic auto-routes to secondary. <strong>Active-Active:</strong> (1) Deploy in both regions with independent databases (or Aurora Global). (2) Latency-based routing to both ALBs, each with health checks. (3) Users always hit lowest-latency healthy region. (4) If one region fails, all traffic goes to the other. <strong>RTO:</strong> Active-Passive ~60-120s (DNS TTL), Active-Active ~30-60s.</div>
                </div>

                <div class="q">
                    <div class="question">Q37: What is the TTL in DNS and how does it impact failover?</div>
                    <div class="answer"><strong>TTL (Time to Live):</strong> How long DNS resolvers and clients cache the record before querying again. High TTL (86400s/24h): fewer DNS queries, lower cost, but slow failover &mdash; clients use stale IP for up to 24h after change. Low TTL (60s): fast failover, but more DNS queries and cost. <strong>Best practices:</strong> Set TTL to 60s before planned changes/migrations. Use 300s (5min) as default for production. Alias records: TTL auto-set by AWS (typically 60s for ELB). <strong>Failover impact:</strong> Even with 60s TTL, some clients/resolvers ignore TTL &mdash; actual failover can take 1-2 minutes. For instant failover, use Global Accelerator (anycast IP, no DNS dependency).</div>
                </div>

                <div class="q">
                    <div class="question">Q38: Explain the difference between Geolocation and Geoproximity routing.</div>
                    <div class="answer"><strong>Geolocation:</strong> Routes based on <em>user's location</em> (continent, country, or US state). Exact match only &mdash; if no record matches user's location, falls back to <strong>default record</strong> (must set one). Use case: compliance (EU users &rarr; EU servers), localized content, access restriction. <strong>Geoproximity:</strong> Routes based on <em>geographic distance</em> between user and resource, with adjustable <strong>bias</strong> (-99 to +99). Positive bias expands the region's catchment area (attracts more traffic). Use case: gradually shift traffic from one region to another without changing infrastructure. Requires <strong>Route 53 Traffic Flow</strong>. Key difference: Geolocation is binary (match/no-match), Geoproximity is distance-based with tunable bias.</div>
                </div>

                <div class="q">
                    <div class="question">Q39: How does Route 53 integrate with other AWS services?</div>
                    <div class="answer"><strong>ELB:</strong> Alias record to ALB/NLB (free, auto-updates IP). <strong>CloudFront:</strong> Alias to distribution for CDN. <strong>S3:</strong> Alias to S3 website endpoint (bucket name must match domain). <strong>API Gateway:</strong> Alias to custom domain. <strong>Elastic Beanstalk:</strong> Alias to environment. <strong>VPC:</strong> Private hosted zones for internal DNS resolution. <strong>Health checks + CloudWatch:</strong> monitor endpoints, trigger alarms. <strong>ACM:</strong> DNS validation for SSL certificates (CNAME record auto-created). <strong>AWS Global Accelerator:</strong> not Route 53 per se, but alternative for anycast-based global routing with instant failover (no DNS dependency).</div>
                </div>

                <div class="q">
                    <div class="question">Q40: You have a global application. How do you minimize latency for users worldwide?</div>
                    <div class="answer"><strong>Option 1 &mdash; Route 53 Latency Routing + Multi-Region:</strong> Deploy app in 3+ regions (us-east-1, eu-west-1, ap-southeast-1). Latency-based routing sends users to nearest region. Each region has ALB + ASG. Database: Aurora Global Database (read replicas in each region, &lt;1s replication). <strong>Option 2 &mdash; CloudFront + Single Region:</strong> If app is mostly read-heavy, put CloudFront in front. Edge caches serve static/dynamic content globally. Origin in one region. <strong>Option 3 &mdash; Global Accelerator:</strong> Anycast IPs route over AWS backbone to nearest region. Better than Route 53 for TCP/UDP workloads and instant failover. <strong>Best practice:</strong> Combine all three &mdash; CloudFront for static, Global Accelerator for API, Route 53 latency for regional backends.</div>
                </div>
            </div>
        </div>

        <!-- Concept 7: RDS & Aurora -->
        <div class="concept" id="concept-7">
            <h2>7. RDS & Aurora</h2>
            <span class="tag">Managed Databases</span>

            <div class="point">
                <h3>RDS Overview & Engines</h3>
                <p><strong>RDS (Relational Database Service):</strong> Managed database &mdash; AWS handles provisioning, patching, backups, failover. You manage: schema, queries, optimization. Supported engines: <strong>MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Aurora</strong>. Cannot SSH into RDS instances. Runs on EC2 instances behind the scenes (db.t3.micro to db.r6g.16xlarge). <strong>Storage:</strong> EBS-backed (gp3, io1/io2). Auto-scaling storage up to configured max (no downtime). Max storage: 64 TB (16 TB for SQL Server).</p>
            </div>

            <div class="point">
                <h3>Multi-AZ Deployments</h3>
                <p><strong>Multi-AZ:</strong> Synchronous standby replica in different AZ for <strong>high availability</strong> (not read scaling). Automatic failover: DNS endpoint switches to standby in ~60-120 seconds. Triggers: AZ outage, primary instance failure, storage failure, manual failover. <strong>Multi-AZ Cluster (new):</strong> 1 writer + 2 readable standby instances across 3 AZs. Transaction logs replicated via semisynchronous replication. Failover in ~35 seconds. Standby can serve read queries (reader endpoint). Available for MySQL and PostgreSQL.</p>
                <pre>// Multi-AZ vs Read Replica
Multi-AZ:
  Purpose: High Availability (failover)
  Replication: Synchronous
  Access: Standby NOT accessible for reads*
  Failover: Automatic (~60-120s)
  Regions: Same region only

Read Replica:
  Purpose: Read scaling (performance)
  Replication: Asynchronous
  Access: Independently queryable
  Failover: Manual promotion
  Regions: Same or cross-region

*Multi-AZ Cluster standby CAN serve reads</pre>
            </div>

            <div class="point">
                <h3>Read Replicas</h3>
                <p>Create up to <strong>15 read replicas</strong> (5 for non-Aurora RDS). Asynchronous replication &mdash; eventual consistency. Can be in same AZ, cross-AZ (free within region), or <strong>cross-region</strong> (network charges apply). Use for: read-heavy workloads, reporting/analytics queries, serving different geo regions. Can be <strong>promoted to standalone</strong> DB (breaks replication). Replicas can have replicas (chaining). Application must handle directing reads to replica endpoint.</p>
            </div>

            <div class="point">
                <h3>Amazon Aurora</h3>
                <p><strong>Aurora:</strong> AWS-proprietary, cloud-optimized. 5x MySQL / 3x PostgreSQL throughput. Storage: auto-grows 10 GB to 128 TB, <strong>6 copies across 3 AZs</strong> (4/6 needed for writes, 3/6 for reads). Self-healing storage (data blocks continuously scanned and repaired). <strong>Writer endpoint:</strong> points to primary. <strong>Reader endpoint:</strong> connection-level load balancing across up to 15 replicas. Failover: &lt;30 seconds (promotes a replica). Supports <strong>backtrack</strong> &mdash; rewind DB to point in time without restore (MySQL only).</p>
            </div>

            <div class="point">
                <h3>Aurora Serverless & Global Database</h3>
                <p><strong>Aurora Serverless v2:</strong> Auto-scales capacity in fine-grained increments (0.5 ACU steps). Scales in seconds. Min/max ACU configurable. Pay per ACU-second. Use for: unpredictable/intermittent workloads, dev/test, multi-tenant SaaS. Can mix Serverless and provisioned instances in same cluster. <strong>Aurora Global Database:</strong> Primary region (read/write) + up to 5 secondary regions (read-only, &lt;1 second replication lag). Designed for global apps and disaster recovery. Failover to secondary region: RPO &lt;1s, RTO &lt;1 minute. Up to 16 read replicas per secondary region.</p>
            </div>

            <div class="point">
                <h3>Backup, Restore & Encryption</h3>
                <p><strong>Automated backups:</strong> Daily full snapshot + transaction logs every 5 minutes. Retention: 1-35 days. Point-in-time restore (PITR) to any second within retention. Restores create a <strong>new DB instance</strong>. <strong>Manual snapshots:</strong> User-triggered, retained until deleted. Can copy cross-region and share cross-account. <strong>Encryption:</strong> At-rest via KMS (AES-256). Must enable at creation (cannot encrypt existing unencrypted DB &mdash; must snapshot, copy with encryption, restore). In-transit: SSL/TLS. Replicas use same encryption key as primary (cross-region replicas use target region's key).</p>
            </div>

            <div class="point">
                <h3>RDS Proxy</h3>
                <p><strong>RDS Proxy:</strong> Fully managed database proxy. <strong>Connection pooling:</strong> Multiplexes thousands of app connections into a small pool of DB connections. Reduces connection overhead by up to 66%. <strong>Failover:</strong> Reduces failover time by up to 66% (maintains connection pool, routes to new primary transparently). <strong>IAM authentication:</strong> Stores credentials in Secrets Manager, enforces IAM auth from apps. Essential for <strong>Lambda</strong> (Lambda functions create many short-lived connections that overwhelm DB). Supports MySQL, PostgreSQL, MariaDB, SQL Server.</p>
                <pre>// Without RDS Proxy (Lambda problem)
1000 Lambda invocations = 1000 DB connections
  â†’ DB max_connections exhausted â†’ errors

// With RDS Proxy
1000 Lambda invocations â†’ RDS Proxy (pool of ~50 connections)
  â†’ DB handles 50 connections comfortably</pre>
            </div>

            <div class="project-block">
                <h4>In Your Project: MedCode AI Platform</h4>
                <p class="arch-label">Aurora MySQL â€” Final Destination for AI-Generated Medical Codes</p>
                <pre>
  Pipeline end: Java App â†’ AI Coding â†’ DB (HERE) â†’ Downstream Apps

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Aurora MySQL Cluster: medcode-prod                          â”‚
  â”‚                                                              â”‚
  â”‚  Writer (db.r6g.xlarge):                                     â”‚
  â”‚    â† Java App (250 units) writes finalized codes             â”‚
  â”‚      { encounter_id, icd10[], cpt[], hcpcs[],               â”‚
  â”‚        confidence, ai_model, sections, coder_id }            â”‚
  â”‚    â† Dashboard writes: coder reviews, approvals, overrides   â”‚
  â”‚    â† All via RDS Proxy (250 units = 250 connections pooled)  â”‚
  â”‚                                                              â”‚
  â”‚  Reader x2 (db.r6g.large):                                   â”‚
  â”‚    â† Dashboard reads: encounter list, code review, search    â”‚
  â”‚    â† Downstream apps: pull finalized codes for billing       â”‚
  â”‚    â† Analytics: accuracy reports, coder productivity         â”‚
  â”‚                                                              â”‚
  â”‚  Serverless v2 Reader (0.5-8 ACU):                           â”‚
  â”‚    â† Month-end reporting bursts (all clients at once)        â”‚
  â”‚                                                              â”‚
  â”‚  Global Database â†’ us-west-2 (DR)                            â”‚
  â”‚    replication < 1s â”‚ failover RTO < 1 min                   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  250 Java units + Lambda â”€â”€â†’ RDS Proxy (pool: ~100 conn)
                              â”€â”€â†’ Aurora Writer
  Without Proxy: 250+ simultaneous connections overwhelm DB</pre>
                <ul>
                    <li><strong>RDS Proxy critical:</strong> 250 Java sectionizer units + Lambda all write to Aurora. Without proxy, 250+ simultaneous connections exhaust <code>max_connections</code>. Proxy pools to ~100 real DB connections</li>
                    <li><strong>What gets stored:</strong> Final AI-generated codes (ICD-10, CPT, HCPCS), confidence scores, section text references, coder review status, audit trail (who approved, when, what changed)</li>
                    <li><strong>Downstream supply:</strong> Other applications (billing, claims, reporting) read finalized codes from Aurora reader endpoint. Decoupled from processing pipeline</li>
                    <li><strong>Serverless v2 reader:</strong> Month-end all clients request reports simultaneously. Burst from 0.5 to 8 ACU automatically, no manual scaling</li>
                    <li><strong>Encryption:</strong> KMS at rest (HIPAA), TLS in transit via <code>require_secure_transport=ON</code>. Java app connects via IAM auth through RDS Proxy</li>
                    <li><strong>Backups:</strong> 35-day retention + pre-deployment snapshots. PITR to any second â€” critical when 250 units processing concurrently, one bad deployment could corrupt data</li>
                </ul>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q41: What is the difference between Multi-AZ and Read Replicas in RDS?</div>
                    <div class="answer"><strong>Multi-AZ:</strong> Purpose is <em>high availability</em>. Synchronous replication to standby in different AZ. Standby is NOT accessible for reads (except Multi-AZ Cluster). Automatic failover (60-120s). Same region only. No performance improvement. <strong>Read Replica:</strong> Purpose is <em>read scalability</em>. Asynchronous replication (eventual consistency). Fully accessible for reads (own endpoint). No automatic failover (manual promotion). Can be cross-region. Improves read performance. <strong>Common combo:</strong> Multi-AZ primary (for HA) + read replicas (for scaling) + read replica in another region (for DR).</div>
                </div>

                <div class="q">
                    <div class="question">Q42: How does Aurora achieve high availability and what makes it different from standard RDS?</div>
                    <div class="answer"><strong>Aurora storage:</strong> Data stored in a <em>cluster volume</em> spanning 3 AZs with 6 copies. Writes need 4/6 quorum, reads need 3/6. Self-healing: background process scans and repairs corrupt blocks. <strong>Failover:</strong> If primary fails, Aurora promotes a replica in &lt;30s (vs 60-120s for RDS). If no replica, creates new primary (~10 min). <strong>Performance:</strong> 5x MySQL, 3x PostgreSQL throughput via log-structured storage (only writes redo logs, not data pages). <strong>Features RDS lacks:</strong> Backtrack (rewind without restore), Global Database (&lt;1s cross-region replication), Serverless v2 (auto-scaling ACUs), up to 15 replicas (vs 5), reader endpoint load balancing.</div>
                </div>

                <div class="q">
                    <div class="question">Q43: When would you use Aurora Serverless vs Provisioned?</div>
                    <div class="answer"><strong>Aurora Serverless v2</strong> for: (1) Unpredictable or variable workloads (startup with unknown traffic). (2) Intermittent usage (dev/test used only during business hours). (3) Multi-tenant SaaS (each tenant has different usage patterns). (4) Mixed workloads (handles spikes without over-provisioning). Scales 0.5 ACU increments in seconds. <strong>Provisioned</strong> for: (1) Predictable, steady-state production workloads. (2) Cost-sensitive scenarios where you know exact capacity. (3) When you need specific instance types for performance tuning. <strong>Hybrid:</strong> v2 supports mixing provisioned and serverless instances in same cluster &mdash; provisioned writer + serverless readers for burst read capacity.</div>
                </div>

                <div class="q">
                    <div class="question">Q44: Why is RDS Proxy essential for Lambda-based applications?</div>
                    <div class="answer">Lambda functions are <strong>ephemeral</strong> &mdash; each invocation opens a new DB connection. Under load, 1000 concurrent Lambdas = 1000 connections, quickly exhausting <code>max_connections</code> (e.g., db.t3.micro allows ~60). <strong>RDS Proxy solves this:</strong> (1) <strong>Connection pooling:</strong> multiplexes thousands of Lambda connections into ~50-100 actual DB connections. (2) <strong>Connection reuse:</strong> Lambda connects to proxy, proxy reuses idle connections from pool. (3) <strong>Faster failover:</strong> Proxy maintains connection pool and transparently routes to new primary during Multi-AZ failover (66% faster). (4) <strong>IAM auth:</strong> Lambda authenticates via IAM, Proxy fetches credentials from Secrets Manager. No credentials in Lambda code.</div>
                </div>

                <div class="q">
                    <div class="question">Q45: How do you encrypt an existing unencrypted RDS database?</div>
                    <div class="answer">You <strong>cannot enable encryption on an existing unencrypted RDS instance</strong>. The process: (1) Create a <strong>snapshot</strong> of the unencrypted DB. (2) <strong>Copy the snapshot</strong> with encryption enabled (select KMS key). (3) <strong>Restore</strong> a new DB instance from the encrypted snapshot. (4) Update application to point to new DB endpoint. (5) Verify and delete old unencrypted instance. For <strong>cross-region encrypted copy:</strong> Copy snapshot to target region with target region's KMS key. <strong>Read replicas:</strong> If master is encrypted, all replicas are encrypted. Cannot create unencrypted replica from encrypted master. Cross-region replicas use destination region's KMS key.</div>
                </div>

                <div class="q">
                    <div class="question">Q46: Explain Aurora Global Database and its DR capabilities.</div>
                    <div class="answer"><strong>Aurora Global Database:</strong> One primary region (read/write) + up to 5 secondary regions (read-only). Uses dedicated replication infrastructure &mdash; <strong>&lt;1 second replication lag</strong> (vs minutes for cross-region read replicas). Each secondary region can have up to 16 replicas. <strong>Planned failover (switchover):</strong> RPO = 0, RTO ~1-2 minutes. For planned migration or region switch. <strong>Unplanned failover:</strong> RPO &lt;1s (some data may not have replicated), RTO &lt;1 minute. Promote secondary to primary via console/CLI. <strong>Use cases:</strong> Global reads (users hit local region), disaster recovery (entire region failure), data sovereignty with read-local/write-global pattern.</div>
                </div>

                <div class="q">
                    <div class="question">Q47: What are RDS parameter groups and option groups?</div>
                    <div class="answer"><strong>Parameter Groups:</strong> Configuration for the database engine (like my.cnf for MySQL). Controls: max_connections, query_cache_size, innodb_buffer_pool_size, log settings. <strong>DB Parameter Group:</strong> applies to single instance. <strong>DB Cluster Parameter Group:</strong> applies to Aurora cluster. Changes: some apply immediately, some require reboot (static vs dynamic parameters). <strong>Option Groups:</strong> Enable additional features for the engine &mdash; Oracle TDE, SQL Server Audit, MySQL memcached plugin. <strong>Best practice:</strong> Never modify default parameter group (it's shared). Create custom parameter group, tune for your workload, and associate with your instance. Track changes via description field.</div>
                </div>

                <div class="q">
                    <div class="question">Q48: Design a database architecture for a high-traffic e-commerce application.</div>
                    <div class="answer"><strong>Architecture:</strong> (1) <strong>Aurora MySQL cluster</strong> with Multi-AZ writer + 3 read replicas (auto-scaling replicas between 1-5). (2) <strong>RDS Proxy</strong> in front for connection pooling (essential if using Lambda). (3) <strong>Reader endpoint</strong> for product catalog reads, <strong>writer endpoint</strong> for orders/transactions. (4) <strong>ElastiCache Redis</strong> for session management + product catalog cache (reduce DB load by 80%). (5) <strong>Aurora Global Database</strong> with secondary in eu-west-1 for DR (RPO &lt;1s). (6) Automated backups: 35-day retention + pre-deployment manual snapshots. (7) <strong>Monitoring:</strong> CloudWatch alarms on CPU, connections, replica lag, freeable memory. <strong>RTO: &lt;1 min, RPO: &lt;1s.</strong></div>
                </div>
            </div>
        </div>

        <!-- Concept 8: DynamoDB -->
        <div class="concept" id="concept-8">
            <h2>8. DynamoDB</h2>
            <span class="tag">NoSQL</span>

            <div class="point">
                <h3>DynamoDB Fundamentals</h3>
                <p><strong>Fully managed NoSQL</strong> key-value and document database. Single-digit millisecond latency at any scale. Serverless &mdash; no instances to manage. Automatic replication across <strong>3 AZs</strong>. Max item size: <strong>400 KB</strong>. Supports JSON documents. <strong>Primary key</strong> options: Partition key only (simple) or Partition key + Sort key (composite). Partition key determines which partition stores the data. Good partition key = high cardinality (e.g., userId, orderId), bad = low cardinality (e.g., status, country).</p>
            </div>

            <div class="point">
                <h3>Capacity Modes</h3>
                <p><strong>Provisioned:</strong> You specify RCU (Read Capacity Units) and WCU (Write Capacity Units). 1 RCU = 1 strongly consistent read/sec (4 KB) or 2 eventually consistent. 1 WCU = 1 write/sec (1 KB). Can enable <strong>Auto Scaling</strong> (target utilization %, min/max). Best for predictable workloads. <strong>On-Demand:</strong> Pay per request. No capacity planning. Instantly scales to thousands of requests/sec. 2.5x more expensive per request than provisioned, but no wasted capacity. Best for unpredictable, spiky workloads. Can switch modes once per 24 hours.</p>
                <pre>// Capacity calculation example
// Table: 250 units, each unit writes 3 items/sec (avg 2 KB)
// Writes: 250 Ã— 3 = 750 writes/sec
// Each write = ceil(2 KB / 1 KB) = 2 WCU
// Total WCU needed: 750 Ã— 2 = 1,500 WCU

// Reads: 250 units, each reads 5 items/sec (avg 3 KB)
// Eventually consistent: 250 Ã— 5 = 1,250 reads/sec
// Each read = ceil(3 KB / 4 KB) = 1 RCU
// Eventually consistent: 1,250 Ã— 1 / 2 = 625 RCU</pre>
            </div>

            <div class="point">
                <h3>Secondary Indexes (GSI & LSI)</h3>
                <p><strong>GSI (Global Secondary Index):</strong> Different partition key and/or sort key. Creates a separate table (projected attributes copied). Has its own RCU/WCU. Can be added/removed anytime. Max 20 per table. <strong>LSI (Local Secondary Index):</strong> Same partition key, different sort key. Shares RCU/WCU with base table. Must be created at table creation. Max 5 per table. <strong>Projections:</strong> KEYS_ONLY, INCLUDE (specific attributes), ALL. Fewer projected attributes = less cost but may require fetches back to base table.</p>
                <pre>// GSI example for MedCode
Base table PK: encounterId, SK: timestamp
GSI-1: PK: clientId, SK: timestamp
  â†’ Query all encounters for a specific client
GSI-2: PK: coderId, SK: status
  â†’ Query all encounters assigned to a coder by status
GSI-3: PK: aiModelVersion, SK: confidenceScore
  â†’ Query results by AI model for accuracy analysis</pre>
            </div>

            <div class="point">
                <h3>DynamoDB Streams</h3>
                <p>Captures <strong>time-ordered sequence</strong> of item-level changes (INSERT, MODIFY, REMOVE). Retained for 24 hours. Stream records contain: KEYS_ONLY, NEW_IMAGE, OLD_IMAGE, or NEW_AND_OLD_IMAGES. Integrates with <strong>Lambda</strong> (event-driven processing), <strong>Kinesis Data Streams</strong> (longer retention, multiple consumers). Use cases: trigger notifications on change, replicate to another table, maintain materialized views, audit trail, event sourcing.</p>
            </div>

            <div class="point">
                <h3>DAX (DynamoDB Accelerator)</h3>
                <p><strong>In-memory cache</strong> for DynamoDB. Microsecond latency (vs millisecond). Fully managed, highly available (multi-AZ cluster). Drop-in compatible &mdash; same API as DynamoDB (just change endpoint). <strong>Item cache:</strong> Caches individual GetItem/BatchGetItem results by primary key. <strong>Query cache:</strong> Caches Query/Scan results. Default TTL: 5 minutes. Best for read-heavy, latency-sensitive workloads. NOT suitable for: write-heavy apps, strongly consistent reads (DAX serves eventually consistent only), apps needing very low cache TTL.</p>
            </div>

            <div class="point">
                <h3>TTL, Transactions & Batch Operations</h3>
                <p><strong>TTL (Time to Live):</strong> Auto-delete expired items at no cost. Set a TTL attribute (epoch timestamp). DynamoDB deletes within 48 hours of expiry. Use for: session data, temp tokens, old logs. <strong>Transactions:</strong> ACID across up to 100 items across multiple tables. <code>TransactWriteItems</code> / <code>TransactGetItems</code>. All-or-nothing &mdash; if one operation fails, entire transaction rolls back. Costs 2x normal (reads/writes for coordination). <strong>Batch:</strong> <code>BatchWriteItem</code> (up to 25 items, 16 MB), <code>BatchGetItem</code> (up to 100 items). NOT transactional &mdash; individual items can fail (handle <code>UnprocessedItems</code>).</p>
            </div>

            <div class="point">
                <h3>Single-Table Design</h3>
                <p>Store multiple entity types in <strong>one table</strong> using generic key names (PK/SK). Overloaded keys: <code>PK=CLIENT#123</code>, <code>SK=ENCOUNTER#456</code>. Benefits: fewer tables, all related data in one query (1 round trip), efficient for access patterns. Use <strong>GSIs</strong> for additional access patterns. <strong>When NOT to use:</strong> Simple CRUD apps, when access patterns are unknown, small teams unfamiliar with DynamoDB. For complex reporting/joins, keep relational DB (Aurora) and use DynamoDB for high-speed operational data.</p>
                <pre>// Single-table design for MedCode
PK                  SK                    Data
CLIENT#C001         META                  { name, plan, units }
CLIENT#C001         ENCOUNTER#E100        { pdfUrl, status, uploadedAt }
CLIENT#C001         ENCOUNTER#E101        { pdfUrl, status, uploadedAt }
ENCOUNTER#E100      TEXTRACT#001          { extractedText, pages }
ENCOUNTER#E100      SECTION#HPI           { text, aiCodes }
ENCOUNTER#E100      SECTION#ASSESSMENT    { text, aiCodes }
ENCOUNTER#E100      CODE#ICD10            { codes: ["E11.9","I10"] }
CODER#COD001        ASSIGNMENT#E100       { status, assignedAt }</pre>
            </div>

            <div class="project-block">
                <h4>In Your Project: MedCode AI Platform</h4>
                <p class="arch-label">DynamoDB as the High-Speed Pipeline Tracker &amp; Results Store</p>
                <pre>
  Full Pipeline (250 Processing Units):

  Client PDF â”€â”€â†’ S3 â”€â”€â†’ SQS-1 â”€â”€â†’ Lambda (Textract) â”€â”€â†’ SQS-2 â”€â”€â†’ Java App â”€â”€â†’ AI â”€â”€â†’ DB â”€â”€â†’ Downstream
       â”‚                    â”‚              â”‚                  â”‚           â”‚         â”‚
       â””â”€â”€ DynamoDB tracks every step â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
                                                                                    â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚  DynamoDB Table: medcode-pipeline-tracker
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚ PK              â”‚ SK                   â”‚ Attributes                  â”‚
  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  â”‚ ENCOUNTER#E100  â”‚ STATUS               â”‚ stage: "AI_CODING"          â”‚
  â”‚  â”‚                 â”‚                      â”‚ progress: 4/6 steps done    â”‚
  â”‚  â”‚ ENCOUNTER#E100  â”‚ TEXTRACT#2024-03-15  â”‚ pageCount: 12, ocrConf: 97% â”‚
  â”‚  â”‚ ENCOUNTER#E100  â”‚ SECTION#HPI          â”‚ text: "Patient presents..." â”‚
  â”‚  â”‚ ENCOUNTER#E100  â”‚ SECTION#ASSESSMENT   â”‚ text: "Diabetes Type 2..."  â”‚
  â”‚  â”‚ ENCOUNTER#E100  â”‚ CODES#FINAL          â”‚ icd10: ["E11.9","I10"]      â”‚
  â”‚  â”‚                 â”‚                      â”‚ cpt: ["99214"], conf: 0.94  â”‚
  â”‚  â”‚ CLIENT#C001     â”‚ STATS#2024-03        â”‚ totalPDFs: 3200, coded: 3100â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚  On-Demand mode: 250 units processing = unpredictable spikes
  â”‚  DynamoDB Streams â†’ Lambda â†’ update dashboard in real-time
  â”‚  TTL: Raw text sections auto-deleted after 90 days (save cost)</pre>
                <ul>
                    <li><strong>Pipeline tracker:</strong> Each encounter's status tracked through 6 stages (uploaded â†’ OCR â†’ sectionized â†’ AI coding â†’ review â†’ finalized). 250 units writing concurrently &mdash; DynamoDB handles this without throttling on On-Demand mode</li>
                    <li><strong>On-Demand capacity:</strong> 250 units create unpredictable write bursts (all units may finish Textract simultaneously). On-Demand auto-scales instantly, no capacity planning needed</li>
                    <li><strong>DynamoDB Streams:</strong> Every status change triggers Lambda â†’ pushes real-time updates to coder dashboard via WebSocket (API Gateway). Coders see live progress of their assigned encounters</li>
                    <li><strong>GSI on clientId + month:</strong> Query all encounters for a client in a billing period. Used for monthly reporting and invoicing</li>
                    <li><strong>TTL on raw text:</strong> Textract output and section text auto-deleted after 90 days. Final codes kept in Aurora for long-term compliance. Saves DynamoDB storage cost</li>
                    <li><strong>Why DynamoDB + Aurora:</strong> DynamoDB for high-speed pipeline tracking and real-time lookups. Aurora for relational queries, reporting, joins across clients/coders/codes. Each DB does what it's best at</li>
                </ul>
            </div>

            <div class="qa">
                <h3>Interview Q&A</h3>

                <div class="q">
                    <div class="question">Q49: When would you choose DynamoDB over a relational database like Aurora?</div>
                    <div class="answer"><strong>Choose DynamoDB when:</strong> (1) Consistent single-digit ms latency at any scale. (2) Known access patterns (key-value lookups, not ad-hoc SQL). (3) High write throughput (thousands/sec). (4) Serverless/event-driven architecture. (5) Simple data model (no complex joins). <strong>Choose Aurora when:</strong> Complex queries with JOINs, ad-hoc reporting, ACID transactions across many tables, relational data model. <strong>In MedCode:</strong> DynamoDB tracks pipeline status (fast writes from 250 units), Aurora stores finalized codes for relational reporting (join encounters + coders + clients + codes). Use both &mdash; each for its strength.</div>
                </div>

                <div class="q">
                    <div class="question">Q50: How do you design a DynamoDB partition key for high throughput?</div>
                    <div class="answer"><strong>Goal:</strong> Even distribution across partitions. <strong>Good keys:</strong> High cardinality &mdash; userId, orderId, encounterId (millions of unique values). Each partition handles 3,000 RCU + 1,000 WCU. <strong>Bad keys:</strong> status (only 5 values &mdash; "pending"/"processing"/"done" = hot partition), date (all today's writes hit one partition). <strong>Solutions for hot partitions:</strong> (1) Add random suffix: <code>date#2024-03-15#3</code> (shard across N partitions, scatter-gather on read). (2) Composite key: <code>clientId#date</code>. (3) Write sharding with known suffix count. In MedCode: <code>ENCOUNTER#E100</code> is ideal &mdash; millions of unique encounters, evenly distributed.</div>
                </div>

                <div class="q">
                    <div class="question">Q51: Explain the difference between GSI and LSI. When would you use each?</div>
                    <div class="answer"><strong>LSI (Local Secondary Index):</strong> Same partition key as table, different sort key. Created at table creation only. Shares table's throughput. Max 5. Use when: you query the same partition but need a different sort order (e.g., encounters by encounterId sorted by status vs sorted by date). <strong>GSI (Global Secondary Index):</strong> Completely different partition key and/or sort key. Own throughput (separate RCU/WCU). Created anytime. Max 20. Use when: you need a completely different access pattern (e.g., base table by encounterId, GSI by clientId or coderId). <strong>Key difference:</strong> LSI = different sort within same partition. GSI = entirely different partition key = different access pattern.</div>
                </div>

                <div class="q">
                    <div class="question">Q52: How do DynamoDB Streams work and what are common use cases?</div>
                    <div class="answer"><strong>Streams</strong> capture ordered sequence of changes (INSERT/MODIFY/REMOVE) with 24-hour retention. View types: KEYS_ONLY, NEW_IMAGE, OLD_IMAGE, NEW_AND_OLD_IMAGES. Lambda reads from stream (event source mapping). <strong>Use cases:</strong> (1) <strong>Real-time dashboard:</strong> Stream â†’ Lambda â†’ WebSocket pushes status updates to coders. (2) <strong>Cross-region replication:</strong> Stream â†’ Lambda â†’ write to table in another region (or use Global Tables). (3) <strong>Materialized views:</strong> Maintain aggregated counts in a separate table. (4) <strong>Audit trail:</strong> Stream old/new images to S3 for compliance. (5) <strong>Event sourcing:</strong> Every change is an event, rebuild state from stream. In MedCode: every pipeline stage update streams to dashboard in real-time.</div>
                </div>

                <div class="q">
                    <div class="question">Q53: What is single-table design and when should you use it?</div>
                    <div class="answer"><strong>Single-table design:</strong> Store multiple entity types (users, orders, products) in one table using generic PK/SK with prefixes (<code>USER#123</code>, <code>ORDER#456</code>). <strong>Benefits:</strong> (1) Fetch related data in 1 query (e.g., client + all encounters with PK=CLIENT#C001). (2) Fewer tables to manage. (3) Efficient transactions (up to 100 items, same table). <strong>When to use:</strong> Well-defined access patterns, high-performance apps, DynamoDB-experienced teams. <strong>When NOT to use:</strong> Unknown access patterns (still evolving), simple CRUD, team unfamiliar with NoSQL modeling. <strong>MedCode:</strong> One table stores encounters, sections, codes, assignments &mdash; single query fetches entire encounter pipeline state.</div>
                </div>

                <div class="q">
                    <div class="question">Q54: How does DAX compare to ElastiCache for DynamoDB caching?</div>
                    <div class="answer"><strong>DAX:</strong> DynamoDB-specific, same API (drop-in), microsecond reads, caches individual items + query results, managed cluster, no code changes needed. <strong>ElastiCache (Redis):</strong> General-purpose, requires code changes (check cache â†’ miss â†’ query DB â†’ store in cache), supports complex data structures (sorted sets, pub/sub), cross-service caching (not just DynamoDB), more control over eviction. <strong>Choose DAX:</strong> Pure DynamoDB reads, need simplest integration, item-level caching. <strong>Choose ElastiCache:</strong> Cache results from multiple sources, need pub/sub, need complex data types, want explicit cache control. <strong>MedCode:</strong> DAX for encounter status lookups (hot reads from dashboard), ElastiCache Redis for cross-service session + leaderboard caching.</div>
                </div>

                <div class="q">
                    <div class="question">Q55: How do DynamoDB transactions work and what are the limitations?</div>
                    <div class="answer"><strong>TransactWriteItems:</strong> Up to 100 actions (Put, Update, Delete, ConditionCheck) across multiple tables. All succeed or all fail (ACID). <strong>TransactGetItems:</strong> Up to 100 items, read atomically. <strong>Cost:</strong> 2x normal (each item consumes double RCU/WCU for coordination). <strong>Limitations:</strong> Max 100 items per transaction, 4 MB total size, no cross-region transactions, items must have different primary keys, no long-running transactions (no locks). <strong>MedCode use case:</strong> When finalizing an encounter: atomically write final codes (CODES#FINAL) + update encounter status ("finalized") + update client stats (totalCoded++) &mdash; all succeed or none. Prevents partial state.</div>
                </div>

                <div class="q">
                    <div class="question">Q56: How would you handle 250 concurrent processing units writing to DynamoDB?</div>
                    <div class="answer"><strong>On-Demand mode:</strong> Best for this scenario. DynamoDB auto-scales to handle burst writes from 250 units. No capacity planning. Adapts to traffic spikes when all units finish Textract simultaneously. <strong>If Provisioned:</strong> Calculate: 250 units Ã— avg 5 writes/sec Ã— 2 WCU each = 2,500 WCU. Enable Auto Scaling (target 70%, min 1,000, max 5,000 WCU). <strong>Avoid hot partitions:</strong> Use encounterId as PK (high cardinality, evenly distributed). <strong>Batch writes:</strong> Use <code>BatchWriteItem</code> (25 items max) to reduce API calls. Handle <code>UnprocessedItems</code> with exponential backoff. <strong>Monitoring:</strong> CloudWatch alarms on <code>ThrottledRequests</code>, <code>ConsumedWriteCapacityUnits</code> to detect issues early.</div>
                </div>
            </div>
        </div>

    </main>
</div>

<script>
    // Mobile menu toggle
    function toggleMenu() {
        document.querySelector('.sidebar').classList.toggle('open');
        document.querySelector('.sidebar-overlay').classList.toggle('show');
    }

    // Close sidebar on link click (mobile)
    document.querySelectorAll('.sidebar nav a').forEach(a => {
        a.addEventListener('click', () => {
            if (window.innerWidth <= 768) {
                document.querySelector('.sidebar').classList.remove('open');
                document.querySelector('.sidebar-overlay').classList.remove('show');
            }
        });
    });

    // Highlight active sidebar link on scroll
    const links = document.querySelectorAll('.sidebar nav a');
    const sections = document.querySelectorAll('.concept');
    window.addEventListener('scroll', () => {
        let current = '';
        sections.forEach(s => {
            if (window.scrollY >= s.offsetTop - 100) current = s.id;
        });
        links.forEach(a => {
            a.classList.remove('active');
            if (a.getAttribute('href') === '#' + current) a.classList.add('active');
        });
    });
</script>
</body>
</html>